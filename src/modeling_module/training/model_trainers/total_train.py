from typing import Dict, Tuple, Optional, Iterable, List, Callable
from pathlib import Path
from dataclasses import asdict, is_dataclass, replace

import numpy as np
import torch

from modeling_module.models.PatchMixer.common.configs import (
    PatchMixerConfigMonthly,
    PatchMixerConfig,
    PatchMixerConfigWeekly,
)
# PatchMixerConfigDaily, Hourly는 별도 클래스 없이 PatchMixerConfig + kwargs로 처리

from modeling_module.models.PatchTST.common.configs import PatchTSTConfig
from modeling_module.models.PatchTST.self_supervised.PatchTST import PatchTSTPretrainModel
from modeling_module.models.Titan.common.configs import TitanConfig
from modeling_module.models.model_builder import (
    build_titan_base,
    build_titan_lmm,
    build_titan_seq2seq,
    build_patch_mixer_base,
    build_patch_mixer_quantile,
    build_patchTST_base,
    build_patchTST_quantile,
)
from modeling_module.training.config import SpikeLossConfig, TrainingConfig, StageConfig
from modeling_module.training.metrics import quantile_metrics
from modeling_module.training.model_trainers.patchmixer_train import train_patchmixer
from modeling_module.training.model_trainers.patchtst_finetune import train_patchtst_finetune
from modeling_module.training.model_trainers.patchtst_pretrain import train_patchtst_pretrain
from modeling_module.training.model_trainers.patchtst_train import train_patchtst
from modeling_module.training.model_trainers.titan_train import train_titan
from modeling_module.utils.exogenous_utils import compose_exo_calendar_cb
from modeling_module.utils.metrics import smape, rmse, mae


# ===================== 공통 유틸 =====================

def _get_part_vocab_size_from_loader(loader) -> int:
    try:
        return len(getattr(loader.dataset, "part_vocab", {}))
    except Exception:
        return 0


def save_model(model: torch.nn.Module, cfg, path: str) -> None:
    path = str(path)
    state = {
        "model_state": model.state_dict(),
        "model_class": model.__class__.__name__,
    }
    if cfg is not None:
        if is_dataclass(cfg):
            state["config"] = asdict(cfg)
        else:
            cfg_dict = getattr(cfg, "__dict__", None)
            if cfg_dict is not None:
                state["config"] = dict(cfg_dict)
            else:
                state["config"] = cfg
    torch.save(state, path)
    print(f'{model} save success! {path}')


def _make_ckpt_path(
        save_dir: Path,
        freq: str,
        model_name: str,
        lookback: int,
        horizon: int,
) -> Path:
    save_dir.mkdir(parents=True, exist_ok=True)
    fname = f"{freq}_{model_name}_L{lookback}_H{horizon}.pt"
    # fname = f"{model_name}.pt"
    return save_dir / fname


def _build_common_train_configs(
        *,
        device: str,
        lookback: int,
        horizon: int,
        freq: str = 'weekly',
        warmup_epochs: Optional[int] = None,
        spike_epochs: Optional[int] = None,
        base_lr: Optional[float] = None,
) -> Tuple[TrainingConfig, TrainingConfig, SpikeLossConfig, Tuple[StageConfig, StageConfig]]:


    # 주기별 학습률/에폭 튜닝
    if warmup_epochs is None:
        if freq == 'hourly':
            warmup_epochs = 20
        elif freq == 'daily':
            warmup_epochs = 30
        else:
            warmup_epochs = 10

    if spike_epochs is None:
        if freq == 'hourly':
            spike_epochs = 50
        elif freq == 'daily':
            spike_epochs = 70
        else:
            spike_epochs = 10

    if base_lr is None:
        if freq == 'hourly':
            base_lr = 1e-4
        elif freq == 'daily':
            base_lr = 2e-4
        else:
            base_lr = 3e-4

    # 1) 2-stage 학습 스케줄
    stg_warmup = StageConfig(
        epochs=warmup_epochs,
        spike_enabled=False,
        lr=base_lr,
        use_horizon_decay=False,
    )

    stg_spike = StageConfig(
        epochs=spike_epochs,
        spike_enabled=True,
        lr=base_lr / 3,
        use_horizon_decay=True,
        tau_h=0.85,
    )
    stages = (stg_warmup, stg_spike)

    # 2) Spike loss 공통 설정
    spike_cfg = SpikeLossConfig(
        enabled=True,
        strategy='mix',
        huber_delta=0.6,
        asym_up_weight=1.0,
        asym_down_weight=8.0,
        mad_k=1.5,
        w_spike=32.0,
        w_norm=1.0,
        alpha_huber=0.6,
        beta_asym=0.4,
        mix_with_baseline=False,
        gamma_baseline=0.0,
    )

    # 3) Point용 공통 TrainingConfig
    point_train_cfg = TrainingConfig(
        device=device,
        lookback=lookback,
        horizon=horizon,
        lr=base_lr,
        weight_decay=1e-3,
        t_max=40,
        patience=100,
        loss_mode='point',
        point_loss='huber',
        huber_delta=0.8,
        use_intermittent=True,
        alpha_zero=3.0,
        alpha_pos=1.0,
        gamma_run=0.3,
        use_horizon_decay=False,
        tau_h=0.85,
        val_use_weights=False,
        spike_loss=spike_cfg,
        max_grad_norm=30.0,
    )

    # 4) Quantile용 공통 TrainingConfig
    quantile_train_cfg = TrainingConfig(
        device=device,
        lookback=lookback,
        horizon=horizon,
        lr=base_lr,
        weight_decay=1e-4,
        t_max=40,
        patience=20,
        loss_mode='quantile',
        quantiles=(0.1, 0.5, 0.9),
        use_intermittent=True,
        alpha_zero=1.2,
        alpha_pos=1.0,
        gamma_run=0.6,
        use_horizon_decay=False,
        tau_h=0.85,
        val_use_weights=False,
        spike_loss=spike_cfg,
        max_grad_norm=30.0,
    )

    return point_train_cfg, quantile_train_cfg, spike_cfg, stages


def _norm_list(xs: Optional[Iterable[str]]) -> List[str]:
    if xs is None:
        return []
    return [str(x).strip().lower() for x in xs if str(x).strip()]

def _contains(xs: List[str], key: str) -> bool:
    return key.lower() in xs


# ---------------------------------------------------------------------
# 모델별 러너: “(freq 공통 계산 결과) + loaders”만 받아서 results를 채움
# ---------------------------------------------------------------------
def _run_patchtst(
        *,
        results: Dict[str, Dict],
        freq: str,
        train_loader, val_loader,
        save_root,
        lookback: int, horizon: int,
        future_exo_cb,
        exo_dim: int,
        patch_len: int, stride: int,
        point_train_cfg, quantile_train_cfg,
        stages,
        device: str,

    use_ssl_pretrain: bool = False,
    ssl_pretrain_epochs: int = 10,
    ssl_mask_ratio: float = 0.3,
    ssl_loss_type: str = "mse",
    ssl_freeze_encoder_before_ft: bool = False,
):
    print(f'exogenous dimension:: {exo_dim}')
    pt_kwargs = dict(
        device=device,
        lookback=lookback,
        horizon=horizon,
        c_in=1,
        d_model=256,
        n_layers=3,
        patch_len=patch_len,
        stride=stride,
        d_future=exo_dim,
        use_revin=True
    )

    # ---------------------------
    # (NEW) 0) self-supervised pretrain (optional)
    # ---------------------------
    pretrain_ckpt_path = None
    if use_ssl_pretrain and save_root is not None:
        pretrain_dir = Path(save_root) / "pretrain"
        pretrain_dir.mkdir(parents=True, exist_ok=True)
        pretrain_ckpt_path = str(pretrain_dir / "patchtst_pretrain_best.pt")

        # pretrain에 쓸 cfg는 PatchTSTConfig 기반으로 동일 backbone 설정을 쓰는 것이 중요
        pt_pre_cfg = PatchTSTConfig(**pt_kwargs, loss_mode="point",
                                    point_loss="mse")  # loss_mode는 실질적으로 pretrain에서 크게 중요하지 않음
        pre_model = PatchTSTPretrainModel(cfg=pt_pre_cfg)

        # pretrain용 TrainingConfig는 point_train_cfg를 재사용해도 되지만,
        # epochs/lr만 명시적으로 바꾸는 것을 권장
        pre_train_cfg = point_train_cfg
        pre_stages = [StageConfig(epochs=ssl_pretrain_epochs, lr=point_train_cfg.lr, spike_enabled=False)]

        print(f"[SSL] PatchTST Pretrain ({freq.capitalize()})")
        _ = train_patchtst_pretrain(
            pre_model,
            train_loader,
            val_loader,
            train_cfg=pre_train_cfg,
            stages=pre_stages,
            mask_ratio=ssl_mask_ratio,
            loss_type=ssl_loss_type,
            save_dir=str(pretrain_dir),
            ckpt_name="patchtst_pretrain_best.pt",
        )

    # ---------------------------
    # 1) PatchTST Base (supervised)  ← 기존 train_patchtst -> finetune로 교체
    # ---------------------------
    pt_base_cfg = PatchTSTConfig(**pt_kwargs, loss_mode='point', point_loss='huber')
    pt_base = build_patchTST_base(pt_base_cfg)

    print(f'PatchTST Base ({freq.capitalize()})')
    if pretrain_ckpt_path is not None:
        best_pt_base = train_patchtst_finetune(
            pt_base, train_loader, val_loader,
            train_cfg=point_train_cfg, stages=list(stages),
            future_exo_cb=future_exo_cb,
            exo_is_normalized=True,
            pretrain_ckpt_path=pretrain_ckpt_path,
            load_strict=False,
            freeze_encoder_before_ft=ssl_freeze_encoder_before_ft,
        )
    else:
        best_pt_base = train_patchtst(
            pt_base, train_loader, val_loader,
            train_cfg=point_train_cfg, stages=list(stages),
            future_exo_cb=future_exo_cb,
        )

    if save_root:
        print('save_root:: ', save_root)
        ckpt_path = _make_ckpt_path(save_root, freq, "PatchTSTBase", lookback, horizon)
        save_model(pt_base, pt_base_cfg, ckpt_path)
        best_pt_base["ckpt_path"] = str(ckpt_path)
        if pretrain_ckpt_path is not None:
            best_pt_base["pretrain_ckpt_path"] = str(pretrain_ckpt_path)
    results['PatchTST Base'] = best_pt_base

    # ---------------------------
    # 2) PatchTST Quantile (supervised)  ← 동일하게 finetune로 교체
    # ---------------------------
    pt_q_cfg = PatchTSTConfig(**pt_kwargs, loss_mode='quantile', quantiles=(0.1, 0.5, 0.9))
    pt_q = build_patchTST_quantile(pt_q_cfg)

    print(f'PatchTST Quantile ({freq.capitalize()})')
    if pretrain_ckpt_path is not None:
        best_pt_q = train_patchtst_finetune(
            pt_q, train_loader, val_loader,
            train_cfg=quantile_train_cfg, stages=list(stages),
            future_exo_cb=future_exo_cb,
            exo_is_normalized=True,
            pretrain_ckpt_path=pretrain_ckpt_path,
            load_strict=False,
            freeze_encoder_before_ft=ssl_freeze_encoder_before_ft,
        )
    else:
        best_pt_q = train_patchtst(
            pt_q, train_loader, val_loader,
            train_cfg=quantile_train_cfg, stages=list(stages),
            future_exo_cb=future_exo_cb,
            exo_is_normalized=True,
        )

    if save_root:
        ckpt_path = _make_ckpt_path(save_root, freq, "PatchTSTQuantile", lookback, horizon)
        save_model(pt_q, pt_q_cfg, ckpt_path)
        best_pt_q["ckpt_path"] = str(ckpt_path)
        if pretrain_ckpt_path is not None:
            best_pt_q["pretrain_ckpt_path"] = str(pretrain_ckpt_path)
    results['PatchTST Quantile'] = best_pt_q

def _run_titan(
        *,
        results: Dict[str, Dict],
        freq: str,
        train_loader, val_loader,
        save_root,
        lookback: int, horizon: int,
        future_exo_cb,
        exo_dim: int,
        patch_len: int, stride: int,
        point_train_cfg, quantile_train_cfg,
        stages,
        device: str,
):
    # --------------------------------------------------
    # 2) Titan
    # --------------------------------------------------
    ti_config = TitanConfig(
        lookback=lookback,
        horizon=horizon,
        input_dim=1,
        d_model=256,
        n_layers=3,
        n_heads=4,
        d_ff=512,
        dropout=0.1,
        contextual_mem_size=256,
        persistent_mem_size=64,
        use_exogenous=True,
        exo_dim=exo_dim,
        final_clamp_nonneg=True,
        use_revin=True,
        revin_use_std=True,
        revin_subtract_last=False,
        revin_affine=True,
        use_lmm=False
    )
    if freq == 'hourly':
        ti_config.contextual_mem_size = 512

    # Titan Base
    ti_base = build_titan_base(ti_config)
    print(f'Titan Base ({freq.capitalize()})')
    best_ti_base = train_titan(
        ti_base, train_loader, val_loader,
        train_cfg=point_train_cfg, stages=list(stages),
        future_exo_cb=future_exo_cb,
    )
    if save_root:
        ckpt_path = _make_ckpt_path(save_root, freq, "TitanBase", lookback, horizon)
        save_model(ti_base, ti_config, ckpt_path)
        best_ti_base["ckpt_path"] = str(ckpt_path)
    results['Titan Base'] = best_ti_base

    # Titan LMM
    ti_config_lmm = replace(ti_config, use_lmm=True)
    ti_lmm = build_titan_lmm(ti_config_lmm)
    print(f'Titan LMM ({freq.capitalize()})')
    best_ti_lmm = train_titan(
        ti_lmm, train_loader, val_loader,
        train_cfg=point_train_cfg, stages=list(stages),
        future_exo_cb=future_exo_cb,
    )
    if save_root:
        ckpt_path = _make_ckpt_path(save_root, freq, "TitanLMM", lookback, horizon)
        save_model(ti_lmm, ti_config_lmm, ckpt_path)
        best_ti_lmm["ckpt_path"] = str(ckpt_path)
    results['Titan LMM'] = best_ti_lmm

    # Titan Seq2Seq
    ti_seq2seq = build_titan_seq2seq(ti_config)
    print(f'Titan Seq2Seq ({freq.capitalize()})')
    best_ti_s2s = train_titan(
        ti_seq2seq, train_loader, val_loader,
        train_cfg=point_train_cfg, stages=list(stages),
        future_exo_cb=future_exo_cb,
    )
    if save_root:
        ckpt_path = _make_ckpt_path(save_root, freq, "TitanSeq2Seq", lookback, horizon)
        save_model(ti_seq2seq, ti_config, ckpt_path)
        best_ti_s2s["ckpt_path"] = str(ckpt_path)
    results['Titan Seq2Seq'] = best_ti_s2s

def _run_patchmixer(
        *,
        results: Dict[str, Dict],
        freq: str,
        train_loader, val_loader,
        save_root,
        lookback: int, horizon: int,
        future_exo_cb,
        exo_dim: int,
        patch_len: int, stride: int,
        point_train_cfg, quantile_train_cfg,
        stages,
        device: str,
):
    # --------------------------------------------------
    # 1) PatchMixer
    # --------------------------------------------------
    # 주기별 Patch Len / Stride 추천값
    if freq == 'hourly':
        patch_len, stride = 24, 12
        season_period = 24
    elif freq == 'daily':
        patch_len, stride = 14, 7
        season_period = 7
    elif freq == 'weekly':
        patch_len, stride = 12, 8
        season_period = 52
    else:  # monthly
        patch_len, stride = 6, 3
        season_period = 12
    # Config kwargs 구성
    pm_kwargs = dict(
        lookback=lookback,
        horizon=horizon,
        device=device,
        enc_in=1,
        d_model=64,
        e_layers=3,
        patch_len=patch_len,
        stride=stride,
        f_out=128,
        head_hidden=128,
        exo_dim=exo_dim,
        use_part_embedding=True,
        part_vocab_size=_get_part_vocab_size_from_loader(train_loader),
        part_embed_dim=16,
        final_nonneg=True,
        use_eol_prior=False,
        exo_is_normalized_default=(freq != 'monthly'),  # 월간만 False 권장
        expander_season_period=season_period,
        expander_n_harmonics=min(season_period // 2, 16),
    )

    # Base
    pm_base_cfg = PatchMixerConfig(**pm_kwargs, loss_mode='point', point_loss='huber', head_dropout=0.05)
    pm_base_model = build_patch_mixer_base(pm_base_cfg)

    print(f'PatchMixer Base ({freq.capitalize()})')
    best_pm_base = train_patchmixer(
        pm_base_model, train_loader, val_loader,
        train_cfg=point_train_cfg, stages=list(stages),
        future_exo_cb=future_exo_cb,
        exo_is_normalized=pm_base_cfg.exo_is_normalized_default,
    )
    if save_root:
        ckpt_path = _make_ckpt_path(save_root, freq, "PatchMixerBase", lookback, horizon)
        save_model(pm_base_model, pm_base_cfg, ckpt_path)
        best_pm_base["ckpt_path"] = str(ckpt_path)
    results['PatchMixer Base'] = best_pm_base

    # Quantile
    pm_q_cfg = PatchMixerConfig(**pm_kwargs, loss_mode='quantile', quantiles=(0.1, 0.5, 0.9), head_dropout=0.02)
    pm_q_model = build_patch_mixer_quantile(pm_q_cfg)

    print(f'PatchMixer Quantile ({freq.capitalize()})')
    best_pm_q = train_patchmixer(
        pm_q_model, train_loader, val_loader,
        train_cfg=quantile_train_cfg, stages=list(stages),
        future_exo_cb=future_exo_cb,
        exo_is_normalized=pm_q_cfg.exo_is_normalized_default,
    )
    if save_root:
        ckpt_path = _make_ckpt_path(save_root, freq, "PatchMixerQuantile", lookback, horizon)
        save_model(pm_q_model, pm_q_cfg, ckpt_path)
        best_pm_q["ckpt_path"] = str(ckpt_path)
    results['PatchMixer Quantile'] = best_pm_q

MODEL_REGISTRY: Dict[str, Callable] = {
    "patchtst": _run_patchtst,
    "titan": _run_titan,
    "patchmixer": _run_patchmixer,
}

# ===================== GENERIC RUNNER (통합) =====================
def _run_total_train_generic(
    train_loader,
    val_loader,
    device: str,
    lookback: int,
    horizon: int,
    freq: str,
    save_dir: Optional[str],
    *,
    models_to_run: Optional[Iterable[str]] = None,
    warmup_epochs: Optional[int] = None,
    spike_epochs: Optional[int] = None,
    base_lr: Optional[float] = None,


    # PatchTST 전용 Property
    use_ssl_pretrain: bool = False,
    ssl_pretrain_epochs: int = 10,
    ssl_mask_ratio: float = 0.3,
    ssl_loss_type: str = "mse",
    ssl_freeze_encoder_before_ft: bool = False
):
    save_root = Path(save_dir) if save_dir is not None else None

    point_train_cfg, quantile_train_cfg, spike_cfg, stages = _build_common_train_configs(
        device=device, lookback=lookback, horizon=horizon, freq=freq,
        warmup_epochs= warmup_epochs, spike_epochs = spike_epochs, base_lr = base_lr
    )

    date_type_map = {"weekly": "W", "monthly": "M", "daily": "D", "hourly": "H"}
    dt_char = date_type_map.get(freq, "W")
    future_exo_cb = compose_exo_calendar_cb(date_type=dt_char)

    exo_dim = 4 if freq in ("daily", "hourly") else 2

    # freq별 patch_len/stride/season_period
    if freq == "hourly":
        patch_len, stride, season_period = 24, 12, 24
    elif freq == "daily":
        patch_len, stride, season_period = 14, 7, 7
    elif freq == "weekly":
        patch_len, stride, season_period = 12, 8, 52
    else:
        patch_len, stride, season_period = 6, 3, 12

    # NEW: 선택값 정규화
    selected = _norm_list(models_to_run)
    if not selected:
        selected = ["patchtst"]  # 기본값(원하시면 전체로 바꿔도 됨)

    # 검증
    unknown = [m for m in selected if m not in MODEL_REGISTRY]
    if unknown:
        raise ValueError(f"Unknown models_to_run: {unknown}. allowed={list(MODEL_REGISTRY.keys())}")

    results: Dict[str, Dict] = {}

    # 선택된 모델만 실행
    for m in selected:
        print(f"\n[total_train] === RUN: {m} ({freq}) ===")
        kwargs = dict(
            results=results,
            freq=freq,
            train_loader=train_loader,
            val_loader=val_loader,
            save_root=save_root,
            lookback=lookback,
            horizon=horizon,
            future_exo_cb=future_exo_cb,
            exo_dim=exo_dim,
            patch_len=patch_len,
            stride=stride,
            point_train_cfg=point_train_cfg,
            quantile_train_cfg=quantile_train_cfg,
            stages=stages,
            device=device,
        )

        if m == "patchtst":
            kwargs.update(dict(
                use_ssl_pretrain=use_ssl_pretrain,
                ssl_pretrain_epochs=ssl_pretrain_epochs,
                ssl_mask_ratio=ssl_mask_ratio,
                ssl_loss_type=ssl_loss_type,
                ssl_freeze_encoder_before_ft=ssl_freeze_encoder_before_ft,
            ))

        MODEL_REGISTRY[m](**kwargs)

    return results


# ===================== EXPORTED FUNCTIONS =====================

def run_total_train_weekly(
        train_loader,
        val_loader,
        device='cuda' if torch.cuda.is_available() else 'cpu',
        *,
        lookback,
        horizon,
        warmup_epochs = None,
        spike_epochs = None,
        base_lr = None,

        save_dir=None,
        models_to_run=None,
        use_ssl_pretrain: bool = False,
        ssl_pretrain_epochs: int = 10,
        ssl_mask_ratio: float = 0.3,
        ssl_loss_type: str = "mse",
        ssl_freeze_encoder_before_ft: bool = False
):
    return _run_total_train_generic(
        train_loader,
        val_loader,
        device,
        lookback,
        horizon,
        'weekly',
        save_dir,
        warmup_epochs= warmup_epochs,
        spike_epochs = spike_epochs,
        base_lr = base_lr,
        models_to_run=models_to_run,
        use_ssl_pretrain=use_ssl_pretrain,
        ssl_pretrain_epochs=ssl_pretrain_epochs,
        ssl_mask_ratio=ssl_mask_ratio,
        ssl_loss_type=ssl_loss_type,
        ssl_freeze_encoder_before_ft=ssl_freeze_encoder_before_ft,
    )


def run_total_train_monthly(
        train_loader,
        val_loader,
        device='cuda' if torch.cuda.is_available() else 'cpu',
        *,
        lookback,
        horizon,
        warmup_epochs = None,
        spike_epochs = None,
        base_lr = None,
        save_dir=None,
        models_to_run=None,
        use_ssl_pretrain: bool = False,
        ssl_pretrain_epochs: int = 10,
        ssl_mask_ratio: float = 0.3,
        ssl_loss_type: str = "mse",
        ssl_freeze_encoder_before_ft: bool = False
):
    return _run_total_train_generic(
        train_loader,
        val_loader,
        device,
        lookback,
        horizon,
        'monthly',
        save_dir,
        warmup_epochs= warmup_epochs,
        spike_epochs = spike_epochs,
        base_lr = base_lr,
        models_to_run=models_to_run,
        use_ssl_pretrain=use_ssl_pretrain,
        ssl_pretrain_epochs=ssl_pretrain_epochs,
        ssl_mask_ratio=ssl_mask_ratio,
        ssl_loss_type=ssl_loss_type,
        ssl_freeze_encoder_before_ft=ssl_freeze_encoder_before_ft,
    )


def run_total_train_daily(
        train_loader,
        val_loader,
        device='cuda' if torch.cuda.is_available() else 'cpu',
        *,
        lookback,
        horizon,
        warmup_epochs = None,
        spike_epochs = None,
        base_lr = None,
        save_dir=None,
        models_to_run=None,
        use_ssl_pretrain: bool = False,
        ssl_pretrain_epochs: int = 10,
        ssl_mask_ratio: float = 0.3,
        ssl_loss_type: str = "mse",
        ssl_freeze_encoder_before_ft: bool = False
):
    return _run_total_train_generic(
        train_loader,
        val_loader,
        device,
        lookback,
        horizon,
        'daily',
        save_dir,
        warmup_epochs= warmup_epochs,
        spike_epochs = spike_epochs,
        base_lr = base_lr,
        models_to_run=models_to_run,
        use_ssl_pretrain=use_ssl_pretrain,
        ssl_pretrain_epochs=ssl_pretrain_epochs,
        ssl_mask_ratio=ssl_mask_ratio,
        ssl_loss_type=ssl_loss_type,
        ssl_freeze_encoder_before_ft=ssl_freeze_encoder_before_ft,
    )


def run_total_train_hourly(
        train_loader,
        val_loader,
        device='cuda' if torch.cuda.is_available() else 'cpu',
        *,
        lookback,
        horizon,
        warmup_epochs = None,
        spike_epochs = None,
        base_lr = None,
        save_dir=None,
        models_to_run=None,
        use_ssl_pretrain: bool = False,
        ssl_pretrain_epochs: int = 10,
        ssl_mask_ratio: float = 0.3,
        ssl_loss_type: str = "mse",
        ssl_freeze_encoder_before_ft: bool = False
):
    return _run_total_train_generic(
        train_loader,
        val_loader,
        device,
        lookback,
        horizon,
        'hourly',
        save_dir,
        warmup_epochs= warmup_epochs,
        spike_epochs = spike_epochs,
        base_lr = base_lr,
        models_to_run=models_to_run,
        use_ssl_pretrain=use_ssl_pretrain,
        ssl_pretrain_epochs=ssl_pretrain_epochs,
        ssl_mask_ratio=ssl_mask_ratio,
        ssl_loss_type=ssl_loss_type,
        ssl_freeze_encoder_before_ft=ssl_freeze_encoder_before_ft,
    )


def summarize_metrics(results: Dict[str, Dict[str, np.ndarray]]) -> Dict[str, Dict[str, float]]:
    table: Dict[str, Dict[str, float]] = {}
    for name, res in results.items():
        y = res['y_true'].reshape(-1)
        yhat = res['y_pred'].reshape(-1)

        row: Dict[str, float] = {
            'MAE': mae(y, yhat),
            'RMSE': rmse(y, yhat),
            'SMAPE': smape(y, yhat),
        }
        if res.get('q_pred') is not None and 0.1 in res['q_pred'] and 0.9 in res['q_pred']:
            result = quantile_metrics(y, yhat)
            row['converage_per_q'] = result['coverage_per_q']
            row['i80_cov'] = result['i80_cov']
            row['i80_wid'] = result['i80_wid']
        table[name] = row
    return table