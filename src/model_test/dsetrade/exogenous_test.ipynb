{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ------------------------------------------------------------\n",
    "# exogenous_train.ipynb - 환경/학습 파라미터/데이터 로드 셋업 셀\n",
    "# 목적:\n",
    "#   - 프로젝트 경로/저장 경로 설정\n",
    "#   - 학습에 사용할 기본 하이퍼파라미터(lookback, horizon, batch_size 등) 정의\n",
    "#   - (선택) 과거/미래 외생변수 설정\n",
    "#   - 학습에 사용할 실데이터 parquet 로드\n",
    "#\n",
    "# 작업자가 이 셀에서 해야 할 일:\n",
    "#   1) dir / save_root 를 본인 PC 경로에 맞게 수정\n",
    "#   2) plan_week(기준 주차)와 lookback/horizon이 실험 의도와 맞는지 확인\n",
    "#   3) df 파일 경로 및 컬럼(id/date/y)이 실제 parquet 스키마와 일치하는지 확인\n",
    "#   4) 외생변수 전략:\n",
    "#       - past_exo_cont_cols / past_exo_cat_cols 를 실제 사용 컬럼으로 채우거나\n",
    "#       - 미래 외생은 future_exo_cb(캘린더 기반)로 생성하는지 결정\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "from modeling_module.utils.exogenous_utils import compose_exo_calendar_cb\n",
    "# - 미래 외생변수(future exogenous) 생성용 콜백을 만들어주는 유틸\n",
    "# - ex) date_type='W', sincos=True이면 주차 기반 sin/cos 캘린더 피처를 horizon 길이만큼 생성\n",
    "\n",
    "import os\n",
    "import time\n",
    "from datetime import date\n",
    "\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# (1) 프로젝트 기본 경로 및 저장 경로 설정\n",
    "# ------------------------------------------------------------\n",
    "dir = \"C:/Users/USER/PycharmProjects/ts_forecaster_lib/raw_data/\"  # 프로젝트 raw_data 루트 경로\n",
    "# - 작업자 변경 포인트:\n",
    "#   - 본인 PC/서버 환경에 맞게 경로를 반드시 수정\n",
    "#   - Windows/리눅스 경로 구분 주의 (예: 리눅스는 /home/... 형태)\n",
    "\n",
    "save_dir = os.path.join(dir, 'fit')\n",
    "# - 학습 결과(모델 ckpt, 로그 등)를 저장할 상위 디렉토리\n",
    "\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "# - save_dir가 없으면 생성 (재실행해도 에러 나지 않도록 exist_ok=True)\n",
    "\n",
    "save_root = os.path.join(save_dir, 'Xpatchtst', '20260119')\n",
    "# - 실험 단위로 구분되는 실제 저장 루트\n",
    "# - 작업자 변경 포인트:\n",
    "#   - 'Xpatchtst'는 실험 카테고리/모델명 prefix로 사용 가능\n",
    "#   - '20260119'는 실험 날짜/버전 태그로 사용 (재현성/추적을 위해 권장)\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# (2) 학습/데이터 기준 시점 및 공통 하이퍼파라미터\n",
    "# ------------------------------------------------------------\n",
    "plan_week = 201103\n",
    "# - 기준 주차(YYYYWW 형태)로 해석되는 값\n",
    "# - 보통 \"예측 시작 주차\" 또는 \"validation cut-off\" 등을 의미할 수 있음\n",
    "# - 작업자 확인 포인트:\n",
    "#   - 이 값이 실제 df의 date 범위와 호환되는지(존재하는 주차인지) 확인\n",
    "#   - 코드 어딘가에서 plan_week를 실제 split/샘플링 기준으로 사용한다면 매우 중요\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# - GPU 사용 가능하면 cuda, 아니면 cpu로 학습\n",
    "# - 작업자 확인 포인트:\n",
    "#   - 서버에서 CUDA 인식이 안 되면 torch.cuda.is_available()가 False로 떨어짐\n",
    "#   - 예상과 다르면 드라이버/CUDA/PyTorch 설치 확인 필요\n",
    "\n",
    "lookback = 52\n",
    "# - 과거 입력 길이 (weekly면 52주 히스토리 사용)\n",
    "\n",
    "horizon = 27\n",
    "# - 미래 예측 길이 (weekly면 27주 ahead 예측)\n",
    "\n",
    "batch_size = 256\n",
    "# - 배치 크기\n",
    "# - 작업자 조정 포인트:\n",
    "#   - GPU 메모리 부족(OOM) 시 128/64로 감소\n",
    "#   - 안정적 학습이 필요하면 너무 큰 배치는 오히려 학습이 둔해질 수 있음(실험적으로 조정)\n",
    "\n",
    "freq = 'weekly'\n",
    "# - 데이터 주기\n",
    "# - DataModule / date util이 freq에 따라 date 처리 로직이 달라질 수 있음\n",
    "\n",
    "split_mode = 'multi'\n",
    "# - 데이터 split 전략 (프로젝트 구현체에 따라 의미가 정의됨)\n",
    "# - 일반적으로:\n",
    "#   - 'multi': 여러 시계열을 함께 학습/평가하는 모드\n",
    "#   - 'single': 특정 1개 시계열/파트만 대상으로 하는 모드\n",
    "# - 작업자 확인 포인트:\n",
    "#   - 프로젝트에서 split_mode별 train/val 분할 방식이 다르면 결과가 크게 달라짐\n",
    "\n",
    "shuffle = True\n",
    "# - DataLoader shuffle 여부\n",
    "# - 일반적으로 train은 True, val은 False가 권장\n",
    "# - 단, 시계열 윈도우 샘플링 방식에 따라 shuffle이 실험 재현성에 영향을 줄 수 있음\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# (3) 데이터 컬럼명 정의 (df 스키마와 반드시 일치해야 함)\n",
    "# ------------------------------------------------------------\n",
    "id_col = 'unique_id'\n",
    "# - 각 시계열을 구분하는 ID 컬럼명 (예: 상품/점포/파트 번호 등)\n",
    "\n",
    "date_col = 'date'\n",
    "# - 시간 축 컬럼명\n",
    "# - weekly면 YYYYWW(int) 혹은 date 타입이 올 수 있음\n",
    "# - 프로젝트 DataModule이 요구하는 타입/포맷(정수 YYYYWW vs date)을 사전에 확인 필요\n",
    "\n",
    "y_col = 'y'\n",
    "# - 타깃(수요/매출) 컬럼명\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# (4) 외생변수(Exogenous) 설정\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# add past exogenous continuous variable columns\n",
    "past_exo_cont_cols = ()\n",
    "# - 과거 구간(lookback)에 대해 함께 입력으로 넣을 \"연속형\" 외생변수 컬럼명 리스트/튜플\n",
    "# - 예: ('price', 'promo_rate', 'stock', ...)\n",
    "# - 현재는 비어있으므로 과거 외생은 사용하지 않음\n",
    "# - 작업자 변경 포인트:\n",
    "#   - df에 해당 컬럼이 존재할 때만 추가\n",
    "#   - 스케일/정규화 여부(특히 가격/재고 등)도 함께 고려\n",
    "\n",
    "# add past exogenous categorical variable columns\n",
    "past_exo_cat_cols = ()\n",
    "# - 과거 구간에 대해 함께 입력으로 넣을 \"범주형\" 외생변수 컬럼명 리스트/튜플\n",
    "# - 예: ('store_type', 'region', ...)\n",
    "# - 일반적으로 embedding으로 들어가며, cardinality 관리가 필요할 수 있음\n",
    "\n",
    "future_exo_cb = compose_exo_calendar_cb(date_type='W', sincos=True)\n",
    "# - 미래 구간(horizon)에 대해 date로부터 외생을 생성하는 callable 콜백\n",
    "# - date_type='W': 주차 기반 캘린더 피처 생성\n",
    "# - sincos=True : 주기성 정보를 sin/cos로 인코딩 (값 범위 대체로 [-1, 1])\n",
    "# - 작업자 확인 포인트:\n",
    "#   - DataLoader/Collate가 실제로 이 콜백을 사용하도록 연결되어 있는지(= future exo 활성화)\n",
    "#   - 모델이 미래 외생을 받을 구조(exo_head 등)를 제대로 빌드하는지\n",
    "#   - fe_dim이 0으로 나오는 경우: 콜백이 비활성/무시되고 있을 수 있음\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# (5) 실데이터 로드\n",
    "# ------------------------------------------------------------\n",
    "df = pl.read_parquet()\n",
    "# - 학습에 사용할 parquet 데이터 로드\n",
    "# - 작업자 확인 포인트(필수):\n",
    "#   1) 파일 경로가 실제 존재하는지\n",
    "#   2) df 컬럼에 id_col/date_col/y_col이 존재하는지\n",
    "#   3) weekly 데이터면 date_col이 YYYYWW로 잘 구성되어 있는지(누락/형변환 이슈)\n",
    "#   4) 외생 컬럼을 추가할 계획이면, 해당 컬럼들이 df에 포함되어 있는지\n",
    "#\n",
    "# 권장(추가 점검 예시):\n",
    "#   - print(df.shape, df.columns)\n",
    "#   - df.select([id_col, date_col, y_col]).head()\n"
   ],
   "id": "c601d613f3e716b9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from modeling_module.training.forecater import DMSForecaster\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import torch\n",
    "from datetime import date, timedelta\n",
    "\n",
    "# ============================================================\n",
    "# [셀 목적]\n",
    "# 1) YYYYWW(ISO Week) 형태의 주차 정수 ↔ 날짜(date) 변환 유틸 제공\n",
    "# 2) inference_loader에서 배치를 받아\n",
    "#    - base_model(포인트 예측)\n",
    "#    - quantile_model(q50 예측)\n",
    "#    을 각각 추론한 뒤\n",
    "#    (oper_part_no, forecast_week) 단위의 \"long-format 결과 테이블\"을 생성\n",
    "#\n",
    "# [작업자가 이 셀에서 해야 할 일]\n",
    "# A. inference_loader 배치 구조가 아래 가정과 동일한지 확인\n",
    "#    - 현재 가정: (x, uid, fe, pe_cont, pe_cat)\n",
    "#      * 과거 코드에서 train loader는 (x, y, uid, fe, pe_cont, pe_cat)였음\n",
    "#      * inference용 loader는 y가 없을 수 있으니 이 형태로 가정\n",
    "#    - 만약 loader가 uid를 tensor가 아닌 string list로 준다면 pid 처리 로직 수정 필요\n",
    "#\n",
    "# B. plan_week의 의미/정의를 명확히 유지\n",
    "#    - \"예측 기준 주차\"로 보고, forecast_week = plan_week + h (h=0..H-1)로 생성\n",
    "#    - 만약 현업에서 plan_week + 1부터 예측해야 하면 range 시작을 1로 바꿔야 함\n",
    "#\n",
    "# C. 모델 입력에 외생변수(fe/pe_cont/pe_cat)를 실제로 넣을지 결정\n",
    "#    - loader에서 fe_dim=0이면 future_exo_batch가 빈 텐서일 수 있음\n",
    "#    - 모델이 exo를 기대하는데 fe가 비어있으면 성능/동작에 영향\n",
    "#\n",
    "# D. 성능/속도 이슈 점검\n",
    "#    - 현재 구현은 배치 내 각 샘플을 1개씩(1-step) predict 호출 -> 느릴 수 있음\n",
    "#    - 대량 inference는 \"배치 단위로 predict\"로 리팩토링 권장\n",
    "# ============================================================\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# YYYYWW(ISO week) 유틸\n",
    "# -----------------------------\n",
    "def yyyyww_to_monday(yyyyww: int) -> date:\n",
    "    \"\"\"\n",
    "    YYYYWW(정수) -> 해당 ISO week의 월요일(date)로 변환.\n",
    "\n",
    "    예) 202601 -> 2026년 ISO week 1의 월요일 날짜\n",
    "\n",
    "    [주의]\n",
    "    - ISO week는 연말/연초에 '연도'가 달라질 수 있음(ISO year 기준)\n",
    "      예: 2025년 12월 말이 ISO 기준으로 2026년 1주에 포함될 수 있음\n",
    "    - 입력 주차가 존재하지 않는 값이면 date.fromisocalendar에서 ValueError 발생\n",
    "    \"\"\"\n",
    "    y = int(yyyyww) // 100\n",
    "    w = int(yyyyww) % 100\n",
    "    return date.fromisocalendar(y, w, 1)  # ISO week에서 월요일=1\n",
    "\n",
    "\n",
    "def monday_to_yyyyww(d: date) -> int:\n",
    "    \"\"\"\n",
    "    date -> YYYYWW(정수)로 변환(ISO year-week 기준)\n",
    "\n",
    "    예) 2026-01-05(월) -> 202601\n",
    "    \"\"\"\n",
    "    iso_y, iso_w, _ = d.isocalendar()\n",
    "    return int(iso_y) * 100 + int(iso_w)\n",
    "\n",
    "\n",
    "def add_week(yyyyww: int, add: int) -> int:\n",
    "    \"\"\"\n",
    "    YYYYWW에 주(week) 단위로 add만큼 더한 YYYYWW 반환.\n",
    "\n",
    "    예) add_week(202601, 1) -> 202602\n",
    "\n",
    "    [작업자 변경 가능 포인트]\n",
    "    - 예측 결과를 plan_week 포함(H개) vs plan_week+1부터(H개)로 만들지 정책에 따라 달라짐\n",
    "    \"\"\"\n",
    "    return monday_to_yyyyww(yyyyww_to_monday(yyyyww) + timedelta(weeks=int(add)))\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 결과 테이블 생성 함수\n",
    "# -----------------------------\n",
    "@torch.no_grad()\n",
    "def make_forecast_result_table(\n",
    "    *,\n",
    "    inference_loader,\n",
    "    base_model: torch.nn.Module,\n",
    "    quantile_model: torch.nn.Module,\n",
    "    plan_week: int,\n",
    "    horizon: int,\n",
    "    device: str = \"cuda\",\n",
    "    max_parts: int = 10_000,          # 필요시 제한\n",
    ") -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    [기능 요약]\n",
    "    inference_loader에서 배치를 순회하며 각 시계열(파트/스토어 등)별로\n",
    "    - base_model: point forecast\n",
    "    - quantile_model: q50 forecast\n",
    "    을 생성하고,\n",
    "    (plan_week, oper_part_no, forecast_week) 단위 long table로 반환한다.\n",
    "\n",
    "    [입력 가정]\n",
    "    inference_loader는 batch마다 아래 형태를 반환한다고 가정:\n",
    "        (x, uid, fe, pe_cont, pe_cat)\n",
    "    - x: (B, L, C)  과거 타깃(및 채널)\n",
    "    - uid: (B,)     각 샘플 식별자\n",
    "    - fe: (B, H, E) 미래 외생(future exo). 없으면 None 또는 shape[-1]=0 가능\n",
    "    - pe_cont: (B, L, E_past_cont) 과거 연속 외생\n",
    "    - pe_cat: (B, L, E_past_cat)   과거 범주 외생\n",
    "\n",
    "    [출력]\n",
    "    polars DataFrame, long format:\n",
    "        plan_week: int\n",
    "        oper_part_no: str\n",
    "        forecast_week: int\n",
    "        base_forecast: float\n",
    "        quantile_forecast: float\n",
    "\n",
    "    [작업자 주의사항]\n",
    "    1) plan_week 포함 여부:\n",
    "       - 현재 weeks = [plan_week + 0, ..., plan_week + H-1]\n",
    "       - 만약 \"예측은 다음 주부터\"라면 range를 1..H 로 바꿔야 함\n",
    "\n",
    "    2) uid 타입:\n",
    "       - uid가 torch.Tensor scalar면 pid.item() 사용 가능\n",
    "       - uid가 이미 문자열이면 그대로 str(uid) 처리해야 함\n",
    "\n",
    "    3) 성능:\n",
    "       - 배치 내 샘플을 i 루프로 1개씩 예측 -> 느림\n",
    "       - production/대량 inference면 DMSForecaster가 batch 입력을 받도록 개선 권장\n",
    "    \"\"\"\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # (1) Forecaster 래핑\n",
    "    # --------------------------------------------------------\n",
    "    # DMSForecaster는 모델 추론을 위한 공통 wrapper로 보이며,\n",
    "    # - target_channel=0: y 채널이 다변량일 때 어떤 채널을 예측 대상으로 볼지 지정\n",
    "    # - fill_mode=\"copy_last\": 입력 부족/결측 시 마지막 값 복사 등 보정 전략 (프로젝트 정책 확인)\n",
    "    base_fc = DMSForecaster(base_model, target_channel=0, fill_mode=\"copy_last\")\n",
    "    q_fc    = DMSForecaster(quantile_model, target_channel=0, fill_mode=\"copy_last\")\n",
    "\n",
    "    rows = []\n",
    "    n_parts = 0\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # (2) inference_loader 순회\n",
    "    # --------------------------------------------------------\n",
    "    for batch in inference_loader:\n",
    "        # [중요] 여기서 배치 언패킹 형태가 loader 구현과 다르면 바로 에러 납니다.\n",
    "        # - 만약 loader가 (x, y, uid, fe, pe_cont, pe_cat) 형태면 아래 줄을 수정해야 합니다.\n",
    "        x, uid, fe, pe_cont, pe_cat = batch\n",
    "\n",
    "        # B: 배치 크기\n",
    "        B = x.size(0)\n",
    "\n",
    "        # ----------------------------------------------------\n",
    "        # (3) 배치 내 샘플(=파트/시계열) 단위 순회\n",
    "        # ----------------------------------------------------\n",
    "        for i in range(B):\n",
    "            # max_parts를 넘기면 early stop\n",
    "            if n_parts >= max_parts:\n",
    "                break\n",
    "\n",
    "            # (3-1) 샘플 1개만 분리하여 (1, L, C) 형태로 만듦\n",
    "            # - DMSForecaster.predict가 batch=1 입력을 받아 처리하도록 구성된 것으로 가정\n",
    "            x1 = x[i:i+1]\n",
    "\n",
    "            # (3-2) 파트/시계열 ID 추출\n",
    "            # - uid가 tensor scalar라면 uid[i]는 tensor(0-d) 또는 tensor(1,)일 수 있음\n",
    "            # - uid가 str/list면 uid[i] 자체가 string일 수 있음\n",
    "            pid = uid[i]  # oper_part_no로 사용(필요하면 mapping)\n",
    "\n",
    "            # (3-3) 외생변수도 같은 인덱스로 1개 샘플만 분리\n",
    "            # - fe/pe_cont/pe_cat가 None이면 그대로 None\n",
    "            fe1  = fe[i:i+1] if fe is not None else None\n",
    "            pec1 = pe_cont[i:i+1] if pe_cont is not None else None\n",
    "            pek1 = pe_cat[i:i+1] if pe_cat is not None else None\n",
    "\n",
    "            # ------------------------------------------------\n",
    "            # (4) Base(Point) 예측\n",
    "            # ------------------------------------------------\n",
    "            pred_base = base_fc.predict(\n",
    "                x1,\n",
    "                horizon=int(horizon),\n",
    "                device=device,\n",
    "                mode=\"eval\",\n",
    "                # part_ids는 forecaster 내부에서 로깅/조건 분기 등에 사용될 수 있음\n",
    "                # - uid 타입이 tensor면 list에 tensor가 들어갈 수 있어 내부에서 str 변환 필요할 수 있음\n",
    "                part_ids=[pid] if pid is not None else None,\n",
    "                # 과거 외생\n",
    "                past_exo_cont=pec1,\n",
    "                past_exo_cat=pek1,\n",
    "                # 미래 외생 (H, E)\n",
    "                future_exo_batch=fe1,\n",
    "            )\n",
    "\n",
    "            # pred_base[\"point\"]를 (H,)로 변환\n",
    "            # - 반환 shape이 (1, H) or (H,) 등 다양할 수 있으므로 reshape(-1)로 평탄화\n",
    "            y_base = np.asarray(pred_base[\"point\"]).reshape(-1)\n",
    "\n",
    "            # ------------------------------------------------\n",
    "            # (5) Quantile(q50) 예측\n",
    "            # ------------------------------------------------\n",
    "            pred_q = q_fc.predict(\n",
    "                x1,\n",
    "                horizon=int(horizon),\n",
    "                device=device,\n",
    "                mode=\"eval\",\n",
    "                part_ids=[pid] if pid is not None else None,\n",
    "                past_exo_cont=pec1,\n",
    "                past_exo_cat=pek1,\n",
    "                future_exo_batch=fe1,\n",
    "            )\n",
    "\n",
    "            # - quantile 모델은 q50 키가 있을 수도 있고 없을 수도 있음(프로젝트 구현체에 따라 다름)\n",
    "            # - q50이 없으면 point를 fallback으로 사용\n",
    "            y_q50 = np.asarray(pred_q.get(\"q50\", pred_q[\"point\"])).reshape(-1)\n",
    "\n",
    "            # ------------------------------------------------\n",
    "            # (6) forecast_week 생성\n",
    "            # ------------------------------------------------\n",
    "            # 현재는 plan_week 포함하여 horizon개 생성:\n",
    "            #   [plan_week, plan_week+1, ..., plan_week+(H-1)]\n",
    "            # 정책상 \"다음주부터\" 예측이면:\n",
    "            #   weeks = [add_week(plan_week, h) for h in range(1, H+1)]\n",
    "            weeks = [add_week(plan_week, h) for h in range(int(horizon))]\n",
    "\n",
    "            # ------------------------------------------------\n",
    "            # (7) long-format row 적재\n",
    "            # ------------------------------------------------\n",
    "            # uid가 tensor(1개 원소)라면 pid.item()으로 파이썬 스칼라 변환 후 문자열화\n",
    "            # uid가 이미 문자열이면 str(pid)로 충분\n",
    "            pid_str = str(pid.item()) if torch.is_tensor(pid) and pid.numel() == 1 else str(pid)\n",
    "\n",
    "            # weeks, y_base, y_q50 길이가 모두 horizon인지 확인 필요\n",
    "            # - 불일치 시 zip이 짧은 쪽에 맞춰 잘리므로 조용히 데이터가 누락될 수 있음\n",
    "            for w, bval, qval in zip(weeks, y_base.tolist(), y_q50.tolist()):\n",
    "                rows.append({\n",
    "                    \"plan_week\": int(plan_week),\n",
    "                    \"oper_part_no\": pid_str,\n",
    "                    \"forecast_week\": int(w),\n",
    "                    \"base_forecast\": float(bval),\n",
    "                    \"quantile_forecast\": float(qval),\n",
    "                })\n",
    "\n",
    "            n_parts += 1\n",
    "\n",
    "        if n_parts >= max_parts:\n",
    "            break\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # (8) polars DataFrame 반환\n",
    "    # --------------------------------------------------------\n",
    "    # rows가 비어있으면 빈 DF가 생성됨\n",
    "    return pl.DataFrame(rows)\n"
   ],
   "id": "607ca19d02bc4d13"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from modeling_module.utils.checkpoint import load_model_dict\n",
    "from modeling_module.models import build_patchTST_quantile, build_patchTST_base\n",
    "from modeling_module.data_loader import MultiPartExoDataModule\n",
    "from modeling_module.training.model_trainers.total_train import run_total_train_weekly\n",
    "\n",
    "# ============================================================\n",
    "# [셀 목적]\n",
    "# 1) \"특정 plan_week 시점\" 기준으로 inference_loader를 생성한다.\n",
    "# 2) 저장된 체크포인트(.pt 등)를 로드하여\n",
    "#    - base_model (PatchTST point forecast)\n",
    "#    - quantile_model (PatchTST quantile forecast: q10/q50/q90 등)\n",
    "#    을 준비한다.\n",
    "# 3) make_forecast_result_table()을 호출해\n",
    "#    long-format 결과 테이블(pl.DataFrame)을 만든다.\n",
    "#\n",
    "# [작업자가 이 셀에서 확인/수정해야 할 핵심 포인트]\n",
    "# A. inference_loader 배치 구조 확인\n",
    "#    - 여기서는 (x, uid, fe, pe_cont, pe_cat)로 가정하고 있음\n",
    "#    - 만약 (x, y, uid, fe, pe_cont, pe_cat)라면 inspect()와 아래 로직 수정 필요\n",
    "#\n",
    "# B. plan_week(plan_dt)의 의미/정합성\n",
    "#    - get_inference_loader_at_plan(plan_dt=plan_week) 호출에서 plan_dt가 \"YYYYWW\"임을 전제로 함\n",
    "#    - 내부 구현이 YYYYMMDD 또는 다른 정수 포맷이면 compose_exo_calendar_cb(date_type='W')와 충돌 가능\n",
    "#\n",
    "# C. save_root 경로 및 builders key 일치 여부\n",
    "#    - load_model_dict()가 디렉토리 구조/파일명 규칙에 의존할 가능성이 큼\n",
    "#    - builders의 key('patchtst', 'patchtst_quantile')가 체크포인트 메타/파일명과 매칭되는지 확인\n",
    "#\n",
    "# D. device 일관성\n",
    "#    - model_loader에서 device로 로드했더라도, make_forecast_result_table에서 device를 동일하게 전달해야 함\n",
    "#\n",
    "# E. 외생변수(exogenous) 사용 여부\n",
    "#    - data_module에 future_exo_cb를 넘기고 있으므로 fe가 생성될 수 있음\n",
    "#    - 모델이 exo 입력을 기대하는 설정인지(model config)와 일치해야 함\n",
    "# ============================================================\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# (0) 체크포인트 로더/빌더 준비\n",
    "# ------------------------------------------------------------\n",
    "# load_model_dict:\n",
    "# - save_root 아래에 저장된 여러 모델 체크포인트를 한 번에 로드하는 유틸로 추정\n",
    "# - builders: {모델이름: build_fn} 형태로 \"아키텍처 생성 함수\"를 넘겨주면,\n",
    "#            체크포인트의 state_dict를 해당 아키텍처에 로드하여 반환하는 형태일 가능성이 높음\n",
    "#\n",
    "# build_patchTST_base:\n",
    "# - point forecast(단일 값)용 PatchTST 생성\n",
    "# build_patchTST_quantile:\n",
    "# - quantile forecast(q10/q50/q90 등)용 PatchTST 생성\n",
    "#\n",
    "from modeling_module.utils.checkpoint import load_model_dict\n",
    "from modeling_module.models import build_patchTST_quantile, build_patchTST_base\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# (1) inference_loader 구성: MultiPartExoDataModule\n",
    "# ------------------------------------------------------------\n",
    "# MultiPartExoDataModule:\n",
    "# - df를 (id_col, date_col) 기준으로 시계열로 묶어 샘플링\n",
    "# - lookback 길이의 과거 입력(x)과 horizon 길이의 미래 구간(y or placeholder)을 구성\n",
    "# - future_exo_cb가 있으면 horizon 구간의 미래 외생(fe)을 생성/결합\n",
    "#\n",
    "# [작업자 체크]\n",
    "# - df, id_col/date_col/y_col, lookback/horizon 등은 \"이 셀 이전\"에서 정의되어 있어야 함\n",
    "# - past_exo_cont_cols/past_exo_cat_cols가 빈 tuple이면 pe_cont/pe_cat이 0차원일 수 있음\n",
    "#\n",
    "from modeling_module.data_loader import MultiPartExoDataModule\n",
    "\n",
    "def inspect(loader, name):\n",
    "    \"\"\"\n",
    "    loader가 실제로 어떤 텐서를 뱉는지 빠르게 확인하는 디버그 함수.\n",
    "\n",
    "    [가정]\n",
    "    - inference_loader는 (x, uid, fe, pe_cont, pe_cat) 형태를 반환한다고 가정\n",
    "      * 학습 loader는 보통 y가 포함되지만, inference에서는 y 없이 구성하는 경우가 많음\n",
    "    - x: (B, L, C)\n",
    "    - uid: (B,) 또는 list[str]\n",
    "    - fe: (B, H, E)\n",
    "    - pe_cont: (B, L, E_past_cont)\n",
    "    - pe_cat: (B, L, E_past_cat)\n",
    "\n",
    "    [작업자 참고]\n",
    "    - 만약 여기서 unpacking 에러가 나면 loader 배치 구조가 다른 것임\n",
    "      -> batch print 후 구조에 맞게 수정 필요\n",
    "    \"\"\"\n",
    "    b = next(iter(loader))\n",
    "\n",
    "    # inference_loader 배치 구조 언패킹\n",
    "    x, uid, fe, pe_cont, pe_cat = b\n",
    "\n",
    "    print(f\"[{name}] x:\", x.shape, x.device, x.dtype)\n",
    "    print(f\"[{name}] fe:\", fe.shape, fe.device, fe.dtype)\n",
    "    print(f\"[{name}] pe:\", pe_cont.shape, pe_cont.device, pe_cont.dtype)\n",
    "\n",
    "    # train_loader에서는 loader.collate_fn.future_exo_cb를 체크했으나\n",
    "    # inference_loader 구현에 따라 collate_fn 속성이 다를 수 있어 주석 처리\n",
    "    # print(f\"[{name}] future_exo_cb is None?\", loader.collate_fn.future_exo_cb is None)\n",
    "\n",
    "    # fe_dim > 0이면 실제 값 일부를 출력해 \"미래 외생이 채워졌는지\" 확인\n",
    "    # - sin/cos 달력 외생이라면 [-1, 1] 범위의 값이 나오는 것이 일반적\n",
    "    if fe.shape[-1] > 0:\n",
    "        print(f\"[{name}] fe sample:\", fe[0, :3, :])\n",
    "\n",
    "\n",
    "data_module = MultiPartExoDataModule(\n",
    "    df=df,\n",
    "    id_col=id_col,\n",
    "    date_col=date_col,\n",
    "    y_col=y_col,\n",
    "    lookback=lookback,\n",
    "    horizon=horizon,\n",
    "    batch_size=batch_size,\n",
    "    past_exo_cont_cols=past_exo_cont_cols,\n",
    "    past_exo_cat_cols=past_exo_cat_cols,\n",
    "    future_exo_cb=future_exo_cb,   # 달력 기반 미래 외생 생성 콜백(weekly + sin/cos)\n",
    "    freq=freq,                     # 'weekly' 등. future_exo_cb의 date_type과 정합성 필수\n",
    "    shuffle=shuffle,\n",
    "    split_mode=split_mode,         # 'multi' 등. 데이터 분할/샘플링 전략\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# (2) 특정 plan_week 기준 inference_loader 생성\n",
    "# ------------------------------------------------------------\n",
    "# get_inference_loader_at_plan(plan_dt=plan_week):\n",
    "# - \"plan_week 시점\"에 대해 예측에 필요한 x/fe/pe_*를 구성해서 loader 반환\n",
    "#\n",
    "# [작업자 체크]\n",
    "# - plan_week가 YYYYWW로 들어가고, 내부도 이를 기대해야 함\n",
    "# - plan_week 시점에 해당하는 lookback 히스토리가 충분히 존재하는지 확인 필요\n",
    "#\n",
    "inference_loader = data_module.get_inference_loader_at_plan(plan_dt=plan_week)\n",
    "\n",
    "# 필요 시 배치 구조를 직접 확인(문제 발생 시 주석 해제)\n",
    "# for batch in inference_loader:\n",
    "#     print(batch)\n",
    "#     break\n",
    "\n",
    "# 배치 shape 확인\n",
    "inspect(inference_loader, 'inference_loader')\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# (3) 모델 빌더 매핑 구성\n",
    "# ------------------------------------------------------------\n",
    "# builders:\n",
    "# - load_model_dict가 체크포인트에 저장된 \"모델 이름/키\"에 따라\n",
    "#   해당 아키텍처를 생성할 수 있도록 build 함수를 제공\n",
    "#\n",
    "# [작업자 체크]\n",
    "# - builders의 key 문자열이 \"저장된 체크포인트의 식별자\"와 정확히 일치해야 함\n",
    "#   예) 저장할 때 model_name='patchtst_quantile'로 저장했다면 동일해야 로드됨\n",
    "#\n",
    "builders = {\n",
    "    'patchtst_quantile': build_patchTST_quantile,\n",
    "    'patchtst': build_patchTST_base,\n",
    "}\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# (4) 체크포인트 로드\n",
    "# ------------------------------------------------------------\n",
    "# load_model_dict(save_root, builders, device=device):\n",
    "# - save_root 아래에서 체크포인트들을 찾아서 로드\n",
    "# - 반환값 예: {'patchtst': nn.Module, 'patchtst_quantile': nn.Module, ...}\n",
    "#\n",
    "# [작업자 체크]\n",
    "# - save_root 경로가 \"학습에서 저장한 경로\"와 동일한지 확인\n",
    "# - device='cuda'인 경우, 로드 과정에서 map_location이 적절히 처리되는지 확인\n",
    "#\n",
    "model_loader = load_model_dict(save_root, builders, device=device)\n",
    "\n",
    "# base_model: 포인트 예측 모델\n",
    "base_model = model_loader['patchtst']\n",
    "\n",
    "# quantile_model: q50 등 분위수 예측 모델\n",
    "quantile_model = model_loader['patchtst_quantile']\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# (5) 예측 결과 테이블 생성\n",
    "# ------------------------------------------------------------\n",
    "# make_forecast_result_table:\n",
    "# - inference_loader를 순회하며 각 uid(oper_part_no)별로\n",
    "#   base_forecast(포인트)와 quantile_forecast(q50)를 계산 후 long-format DF 반환\n",
    "#\n",
    "# [작업자 체크]\n",
    "# - make_forecast_result_table 내부에서 \"plan_week 포함 여부\" 정책 확인 필요\n",
    "# - max_parts는 대량 결과 생성 시 메모리/시간을 제한하기 위한 장치\n",
    "#\n",
    "result_df = make_forecast_result_table(\n",
    "    inference_loader=inference_loader,\n",
    "    base_model=base_model,\n",
    "    quantile_model=quantile_model,\n",
    "    plan_week=plan_week,\n",
    "    horizon=horizon,\n",
    "    device=device,\n",
    "    max_parts=10_000,\n",
    ")\n",
    "\n",
    "# (선택) 결과 확인 예시\n",
    "# print(result_df.head())\n",
    "# print(result_df.shape)\n"
   ],
   "id": "acb5835fffa9a98e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "result_df",
   "id": "d231c7de7ba70792",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
