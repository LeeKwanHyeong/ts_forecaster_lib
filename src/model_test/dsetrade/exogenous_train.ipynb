{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Preparation",
   "id": "92d2ae70b754f3ad"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-01-19T02:08:40.801133Z",
     "start_time": "2026-01-19T02:08:39.593208Z"
    }
   },
   "source": [
    "import os\n",
    "import time\n",
    "from datetime import date\n",
    "\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "\n",
    "dir = \"C:/Users/USER/PycharmProjects/ts_forecaster_lib/raw_data/\"        # default project directory\\\n",
    "save_dir = os.path.join(dir, 'fit')\n",
    "os.makedirs(save_dir, exist_ok = True)\n",
    "save_root = os.path.join(save_dir, 'Xpatchtst', '20260119')\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "lookback = 52\n",
    "horizon = 27\n",
    "batch_size = 256\n",
    "freq = 'weekly'\n",
    "split_mode = 'multi'\n",
    "shuffle = True\n",
    "id_col = 'unique_id'\n",
    "date_col = 'date'\n",
    "y_col = 'y'\n",
    "\n",
    "# add past exogenous continuous variable columns\n",
    "past_exo_cont_cols = (\n",
    "    # \"exo_p_y_lag_1w\",\n",
    "    \"exo_p_y_lag_2w\",\n",
    "    # \"exo_p_y_lag_52w\",\n",
    "    \"exo_p_y_rollmean_4w\",\"exo_p_y_rollmean_12w\",\"exo_p_y_rollstd_4w\",\n",
    "    # \"exo_p_weeks_since_holiday\",\n",
    "    # \"exo_p_temperature\",\n",
    "    # \"exo_p_fuel_price\",\n",
    "    # \"exo_p_cpi\",\n",
    "    # \"exo_p_unemployment\",\n",
    "    # \"exo_p_markdown_sum\",\n",
    "    # \"exo_p_markdown1\",\n",
    "    # \"exo_p_markdown2\",\n",
    "    # \"exo_p_markdown3\",\n",
    "    # \"exo_p_markdown4\",\n",
    "    # \"exo_p_markdown5\",\n",
    "    # \"exo_markdown1_isnull\",\n",
    "    # \"exo_markdown2_isnull\",\n",
    "    # \"exo_markdown3_isnull\",\n",
    "    # \"exo_markdown4_isnull\",\n",
    "    # \"exo_markdown5_isnull\",\n",
    ")\n",
    "\n",
    "# add past exogenous categorical variable columns\n",
    "past_exo_cat_cols = (\n",
    "    # \"exo_c_woy_bucket\",\n",
    ")\n",
    "\n",
    "future_exo_cb = None\n",
    "\n",
    "# real dataframe\n",
    "df = pl.read_parquet(dir + 'train_data/walmart_best_feature_train.parquet')"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T02:08:56.575064Z",
     "start_time": "2026-01-19T02:08:41.340584Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from modeling_module.training.model_trainers.total_train import run_total_train_weekly\n",
    "from modeling_module.data_loader import MultiPartExoDataModule\n",
    "\n",
    "def inspect(loader, name):\n",
    "    b = next(iter(loader))\n",
    "    x, y, uid, fe, pe_cont, pe_cat = b\n",
    "    print(f\"[{name}] x:\", x.shape, x.device, x.dtype)\n",
    "    print(f\"[{name}] fe:\", fe.shape, fe.device, fe.dtype)\n",
    "    print(f\"[{name}] pe:\", pe_cont.shape, pe_cont.device, pe_cont.dtype)\n",
    "    print(f\"[{name}] future_exo_cb is None?\", loader.collate_fn.future_exo_cb is None)\n",
    "    if fe.shape[-1] > 0:\n",
    "        print(f\"[{name}] fe sample:\", fe[0, :3, :])\n",
    "\n",
    "data_module = MultiPartExoDataModule(\n",
    "    df = df,\n",
    "    id_col = id_col,\n",
    "    date_col = date_col,\n",
    "    y_col = y_col,\n",
    "    lookback = lookback,\n",
    "    horizon = horizon,\n",
    "    batch_size = batch_size,\n",
    "    past_exo_cont_cols = past_exo_cont_cols,\n",
    "    past_exo_cat_cols = past_exo_cat_cols,\n",
    "    future_exo_cb = future_exo_cb,\n",
    "    freq = freq,\n",
    "    shuffle = shuffle,\n",
    "    split_mode = split_mode,\n",
    ")\n",
    "\n",
    "train_loader = data_module.get_train_loader()\n",
    "val_loader = data_module.get_val_loader()\n",
    "\n",
    "inspect(train_loader, 'train_loader')\n",
    "\n",
    "run_total_train_weekly(\n",
    "    train_loader, val_loader, device = device,\n",
    "    lookback = lookback, horizon = horizon,\n",
    "    warmup_epochs = 30, spike_epochs = 0,\n",
    "    save_dir = save_root,\n",
    "    use_exogenous_mode = True,\n",
    "    models_to_run = ['patchtst'], use_ssl_pretrain = True\n",
    ")"
   ],
   "id": "df3e41f811589d99",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train_loader] x: torch.Size([256, 52, 1]) cpu torch.float32\n",
      "[train_loader] fe: torch.Size([256, 27, 0]) cpu torch.float32\n",
      "[train_loader] pe: torch.Size([256, 52, 4]) cpu torch.float32\n",
      "[train_loader] future_exo_cb is None? True\n",
      "\n",
      "[total_train] === RUN: patchtst (weekly) ===\n",
      "[DBG-pt_kwargs] use_exogenous_mode=True | d_future=2 | d_past_cont=4 | d_past_cat=0\n",
      "[SSL] PatchTST Pretrain (Weekly)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\PycharmProjects\\ts_forecaster_lib\\src\\modeling_module\\models\\PatchTST\\self_supervised\\backbone.py:158: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  return nn.TransformerEncoder(enc_layer, num_layers=self.e_layers)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pretrain] stage=0 epochs=10 lr=0.0003 wd=0.001 mask_ratio=0.3 loss=mse\n",
      "[Pretrain][stage=0 ep=1/10] train=1.068017 val=1.017288\n",
      "[Pretrain][stage=0 ep=2/10] train=0.956214 val=0.875542\n",
      "[Pretrain][stage=0 ep=3/10] train=0.911285 val=0.901295\n",
      "[Pretrain][stage=0 ep=4/10] train=0.854913 val=0.896292\n",
      "[Pretrain][stage=0 ep=5/10] train=0.846559 val=0.884240\n",
      "[Pretrain][stage=0 ep=6/10] train=0.804724 val=0.805929\n",
      "[Pretrain][stage=0 ep=7/10] train=0.804156 val=0.749205\n",
      "[Pretrain][stage=0 ep=8/10] train=0.729104 val=0.733138\n",
      "[Pretrain][stage=0 ep=9/10] train=0.704641 val=0.664551\n",
      "[Pretrain][stage=0 ep=10/10] train=0.672732 val=0.659830\n",
      "[Pretrain] done | best_val=0.659830\n",
      "[DBG-backbone-init] d_past_cont=4 cont_input_dim=48 target_input_dim=12 total_input_dim=60\n",
      "PatchTST Base (Weekly)\n",
      "[Finetune] loaded pretrain ckpt: C:\\Users\\USER\\PycharmProjects\\ts_forecaster_lib\\raw_data\\fit\\Xpatchtst\\20260119\\pretrain\\patchtst_pretrain_best.pt\n",
      "[Finetune] load_strict=False\n",
      "[Finetune] missing_keys=69 unexpected_keys=41\n",
      "  - missing (first 20): ['backbone.pos_enc', 'backbone.input_proj.weight', 'backbone.input_proj.bias', 'backbone.encoder.layers.0.mha.W_Q.weight', 'backbone.encoder.layers.0.mha.W_Q.bias', 'backbone.encoder.layers.0.mha.W_K.weight', 'backbone.encoder.layers.0.mha.W_K.bias', 'backbone.encoder.layers.0.mha.W_V.weight', 'backbone.encoder.layers.0.mha.W_V.bias', 'backbone.encoder.layers.0.mha.to_out.0.weight', 'backbone.encoder.layers.0.mha.to_out.0.bias', 'backbone.encoder.layers.0.norm_attn.1.weight', 'backbone.encoder.layers.0.norm_attn.1.bias', 'backbone.encoder.layers.0.norm_attn.1.running_mean', 'backbone.encoder.layers.0.norm_attn.1.running_var', 'backbone.encoder.layers.0.ff.0.weight', 'backbone.encoder.layers.0.ff.0.bias', 'backbone.encoder.layers.0.ff.3.weight', 'backbone.encoder.layers.0.ff.3.bias', 'backbone.encoder.layers.0.norm_ffn.1.weight']\n",
      "  - unexpected (first 20): ['backbone.mask_token', 'backbone.patch_embed.weight', 'backbone.patch_embed.bias', 'backbone.pretrain_head.weight', 'backbone.pretrain_head.bias', 'backbone.encoder.layers.0.self_attn.in_proj_weight', 'backbone.encoder.layers.0.self_attn.in_proj_bias', 'backbone.encoder.layers.0.self_attn.out_proj.weight', 'backbone.encoder.layers.0.self_attn.out_proj.bias', 'backbone.encoder.layers.0.linear1.weight', 'backbone.encoder.layers.0.linear1.bias', 'backbone.encoder.layers.0.linear2.weight', 'backbone.encoder.layers.0.linear2.bias', 'backbone.encoder.layers.0.norm1.weight', 'backbone.encoder.layers.0.norm1.bias', 'backbone.encoder.layers.0.norm2.weight', 'backbone.encoder.layers.0.norm2.bias', 'backbone.encoder.layers.1.self_attn.in_proj_weight', 'backbone.encoder.layers.1.self_attn.in_proj_bias', 'backbone.encoder.layers.1.self_attn.out_proj.weight']\n",
      "[train_patchtst] point head rebuilt: d_future 2 -> 2\n",
      "\n",
      "[train_patchtst] ===== Stage 1/2 =====\n",
      "  - spike: OFF\n",
      "  - epochs: 30 | lr=0.0003 | horizon_decay=False\n",
      "[train_patchtst] Effective TrainingConfig:\n",
      "{\n",
      "  \"device\": \"cuda\",\n",
      "  \"log_every\": 100,\n",
      "  \"use_amp\": true,\n",
      "  \"lookback\": 52,\n",
      "  \"horizon\": 27,\n",
      "  \"epochs\": 30,\n",
      "  \"lr\": 0.0003,\n",
      "  \"weight_decay\": 0.001,\n",
      "  \"t_max\": 40,\n",
      "  \"patience\": 100,\n",
      "  \"max_grad_norm\": 30.0,\n",
      "  \"amp_device\": \"cuda\",\n",
      "  \"loss_mode\": \"point\",\n",
      "  \"point_loss\": \"huber\",\n",
      "  \"huber_delta\": 0.8,\n",
      "  \"q_star\": 0.5,\n",
      "  \"use_cost_q_star\": false,\n",
      "  \"Cu\": 1.0,\n",
      "  \"Co\": 1.0,\n",
      "  \"quantiles\": [\n",
      "    0.1,\n",
      "    0.5,\n",
      "    0.9\n",
      "  ],\n",
      "  \"use_intermittent\": true,\n",
      "  \"alpha_zero\": 3.0,\n",
      "  \"alpha_pos\": 1.0,\n",
      "  \"gamma_run\": 0.3,\n",
      "  \"cap\": null,\n",
      "  \"use_horizon_decay\": false,\n",
      "  \"tau_h\": 0.85,\n",
      "  \"val_use_weights\": false,\n",
      "  \"spike_loss\": {\n",
      "    \"enabled\": false,\n",
      "    \"strategy\": \"mix\",\n",
      "    \"huber_delta\": 0.6,\n",
      "    \"asym_up_weight\": 1.0,\n",
      "    \"asym_down_weight\": 2.0,\n",
      "    \"mad_k\": 1.5,\n",
      "    \"w_spike\": 4.0,\n",
      "    \"w_norm\": 1.0,\n",
      "    \"alpha_huber\": 0.6,\n",
      "    \"beta_asym\": 0.4,\n",
      "    \"mix_with_baseline\": false,\n",
      "    \"gamma_baseline\": 0.0\n",
      "  },\n",
      "  \"lambda_hist_scale\": 0.1,\n",
      "  \"lambda_hist_var\": 0.03,\n",
      "  \"hist_window\": 12,\n",
      "  \"anchor_last_k\": 8,\n",
      "  \"anchor_weight\": 0.05\n",
      "}\n",
      "[CommonTrainer] TrainingConfig (final)\n",
      "{\n",
      "  \"device\": \"cuda\",\n",
      "  \"log_every\": 100,\n",
      "  \"use_amp\": true,\n",
      "  \"lookback\": 52,\n",
      "  \"horizon\": 27,\n",
      "  \"epochs\": 30,\n",
      "  \"lr\": 0.0003,\n",
      "  \"weight_decay\": 0.001,\n",
      "  \"t_max\": 40,\n",
      "  \"patience\": 100,\n",
      "  \"max_grad_norm\": 30.0,\n",
      "  \"amp_device\": \"cuda\",\n",
      "  \"loss_mode\": \"point\",\n",
      "  \"point_loss\": \"huber\",\n",
      "  \"huber_delta\": 0.8,\n",
      "  \"q_star\": 0.5,\n",
      "  \"use_cost_q_star\": false,\n",
      "  \"Cu\": 1.0,\n",
      "  \"Co\": 1.0,\n",
      "  \"quantiles\": [\n",
      "    0.1,\n",
      "    0.5,\n",
      "    0.9\n",
      "  ],\n",
      "  \"use_intermittent\": true,\n",
      "  \"alpha_zero\": 3.0,\n",
      "  \"alpha_pos\": 1.0,\n",
      "  \"gamma_run\": 0.3,\n",
      "  \"cap\": null,\n",
      "  \"use_horizon_decay\": false,\n",
      "  \"tau_h\": 0.85,\n",
      "  \"val_use_weights\": false,\n",
      "  \"spike_loss\": {\n",
      "    \"enabled\": false,\n",
      "    \"strategy\": \"mix\",\n",
      "    \"huber_delta\": 0.6,\n",
      "    \"asym_up_weight\": 1.0,\n",
      "    \"asym_down_weight\": 2.0,\n",
      "    \"mad_k\": 1.5,\n",
      "    \"w_spike\": 4.0,\n",
      "    \"w_norm\": 1.0,\n",
      "    \"alpha_huber\": 0.6,\n",
      "    \"beta_asym\": 0.4,\n",
      "    \"mix_with_baseline\": false,\n",
      "    \"gamma_baseline\": 0.0\n",
      "  },\n",
      "  \"lambda_hist_scale\": 0.1,\n",
      "  \"lambda_hist_var\": 0.03,\n",
      "  \"hist_window\": 12,\n",
      "  \"anchor_last_k\": 8,\n",
      "  \"anchor_weight\": 0.05\n",
      "}\n",
      "[EXO-batch] exo normalized to shape=(256, 27, 2) (expect [B,H,E])\n",
      "Epoch 1/30 | LR 0.000300 | Train 77509.692708 | Val 70189.429688\n",
      "Epoch 2/30 | LR 0.000298 | Train 73731.205729 | Val 68999.335938\n",
      "Epoch 3/30 | LR 0.000296 | Train 73150.534722 | Val 68274.225260\n",
      "Epoch 4/30 | LR 0.000293 | Train 72759.276042 | Val 66854.875000\n",
      "Epoch 5/30 | LR 0.000289 | Train 72723.203125 | Val 69047.286458\n",
      "Epoch 6/30 | LR 0.000284 | Train 72264.316840 | Val 68957.617188\n",
      "Epoch 7/30 | LR 0.000278 | Train 71414.245660 | Val 68392.908854\n",
      "Epoch 8/30 | LR 0.000271 | Train 70907.621528 | Val 64775.872396\n",
      "Epoch 9/30 | LR 0.000264 | Train 70474.708333 | Val 64678.597656\n",
      "Epoch 10/30 | LR 0.000256 | Train 69760.655382 | Val 63927.687500\n",
      "Epoch 11/30 | LR 0.000247 | Train 69048.179253 | Val 64174.075521\n",
      "Epoch 12/30 | LR 0.000238 | Train 68740.774740 | Val 63231.358073\n",
      "Epoch 13/30 | LR 0.000228 | Train 67832.537760 | Val 62226.717448\n",
      "Epoch 14/30 | LR 0.000218 | Train 67178.510851 | Val 62117.800781\n",
      "Epoch 15/30 | LR 0.000207 | Train 66699.345052 | Val 61649.401042\n",
      "Epoch 16/30 | LR 0.000196 | Train 66177.884983 | Val 60669.623698\n",
      "Epoch 17/30 | LR 0.000185 | Train 65636.464410 | Val 60943.912760\n",
      "Epoch 18/30 | LR 0.000173 | Train 65026.991319 | Val 62234.037760\n",
      "Epoch 19/30 | LR 0.000162 | Train 64807.295139 | Val 60860.395833\n",
      "Epoch 20/30 | LR 0.000150 | Train 64659.101128 | Val 60964.860677\n",
      "Epoch 21/30 | LR 0.000138 | Train 64323.275608 | Val 59971.723958\n",
      "Epoch 22/30 | LR 0.000127 | Train 63501.732205 | Val 58736.404948\n",
      "Epoch 23/30 | LR 0.000115 | Train 63147.107205 | Val 59586.356771\n",
      "Epoch 24/30 | LR 0.000104 | Train 62929.434462 | Val 58414.231771\n",
      "Epoch 25/30 | LR 0.000093 | Train 62730.512153 | Val 59088.385417\n",
      "Epoch 26/30 | LR 0.000082 | Train 62257.758247 | Val 57754.489583\n",
      "Epoch 27/30 | LR 0.000072 | Train 61632.459201 | Val 57254.678385\n",
      "Epoch 28/30 | LR 0.000062 | Train 61512.291667 | Val 57159.627604\n",
      "Epoch 29/30 | LR 0.000053 | Train 61512.801649 | Val 58592.950521\n",
      "Epoch 30/30 | LR 0.000044 | Train 61234.722222 | Val 57734.742188\n",
      "\n",
      "[train_patchtst] ===== Stage 2/2 =====\n",
      "  - spike: ON\n",
      "  - epochs: 0 | lr=9.999999999999999e-05 | horizon_decay=True\n",
      "[train_patchtst] Effective TrainingConfig:\n",
      "{\n",
      "  \"device\": \"cuda\",\n",
      "  \"log_every\": 100,\n",
      "  \"use_amp\": true,\n",
      "  \"lookback\": 52,\n",
      "  \"horizon\": 27,\n",
      "  \"epochs\": 0,\n",
      "  \"lr\": 9.999999999999999e-05,\n",
      "  \"weight_decay\": 0.001,\n",
      "  \"t_max\": 40,\n",
      "  \"patience\": 100,\n",
      "  \"max_grad_norm\": 30.0,\n",
      "  \"amp_device\": \"cuda\",\n",
      "  \"loss_mode\": \"point\",\n",
      "  \"point_loss\": \"huber\",\n",
      "  \"huber_delta\": 0.8,\n",
      "  \"q_star\": 0.5,\n",
      "  \"use_cost_q_star\": false,\n",
      "  \"Cu\": 1.0,\n",
      "  \"Co\": 1.0,\n",
      "  \"quantiles\": [\n",
      "    0.1,\n",
      "    0.5,\n",
      "    0.9\n",
      "  ],\n",
      "  \"use_intermittent\": true,\n",
      "  \"alpha_zero\": 3.0,\n",
      "  \"alpha_pos\": 1.0,\n",
      "  \"gamma_run\": 0.3,\n",
      "  \"cap\": null,\n",
      "  \"use_horizon_decay\": true,\n",
      "  \"tau_h\": 0.85,\n",
      "  \"val_use_weights\": false,\n",
      "  \"spike_loss\": {\n",
      "    \"enabled\": true,\n",
      "    \"strategy\": \"mix\",\n",
      "    \"huber_delta\": 0.6,\n",
      "    \"asym_up_weight\": 1.0,\n",
      "    \"asym_down_weight\": 2.0,\n",
      "    \"mad_k\": 1.5,\n",
      "    \"w_spike\": 4.0,\n",
      "    \"w_norm\": 1.0,\n",
      "    \"alpha_huber\": 0.6,\n",
      "    \"beta_asym\": 0.4,\n",
      "    \"mix_with_baseline\": false,\n",
      "    \"gamma_baseline\": 0.0\n",
      "  },\n",
      "  \"lambda_hist_scale\": 0.1,\n",
      "  \"lambda_hist_var\": 0.03,\n",
      "  \"hist_window\": 12,\n",
      "  \"anchor_last_k\": 8,\n",
      "  \"anchor_weight\": 0.05\n",
      "}\n",
      "[CommonTrainer] TrainingConfig (final)\n",
      "{\n",
      "  \"device\": \"cuda\",\n",
      "  \"log_every\": 100,\n",
      "  \"use_amp\": true,\n",
      "  \"lookback\": 52,\n",
      "  \"horizon\": 27,\n",
      "  \"epochs\": 0,\n",
      "  \"lr\": 9.999999999999999e-05,\n",
      "  \"weight_decay\": 0.001,\n",
      "  \"t_max\": 40,\n",
      "  \"patience\": 100,\n",
      "  \"max_grad_norm\": 30.0,\n",
      "  \"amp_device\": \"cuda\",\n",
      "  \"loss_mode\": \"point\",\n",
      "  \"point_loss\": \"huber\",\n",
      "  \"huber_delta\": 0.8,\n",
      "  \"q_star\": 0.5,\n",
      "  \"use_cost_q_star\": false,\n",
      "  \"Cu\": 1.0,\n",
      "  \"Co\": 1.0,\n",
      "  \"quantiles\": [\n",
      "    0.1,\n",
      "    0.5,\n",
      "    0.9\n",
      "  ],\n",
      "  \"use_intermittent\": true,\n",
      "  \"alpha_zero\": 3.0,\n",
      "  \"alpha_pos\": 1.0,\n",
      "  \"gamma_run\": 0.3,\n",
      "  \"cap\": null,\n",
      "  \"use_horizon_decay\": true,\n",
      "  \"tau_h\": 0.85,\n",
      "  \"val_use_weights\": false,\n",
      "  \"spike_loss\": {\n",
      "    \"enabled\": true,\n",
      "    \"strategy\": \"mix\",\n",
      "    \"huber_delta\": 0.6,\n",
      "    \"asym_up_weight\": 1.0,\n",
      "    \"asym_down_weight\": 2.0,\n",
      "    \"mad_k\": 1.5,\n",
      "    \"w_spike\": 4.0,\n",
      "    \"w_norm\": 1.0,\n",
      "    \"alpha_huber\": 0.6,\n",
      "    \"beta_asym\": 0.4,\n",
      "    \"mix_with_baseline\": false,\n",
      "    \"gamma_baseline\": 0.0\n",
      "  },\n",
      "  \"lambda_hist_scale\": 0.1,\n",
      "  \"lambda_hist_var\": 0.03,\n",
      "  \"hist_window\": 12,\n",
      "  \"anchor_last_k\": 8,\n",
      "  \"anchor_weight\": 0.05\n",
      "}\n",
      "[EXO-train] inferred E=2 | future_exo_cb? True | exo_is_normalized=True\n",
      "save_root::  C:\\Users\\USER\\PycharmProjects\\ts_forecaster_lib\\raw_data\\fit\\Xpatchtst\\20260119\n",
      "PatchTSTPointModel(\n",
      "  (backbone): SupervisedBackbone(\n",
      "    (cat_embs): ModuleList()\n",
      "    (input_proj): Linear(in_features=60, out_features=256, bias=True)\n",
      "    (attn_core): FullAttention(\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (encoder): TSTEncoder(\n",
      "      (layers): ModuleList(\n",
      "        (0-2): 3 x TSTEncoderLayer(\n",
      "          (mha): MultiHeadAttention(\n",
      "            (W_Q): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (W_K): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (W_V): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (core): FullAttentionWithLogits(\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (to_out): Sequential(\n",
      "              (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (1): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (dropout_attn): Dropout(p=0.0, inplace=False)\n",
      "          (norm_attn): Sequential(\n",
      "            (0): Transpose()\n",
      "            (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): Transpose()\n",
      "          )\n",
      "          (ff): Sequential(\n",
      "            (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (1): GELU(approximate='none')\n",
      "            (2): Dropout(p=0.0, inplace=False)\n",
      "            (3): Linear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (dropout_ffn): Dropout(p=0.0, inplace=False)\n",
      "          (norm_ffn): Sequential(\n",
      "            (0): Transpose()\n",
      "            (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): Transpose()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (norm_out): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (head): PointHeadWithExo(\n",
      "    (future_proj): Linear(in_features=54, out_features=256, bias=True)\n",
      "    (proj): Linear(in_features=512, out_features=27, bias=True)\n",
      "  )\n",
      "  (revin_layer): RevIN()\n",
      ") save success! C:\\Users\\USER\\PycharmProjects\\ts_forecaster_lib\\raw_data\\fit\\Xpatchtst\\20260119\\weekly_PatchTSTBase_L52_H27.pt\n",
      "[DBG-backbone-init] d_past_cont=4 cont_input_dim=48 target_input_dim=12 total_input_dim=60\n",
      "PatchTST Quantile (Weekly)\n",
      "[Finetune] loaded pretrain ckpt: C:\\Users\\USER\\PycharmProjects\\ts_forecaster_lib\\raw_data\\fit\\Xpatchtst\\20260119\\pretrain\\patchtst_pretrain_best.pt\n",
      "[Finetune] load_strict=False\n",
      "[Finetune] missing_keys=71 unexpected_keys=41\n",
      "  - missing (first 20): ['backbone.pos_enc', 'backbone.input_proj.weight', 'backbone.input_proj.bias', 'backbone.encoder.layers.0.mha.W_Q.weight', 'backbone.encoder.layers.0.mha.W_Q.bias', 'backbone.encoder.layers.0.mha.W_K.weight', 'backbone.encoder.layers.0.mha.W_K.bias', 'backbone.encoder.layers.0.mha.W_V.weight', 'backbone.encoder.layers.0.mha.W_V.bias', 'backbone.encoder.layers.0.mha.to_out.0.weight', 'backbone.encoder.layers.0.mha.to_out.0.bias', 'backbone.encoder.layers.0.norm_attn.1.weight', 'backbone.encoder.layers.0.norm_attn.1.bias', 'backbone.encoder.layers.0.norm_attn.1.running_mean', 'backbone.encoder.layers.0.norm_attn.1.running_var', 'backbone.encoder.layers.0.ff.0.weight', 'backbone.encoder.layers.0.ff.0.bias', 'backbone.encoder.layers.0.ff.3.weight', 'backbone.encoder.layers.0.ff.3.bias', 'backbone.encoder.layers.0.norm_ffn.1.weight']\n",
      "  - unexpected (first 20): ['backbone.mask_token', 'backbone.patch_embed.weight', 'backbone.patch_embed.bias', 'backbone.pretrain_head.weight', 'backbone.pretrain_head.bias', 'backbone.encoder.layers.0.self_attn.in_proj_weight', 'backbone.encoder.layers.0.self_attn.in_proj_bias', 'backbone.encoder.layers.0.self_attn.out_proj.weight', 'backbone.encoder.layers.0.self_attn.out_proj.bias', 'backbone.encoder.layers.0.linear1.weight', 'backbone.encoder.layers.0.linear1.bias', 'backbone.encoder.layers.0.linear2.weight', 'backbone.encoder.layers.0.linear2.bias', 'backbone.encoder.layers.0.norm1.weight', 'backbone.encoder.layers.0.norm1.bias', 'backbone.encoder.layers.0.norm2.weight', 'backbone.encoder.layers.0.norm2.bias', 'backbone.encoder.layers.1.self_attn.in_proj_weight', 'backbone.encoder.layers.1.self_attn.in_proj_bias', 'backbone.encoder.layers.1.self_attn.out_proj.weight']\n",
      "[train_patchtst] quantile head rebuilt: d_future 2 -> 2\n",
      "\n",
      "[train_patchtst] ===== Stage 1/2 =====\n",
      "  - spike: OFF\n",
      "  - epochs: 30 | lr=0.0003 | horizon_decay=False\n",
      "[train_patchtst] Effective TrainingConfig:\n",
      "{\n",
      "  \"device\": \"cuda\",\n",
      "  \"log_every\": 100,\n",
      "  \"use_amp\": true,\n",
      "  \"lookback\": 52,\n",
      "  \"horizon\": 27,\n",
      "  \"epochs\": 30,\n",
      "  \"lr\": 0.0003,\n",
      "  \"weight_decay\": 0.0001,\n",
      "  \"t_max\": 40,\n",
      "  \"patience\": 20,\n",
      "  \"max_grad_norm\": 30.0,\n",
      "  \"amp_device\": \"cuda\",\n",
      "  \"loss_mode\": \"quantile\",\n",
      "  \"point_loss\": \"mse\",\n",
      "  \"huber_delta\": 5.0,\n",
      "  \"q_star\": 0.5,\n",
      "  \"use_cost_q_star\": false,\n",
      "  \"Cu\": 1.0,\n",
      "  \"Co\": 1.0,\n",
      "  \"quantiles\": [\n",
      "    0.1,\n",
      "    0.5,\n",
      "    0.9\n",
      "  ],\n",
      "  \"use_intermittent\": true,\n",
      "  \"alpha_zero\": 1.2,\n",
      "  \"alpha_pos\": 1.0,\n",
      "  \"gamma_run\": 0.6,\n",
      "  \"cap\": null,\n",
      "  \"use_horizon_decay\": false,\n",
      "  \"tau_h\": 0.85,\n",
      "  \"val_use_weights\": false,\n",
      "  \"spike_loss\": {\n",
      "    \"enabled\": false,\n",
      "    \"strategy\": \"mix\",\n",
      "    \"huber_delta\": 0.6,\n",
      "    \"asym_up_weight\": 1.0,\n",
      "    \"asym_down_weight\": 2.0,\n",
      "    \"mad_k\": 1.5,\n",
      "    \"w_spike\": 4.0,\n",
      "    \"w_norm\": 1.0,\n",
      "    \"alpha_huber\": 0.6,\n",
      "    \"beta_asym\": 0.4,\n",
      "    \"mix_with_baseline\": false,\n",
      "    \"gamma_baseline\": 0.0\n",
      "  },\n",
      "  \"lambda_hist_scale\": 0.1,\n",
      "  \"lambda_hist_var\": 0.03,\n",
      "  \"hist_window\": 12,\n",
      "  \"anchor_last_k\": 8,\n",
      "  \"anchor_weight\": 0.05\n",
      "}\n",
      "[CommonTrainer] TrainingConfig (final)\n",
      "{\n",
      "  \"device\": \"cuda\",\n",
      "  \"log_every\": 100,\n",
      "  \"use_amp\": true,\n",
      "  \"lookback\": 52,\n",
      "  \"horizon\": 27,\n",
      "  \"epochs\": 30,\n",
      "  \"lr\": 0.0003,\n",
      "  \"weight_decay\": 0.0001,\n",
      "  \"t_max\": 40,\n",
      "  \"patience\": 20,\n",
      "  \"max_grad_norm\": 30.0,\n",
      "  \"amp_device\": \"cuda\",\n",
      "  \"loss_mode\": \"quantile\",\n",
      "  \"point_loss\": \"mse\",\n",
      "  \"huber_delta\": 5.0,\n",
      "  \"q_star\": 0.5,\n",
      "  \"use_cost_q_star\": false,\n",
      "  \"Cu\": 1.0,\n",
      "  \"Co\": 1.0,\n",
      "  \"quantiles\": [\n",
      "    0.1,\n",
      "    0.5,\n",
      "    0.9\n",
      "  ],\n",
      "  \"use_intermittent\": true,\n",
      "  \"alpha_zero\": 1.2,\n",
      "  \"alpha_pos\": 1.0,\n",
      "  \"gamma_run\": 0.6,\n",
      "  \"cap\": null,\n",
      "  \"use_horizon_decay\": false,\n",
      "  \"tau_h\": 0.85,\n",
      "  \"val_use_weights\": false,\n",
      "  \"spike_loss\": {\n",
      "    \"enabled\": false,\n",
      "    \"strategy\": \"mix\",\n",
      "    \"huber_delta\": 0.6,\n",
      "    \"asym_up_weight\": 1.0,\n",
      "    \"asym_down_weight\": 2.0,\n",
      "    \"mad_k\": 1.5,\n",
      "    \"w_spike\": 4.0,\n",
      "    \"w_norm\": 1.0,\n",
      "    \"alpha_huber\": 0.6,\n",
      "    \"beta_asym\": 0.4,\n",
      "    \"mix_with_baseline\": false,\n",
      "    \"gamma_baseline\": 0.0\n",
      "  },\n",
      "  \"lambda_hist_scale\": 0.1,\n",
      "  \"lambda_hist_var\": 0.03,\n",
      "  \"hist_window\": 12,\n",
      "  \"anchor_last_k\": 8,\n",
      "  \"anchor_weight\": 0.05\n",
      "}\n",
      "[EXO-batch] exo normalized to shape=(256, 27, 2) (expect [B,H,E])\n",
      "Epoch 1/30 | LR 0.000300 | Train 35742.936632 | Val 32217.054036\n",
      "Epoch 2/30 | LR 0.000298 | Train 32449.522569 | Val 31454.060547\n",
      "Epoch 3/30 | LR 0.000296 | Train 32172.022786 | Val 31349.296224\n",
      "Epoch 4/30 | LR 0.000293 | Train 32105.360026 | Val 31326.376953\n",
      "Epoch 5/30 | LR 0.000289 | Train 32273.170573 | Val 31057.810547\n",
      "Epoch 6/30 | LR 0.000284 | Train 32142.616319 | Val 31347.810547\n",
      "Epoch 7/30 | LR 0.000278 | Train 32130.340061 | Val 31497.921224\n",
      "Epoch 8/30 | LR 0.000271 | Train 32037.513021 | Val 31209.369792\n",
      "Epoch 9/30 | LR 0.000264 | Train 31818.430556 | Val 30919.121745\n",
      "Epoch 10/30 | LR 0.000256 | Train 31565.079644 | Val 30487.401693\n",
      "Epoch 11/30 | LR 0.000247 | Train 31347.435981 | Val 30135.938802\n",
      "Epoch 12/30 | LR 0.000238 | Train 31004.432075 | Val 30008.852214\n",
      "Epoch 13/30 | LR 0.000228 | Train 30705.681207 | Val 29317.429036\n",
      "Epoch 14/30 | LR 0.000218 | Train 30256.995660 | Val 29229.950521\n",
      "Epoch 15/30 | LR 0.000207 | Train 29988.361545 | Val 29037.586589\n",
      "Epoch 16/30 | LR 0.000196 | Train 29635.557509 | Val 28539.733073\n",
      "Epoch 17/30 | LR 0.000185 | Train 29362.646701 | Val 28191.399740\n",
      "Epoch 18/30 | LR 0.000173 | Train 28950.614800 | Val 27926.766276\n",
      "Epoch 19/30 | LR 0.000162 | Train 28673.536241 | Val 27567.126302\n",
      "Epoch 20/30 | LR 0.000150 | Train 28582.247396 | Val 27818.232422\n",
      "Epoch 21/30 | LR 0.000138 | Train 28316.203559 | Val 27558.899740\n",
      "Epoch 22/30 | LR 0.000127 | Train 28051.342665 | Val 27335.711589\n",
      "Epoch 23/30 | LR 0.000115 | Train 27962.873047 | Val 26892.852865\n",
      "Epoch 24/30 | LR 0.000104 | Train 27759.717882 | Val 27044.580729\n",
      "Epoch 25/30 | LR 0.000093 | Train 27506.017578 | Val 26751.689453\n",
      "Epoch 26/30 | LR 0.000082 | Train 27337.583333 | Val 26643.064453\n",
      "Epoch 27/30 | LR 0.000072 | Train 27153.459635 | Val 26507.040365\n",
      "Epoch 28/30 | LR 0.000062 | Train 26991.995877 | Val 26520.961589\n",
      "Epoch 29/30 | LR 0.000053 | Train 26821.154080 | Val 26259.734375\n",
      "Epoch 30/30 | LR 0.000044 | Train 26721.245226 | Val 26496.577474\n",
      "\n",
      "[train_patchtst] ===== Stage 2/2 =====\n",
      "  - spike: ON\n",
      "  - epochs: 0 | lr=9.999999999999999e-05 | horizon_decay=True\n",
      "[train_patchtst] Effective TrainingConfig:\n",
      "{\n",
      "  \"device\": \"cuda\",\n",
      "  \"log_every\": 100,\n",
      "  \"use_amp\": true,\n",
      "  \"lookback\": 52,\n",
      "  \"horizon\": 27,\n",
      "  \"epochs\": 0,\n",
      "  \"lr\": 9.999999999999999e-05,\n",
      "  \"weight_decay\": 0.0001,\n",
      "  \"t_max\": 40,\n",
      "  \"patience\": 20,\n",
      "  \"max_grad_norm\": 30.0,\n",
      "  \"amp_device\": \"cuda\",\n",
      "  \"loss_mode\": \"quantile\",\n",
      "  \"point_loss\": \"mse\",\n",
      "  \"huber_delta\": 5.0,\n",
      "  \"q_star\": 0.5,\n",
      "  \"use_cost_q_star\": false,\n",
      "  \"Cu\": 1.0,\n",
      "  \"Co\": 1.0,\n",
      "  \"quantiles\": [\n",
      "    0.1,\n",
      "    0.5,\n",
      "    0.9\n",
      "  ],\n",
      "  \"use_intermittent\": true,\n",
      "  \"alpha_zero\": 1.2,\n",
      "  \"alpha_pos\": 1.0,\n",
      "  \"gamma_run\": 0.6,\n",
      "  \"cap\": null,\n",
      "  \"use_horizon_decay\": true,\n",
      "  \"tau_h\": 0.85,\n",
      "  \"val_use_weights\": false,\n",
      "  \"spike_loss\": {\n",
      "    \"enabled\": true,\n",
      "    \"strategy\": \"mix\",\n",
      "    \"huber_delta\": 0.6,\n",
      "    \"asym_up_weight\": 1.0,\n",
      "    \"asym_down_weight\": 2.0,\n",
      "    \"mad_k\": 1.5,\n",
      "    \"w_spike\": 4.0,\n",
      "    \"w_norm\": 1.0,\n",
      "    \"alpha_huber\": 0.6,\n",
      "    \"beta_asym\": 0.4,\n",
      "    \"mix_with_baseline\": false,\n",
      "    \"gamma_baseline\": 0.0\n",
      "  },\n",
      "  \"lambda_hist_scale\": 0.1,\n",
      "  \"lambda_hist_var\": 0.03,\n",
      "  \"hist_window\": 12,\n",
      "  \"anchor_last_k\": 8,\n",
      "  \"anchor_weight\": 0.05\n",
      "}\n",
      "[CommonTrainer] TrainingConfig (final)\n",
      "{\n",
      "  \"device\": \"cuda\",\n",
      "  \"log_every\": 100,\n",
      "  \"use_amp\": true,\n",
      "  \"lookback\": 52,\n",
      "  \"horizon\": 27,\n",
      "  \"epochs\": 0,\n",
      "  \"lr\": 9.999999999999999e-05,\n",
      "  \"weight_decay\": 0.0001,\n",
      "  \"t_max\": 40,\n",
      "  \"patience\": 20,\n",
      "  \"max_grad_norm\": 30.0,\n",
      "  \"amp_device\": \"cuda\",\n",
      "  \"loss_mode\": \"quantile\",\n",
      "  \"point_loss\": \"mse\",\n",
      "  \"huber_delta\": 5.0,\n",
      "  \"q_star\": 0.5,\n",
      "  \"use_cost_q_star\": false,\n",
      "  \"Cu\": 1.0,\n",
      "  \"Co\": 1.0,\n",
      "  \"quantiles\": [\n",
      "    0.1,\n",
      "    0.5,\n",
      "    0.9\n",
      "  ],\n",
      "  \"use_intermittent\": true,\n",
      "  \"alpha_zero\": 1.2,\n",
      "  \"alpha_pos\": 1.0,\n",
      "  \"gamma_run\": 0.6,\n",
      "  \"cap\": null,\n",
      "  \"use_horizon_decay\": true,\n",
      "  \"tau_h\": 0.85,\n",
      "  \"val_use_weights\": false,\n",
      "  \"spike_loss\": {\n",
      "    \"enabled\": true,\n",
      "    \"strategy\": \"mix\",\n",
      "    \"huber_delta\": 0.6,\n",
      "    \"asym_up_weight\": 1.0,\n",
      "    \"asym_down_weight\": 2.0,\n",
      "    \"mad_k\": 1.5,\n",
      "    \"w_spike\": 4.0,\n",
      "    \"w_norm\": 1.0,\n",
      "    \"alpha_huber\": 0.6,\n",
      "    \"beta_asym\": 0.4,\n",
      "    \"mix_with_baseline\": false,\n",
      "    \"gamma_baseline\": 0.0\n",
      "  },\n",
      "  \"lambda_hist_scale\": 0.1,\n",
      "  \"lambda_hist_var\": 0.03,\n",
      "  \"hist_window\": 12,\n",
      "  \"anchor_last_k\": 8,\n",
      "  \"anchor_weight\": 0.05\n",
      "}\n",
      "[EXO-train] inferred E=2 | future_exo_cb? True | exo_is_normalized=True\n",
      "PatchTSTQuantileModel(\n",
      "  (backbone): SupervisedBackbone(\n",
      "    (cat_embs): ModuleList()\n",
      "    (input_proj): Linear(in_features=60, out_features=256, bias=True)\n",
      "    (attn_core): FullAttention(\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (encoder): TSTEncoder(\n",
      "      (layers): ModuleList(\n",
      "        (0-2): 3 x TSTEncoderLayer(\n",
      "          (mha): MultiHeadAttention(\n",
      "            (W_Q): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (W_K): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (W_V): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (core): FullAttentionWithLogits(\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (to_out): Sequential(\n",
      "              (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (1): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (dropout_attn): Dropout(p=0.0, inplace=False)\n",
      "          (norm_attn): Sequential(\n",
      "            (0): Transpose()\n",
      "            (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): Transpose()\n",
      "          )\n",
      "          (ff): Sequential(\n",
      "            (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (1): GELU(approximate='none')\n",
      "            (2): Dropout(p=0.0, inplace=False)\n",
      "            (3): Linear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (dropout_ffn): Dropout(p=0.0, inplace=False)\n",
      "          (norm_ffn): Sequential(\n",
      "            (0): Transpose()\n",
      "            (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): Transpose()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (norm_out): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (head): QuantileHeadWithExo(\n",
      "    (future_proj): Linear(in_features=54, out_features=256, bias=True)\n",
      "    (net): Sequential(\n",
      "      (0): Linear(in_features=512, out_features=128, bias=True)\n",
      "      (1): GELU(approximate='none')\n",
      "      (2): Linear(in_features=128, out_features=81, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (revin_layer): RevIN()\n",
      ") save success! C:\\Users\\USER\\PycharmProjects\\ts_forecaster_lib\\raw_data\\fit\\Xpatchtst\\20260119\\weekly_PatchTSTQuantile_L52_H27.pt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'PatchTST Base': {'model': PatchTSTPointModel(\n",
       "    (backbone): SupervisedBackbone(\n",
       "      (cat_embs): ModuleList()\n",
       "      (input_proj): Linear(in_features=60, out_features=256, bias=True)\n",
       "      (attn_core): FullAttention(\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (encoder): TSTEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0-2): 3 x TSTEncoderLayer(\n",
       "            (mha): MultiHeadAttention(\n",
       "              (W_Q): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (W_K): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (W_V): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (core): FullAttentionWithLogits(\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (dropout_attn): Dropout(p=0.0, inplace=False)\n",
       "            (norm_attn): Sequential(\n",
       "              (0): Transpose()\n",
       "              (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): Transpose()\n",
       "            )\n",
       "            (ff): Sequential(\n",
       "              (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (1): GELU(approximate='none')\n",
       "              (2): Dropout(p=0.0, inplace=False)\n",
       "              (3): Linear(in_features=256, out_features=256, bias=True)\n",
       "            )\n",
       "            (dropout_ffn): Dropout(p=0.0, inplace=False)\n",
       "            (norm_ffn): Sequential(\n",
       "              (0): Transpose()\n",
       "              (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): Transpose()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (norm_out): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (head): PointHeadWithExo(\n",
       "      (future_proj): Linear(in_features=54, out_features=256, bias=True)\n",
       "      (proj): Linear(in_features=512, out_features=27, bias=True)\n",
       "    )\n",
       "    (revin_layer): RevIN()\n",
       "  ),\n",
       "  'cfg': TrainingConfig(device='cuda', log_every=100, use_amp=True, lookback=52, horizon=27, epochs=0, lr=9.999999999999999e-05, weight_decay=0.001, t_max=40, patience=100, max_grad_norm=30.0, amp_device='cuda', loss_mode='point', point_loss='huber', huber_delta=0.8, q_star=0.5, use_cost_q_star=False, Cu=1.0, Co=1.0, quantiles=(0.1, 0.5, 0.9), use_intermittent=True, alpha_zero=3.0, alpha_pos=1.0, gamma_run=0.3, cap=None, use_horizon_decay=True, tau_h=0.85, val_use_weights=False, spike_loss=SpikeLossConfig(enabled=True, strategy='mix', huber_delta=0.6, asym_up_weight=1.0, asym_down_weight=2.0, mad_k=1.5, w_spike=4.0, w_norm=1.0, alpha_huber=0.6, beta_asym=0.4, mix_with_baseline=False, gamma_baseline=0.0), lambda_hist_scale=0.1, lambda_hist_var=0.03, hist_window=12, anchor_last_k=8, anchor_weight=0.05),\n",
       "  'ckpt_path': 'C:\\\\Users\\\\USER\\\\PycharmProjects\\\\ts_forecaster_lib\\\\raw_data\\\\fit\\\\Xpatchtst\\\\20260119\\\\weekly_PatchTSTBase_L52_H27.pt',\n",
       "  'pretrain_ckpt_path': 'C:\\\\Users\\\\USER\\\\PycharmProjects\\\\ts_forecaster_lib\\\\raw_data\\\\fit\\\\Xpatchtst\\\\20260119\\\\pretrain\\\\patchtst_pretrain_best.pt'},\n",
       " 'PatchTST Quantile': {'model': PatchTSTQuantileModel(\n",
       "    (backbone): SupervisedBackbone(\n",
       "      (cat_embs): ModuleList()\n",
       "      (input_proj): Linear(in_features=60, out_features=256, bias=True)\n",
       "      (attn_core): FullAttention(\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (encoder): TSTEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0-2): 3 x TSTEncoderLayer(\n",
       "            (mha): MultiHeadAttention(\n",
       "              (W_Q): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (W_K): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (W_V): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (core): FullAttentionWithLogits(\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (dropout_attn): Dropout(p=0.0, inplace=False)\n",
       "            (norm_attn): Sequential(\n",
       "              (0): Transpose()\n",
       "              (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): Transpose()\n",
       "            )\n",
       "            (ff): Sequential(\n",
       "              (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (1): GELU(approximate='none')\n",
       "              (2): Dropout(p=0.0, inplace=False)\n",
       "              (3): Linear(in_features=256, out_features=256, bias=True)\n",
       "            )\n",
       "            (dropout_ffn): Dropout(p=0.0, inplace=False)\n",
       "            (norm_ffn): Sequential(\n",
       "              (0): Transpose()\n",
       "              (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): Transpose()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (norm_out): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (head): QuantileHeadWithExo(\n",
       "      (future_proj): Linear(in_features=54, out_features=256, bias=True)\n",
       "      (net): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=128, out_features=81, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (revin_layer): RevIN()\n",
       "  ),\n",
       "  'cfg': TrainingConfig(device='cuda', log_every=100, use_amp=True, lookback=52, horizon=27, epochs=0, lr=9.999999999999999e-05, weight_decay=0.0001, t_max=40, patience=20, max_grad_norm=30.0, amp_device='cuda', loss_mode='quantile', point_loss='mse', huber_delta=5.0, q_star=0.5, use_cost_q_star=False, Cu=1.0, Co=1.0, quantiles=(0.1, 0.5, 0.9), use_intermittent=True, alpha_zero=1.2, alpha_pos=1.0, gamma_run=0.6, cap=None, use_horizon_decay=True, tau_h=0.85, val_use_weights=False, spike_loss=SpikeLossConfig(enabled=True, strategy='mix', huber_delta=0.6, asym_up_weight=1.0, asym_down_weight=2.0, mad_k=1.5, w_spike=4.0, w_norm=1.0, alpha_huber=0.6, beta_asym=0.4, mix_with_baseline=False, gamma_baseline=0.0), lambda_hist_scale=0.1, lambda_hist_var=0.03, hist_window=12, anchor_last_k=8, anchor_weight=0.05),\n",
       "  'ckpt_path': 'C:\\\\Users\\\\USER\\\\PycharmProjects\\\\ts_forecaster_lib\\\\raw_data\\\\fit\\\\Xpatchtst\\\\20260119\\\\weekly_PatchTSTQuantile_L52_H27.pt',\n",
       "  'pretrain_ckpt_path': 'C:\\\\Users\\\\USER\\\\PycharmProjects\\\\ts_forecaster_lib\\\\raw_data\\\\fit\\\\Xpatchtst\\\\20260119\\\\pretrain\\\\patchtst_pretrain_best.pt'}}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
