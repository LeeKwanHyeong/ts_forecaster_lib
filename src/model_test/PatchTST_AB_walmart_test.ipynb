{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# PatchTST AB Test (Walmart) — Refactored Notebook\n",
    "\n",
    "Variants:\n",
    "- **A0**: y-only (no exogenous)\n",
    "- **A1**: time/calendar exogenous (sin/cos)\n",
    "- **A2**: time/calendar + holiday (vectorized, batch-safe)\n",
    "\n",
    "Key fixes:\n",
    "- `compose_exo_calendar_cb` supports **batched** `start_idx` so the DataLoader collate can call it once per batch.\n",
    "- `get_train_loader(batch_size=...)` is called explicitly to avoid silent fallback to the default 32.\n"
   ],
   "id": "70944070f96a4c2f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "SRC = \"/Users/igwanhyeong/PycharmProjects/ts_forecaster_lib/src\"\n",
    "if SRC not in sys.path:\n",
    "    sys.path.insert(0, SRC)\n",
    "\n",
    "\n",
    "# LTB modules (expected to exist in your repo)\n",
    "\n",
    "# optional\n",
    "\n",
    "'''\n",
    "pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128\n",
    "https://developer.nvidia.com/cuda-12-8-0-download-archive\n",
    "'''\n",
    "\n",
    "MAC_DIR = \"/Users/igwanhyeong/PycharmProjects/ts_forecaster_lib/raw_data/\"\n",
    "WINDOW_DIR = \"C:/Users/USER/PycharmProjects/ts_forecaster_lib/raw_data/\"\n",
    "\n",
    "DIR = WINDOW_DIR if sys.platform == \"win32\" else MAC_DIR\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(\"DIR:\", DIR)\n",
    "print(\"device:\", device)\n",
    "if device == \"cuda\":\n",
    "    print(\"cuda:\", torch.version.cuda, \"gpu_count:\", torch.cuda.device_count())"
   ],
   "id": "bc5f589c0a2d8ae0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df = pl.read_parquet(DIR + 'train_data/walmart_best_feature_train.parquet')\n",
    "df"
   ],
   "id": "902b46160d02277c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import random\n",
    "\n",
    "past_exo_cont_cols = (\n",
    "    # \"exo_p_y_lag_1w\",\n",
    "    \"exo_p_y_lag_2w\",\n",
    "    # \"exo_p_y_lag_52w\",\n",
    "    \"exo_p_y_rollmean_4w\",\"exo_p_y_rollmean_12w\",\"exo_p_y_rollstd_4w\",\n",
    "    # \"exo_p_weeks_since_holiday`\",\n",
    "    # \"exo_p_temperature\",\n",
    "    # \"exo_p_fuel_price\",\n",
    "    # \"exo_p_cpi\",\n",
    "    # \"exo_p_unemployment\",\n",
    "    # \"exo_p_markdown_sum\",\n",
    "    # \"exo_p_markdown1\",\n",
    "    # \"exo_p_markdown2\",\n",
    "    # \"exo_p_markdown3\",\n",
    "    # \"exo_p_markdown4\",\n",
    "    # \"exo_p_markdown5\",\n",
    "    # \"exo_markdown1_isnull\",\n",
    "    # \"exo_markdown2_isnull\",\n",
    "    # \"exo_markdown3_isnull\",\n",
    "    # \"exo_markdown4_isnull\",\n",
    "    # \"exo_markdown5_isnull\",\n",
    ")\n",
    "past_exo_cat_cols = (\n",
    "    # \"exo_c_woy_bucket\",\n",
    ")\n",
    "\n",
    "lookback = 52\n",
    "horizon = 27\n",
    "batch_size = 256\n",
    "\n",
    "freq = \"weekly\"          # walmart dt is weekly\n",
    "split_mode = \"multi\"     # id-disjoint split (leakage-safe)\n",
    "shuffle = True\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "print(\"device:\", device)\n"
   ],
   "id": "551c7de19d52e0bb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "from modeling_module.utils.exogenous_utils import compose_exo_calendar_cb\n",
    "\n",
    "future_exo_cb_time = compose_exo_calendar_cb(date_type=freq)\n",
    "\n",
    "# ============================================================\n",
    "# Holiday lookup (vectorized) + FutureExo callback (time + holiday)\n",
    "# ============================================================\n",
    "holiday_map_dayidx = {\n",
    "    int(row[0]): float(row[1])\n",
    "    for row in (\n",
    "        df.select([\"date_idx\", \"exo_is_holiday\"])\n",
    "          .group_by(\"date_idx\")\n",
    "          .agg(pl.max(\"exo_is_holiday\").alias(\"exo_is_holiday\"))\n",
    "          .sort(\"date_idx\")\n",
    "          .iter_rows()\n",
    "    )\n",
    "}\n",
    "\n",
    "def build_holiday_array(holiday_map_dayidx: dict[int, float], *, pad: int = 0) -> np.ndarray:\n",
    "    if not holiday_map_dayidx:\n",
    "        return np.zeros((1,), dtype=np.float32)\n",
    "    max_k = max(int(k) for k in holiday_map_dayidx.keys())\n",
    "    arr = np.zeros((max_k + 1 + int(pad),), dtype=np.float32)\n",
    "    for k, v in holiday_map_dayidx.items():\n",
    "        kk = int(k)\n",
    "        if kk >= 0:\n",
    "            arr[kk] = float(v)\n",
    "    return arr\n",
    "\n",
    "class FutureExoTimePlusHoliday:\n",
    "    def __init__(self, holiday_by_dayidx: np.ndarray, *, step_days: int = 7):\n",
    "        self.holiday = holiday_by_dayidx.astype(np.float32, copy=False)\n",
    "        self.step_days = int(step_days)\n",
    "\n",
    "    def __call__(self, start_idx, H: int, device: str = \"cpu\"):\n",
    "        # 1) calendar exo (batch-safe)\n",
    "        cal = future_exo_cb_time(start_idx, H, device=device)  # scalar: (H,E) | batch: (B,H,E)\n",
    "\n",
    "        # 2) holiday exo (vectorized in numpy)\n",
    "        is_scalar = isinstance(start_idx, (int, np.integer))\n",
    "        if is_scalar:\n",
    "            s = np.asarray([int(start_idx)], dtype=np.int64)\n",
    "        else:\n",
    "            s = np.asarray(start_idx, dtype=np.int64).reshape(-1)\n",
    "\n",
    "        B = s.shape[0]\n",
    "        H = int(H)\n",
    "\n",
    "        offsets = (self.step_days * np.arange(H, dtype=np.int64))[None, :]  # (1,H)\n",
    "        idx = s[:, None] + offsets                                          # (B,H)\n",
    "\n",
    "        hol = np.zeros((B, H), dtype=np.float32)\n",
    "        valid = (idx >= 0) & (idx < self.holiday.shape[0])\n",
    "        hol[valid] = self.holiday[idx[valid]]\n",
    "        hol_t = torch.from_numpy(hol).unsqueeze(-1)                         # (B,H,1), CPU\n",
    "\n",
    "        target_device = cal.device  # cal이 이미 cuda일 수 있음\n",
    "        cal = cal.to(target_device, dtype=torch.float32)\n",
    "        hol_t = hol_t.to(target_device, dtype=torch.float32)\n",
    "\n",
    "        # 3) concat\n",
    "        if is_scalar:\n",
    "            out = torch.cat([cal.to(torch.float32).unsqueeze(0), hol_t], dim=-1)[0]  # (H,E+1)\n",
    "        else:\n",
    "            out = torch.cat([cal.to(torch.float32), hol_t], dim=-1)                  # (B,H,E+1)\n",
    "\n",
    "        return out\n",
    "\n",
    "holiday_by_dayidx = build_holiday_array(holiday_map_dayidx, pad=7 * (horizon + 2))\n",
    "future_exo_cb_time_plus_holiday = FutureExoTimePlusHoliday(holiday_by_dayidx, step_days=7)\n"
   ],
   "id": "acfbc0170b17f162",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import importlib, inspect\n",
    "m = importlib.import_module(\"modeling_module.models.PatchTST.supervised.PatchTST\")\n",
    "\n",
    "print(\"[PatchTST module file]\", m.__file__)\n",
    "print(\"[DistHeadWithExo class]\", m.DistHeadWithExo)\n",
    "print(\"[DistHeadWithExo forward file]\", inspect.getsourcefile(m.DistHeadWithExo.forward))\n",
    "print(\"[DistHeadWithExo forward sig ]\", inspect.signature(m.DistHeadWithExo.forward))"
   ],
   "id": "562d5ca8830c45af",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import inspect\n",
    "import modeling_module.models.PatchTST.supervised.PatchTST as M\n",
    "\n",
    "print(\"PatchTST module file:\", M.__file__)\n",
    "print(\"Dist forward signature:\", inspect.signature(M.PatchTSTDistModel.forward))\n",
    "print(\"Dist forward head_out checker snippet:\\n\", \"\\n\".join(inspect.getsource(M.PatchTSTDistModel.forward).splitlines()[:40]))\n"
   ],
   "id": "61339651e409bbbb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from modeling_module.models.model_builder import build_patchTST_dist\n",
    "from modeling_module.training.model_losses.loss_module import DistributionLoss, MQLoss, MAE\n",
    "from modeling_module.utils.metrics import mae, rmse, smape\n",
    "from modeling_module.utils.eval_utils import eval_on_loader, eval_on_loader_quantile\n",
    "from modeling_module.utils.checkpoint import load_model_dict\n",
    "from modeling_module.models import build_patchTST_quantile, build_patchTST_base\n",
    "from modeling_module.training.model_trainers.total_train import run_total_train_weekly\n",
    "from modeling_module.data_loader import MultiPartExoDataModule\n",
    "import pandas as pd\n",
    "rows = []  # seed loop 밖에서 선언\n",
    "\n",
    "def set_seed(seed: int = 11):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.backends.mps.is_available():\n",
    "        torch.mps.manual_seed(seed)\n",
    "    elif torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def build_datamodule(variant: str, seed: int) -> MultiPartExoDataModule:\n",
    "    if variant == \"A0\":\n",
    "        future_exo_cb = None\n",
    "    elif variant == \"A1\":\n",
    "        future_exo_cb = future_exo_cb_time\n",
    "    elif variant == \"A2\":\n",
    "        future_exo_cb = future_exo_cb_time_plus_holiday\n",
    "    else:\n",
    "        raise ValueError(variant)\n",
    "\n",
    "    return MultiPartExoDataModule(\n",
    "        df=df,\n",
    "        id_col=\"unique_id\",\n",
    "        date_col=\"date\",\n",
    "        y_col=\"y\",\n",
    "        lookback=lookback,\n",
    "        horizon=horizon,\n",
    "        batch_size=batch_size,\n",
    "        past_exo_cont_cols=past_exo_cont_cols,\n",
    "        past_exo_cat_cols=past_exo_cat_cols,\n",
    "        future_exo_cb=future_exo_cb,\n",
    "        freq=freq,\n",
    "        shuffle=shuffle,\n",
    "        split_mode=split_mode,\n",
    "        seed=seed,\n",
    "    )\n",
    "\n",
    "def inspect(loader, name):\n",
    "    b = next(iter(loader))\n",
    "    x, y, uid, fe, pe_cont, pe_cat = b\n",
    "    print(f\"[{name}] x:\", x.shape, x.device, x.dtype)\n",
    "    print(f\"[{name}] fe:\", fe.shape, fe.device, fe.dtype)\n",
    "    print(f\"[{name}] pe:\", pe_cont.shape, pe_cont.device, pe_cont.dtype)\n",
    "    print(f\"[{name}] future_exo_cb is None?\", loader.collate_fn.future_exo_cb is None)\n",
    "    if fe.shape[-1] > 0:\n",
    "        print(f\"[{name}] fe sample:\", fe[0, :3, :])\n",
    "\n",
    "# seed_list = [11, 22, 33, 44, 55]\n",
    "seed_list = [55]\n",
    "for seed in seed_list:\n",
    "    set_seed(seed)\n",
    "\n",
    "    save_dir = os.path.join(DIR, \"fit\", \"walmart_patchtst_ab\")\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    save_root_A0 = os.path.join(save_dir, \"A0_y_only\", f\"seed_{seed}\")\n",
    "    save_root_A1 = os.path.join(save_dir, \"A1_time_exog\", f\"seed_{seed}\")\n",
    "    save_root_A2 = os.path.join(save_dir, \"A2_time_holiday\", f\"seed_{seed}\")\n",
    "\n",
    "    data_module_A0 = build_datamodule(\"A0\", seed)\n",
    "    data_module_A1 = build_datamodule(\"A1\", seed)\n",
    "    data_module_A2 = build_datamodule(\"A2\", seed)\n",
    "\n",
    "    train_loader_A0 = data_module_A0.get_train_loader(batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    val_loader_A0   = data_module_A0.get_val_loader()\n",
    "\n",
    "    train_loader_A1 = data_module_A1.get_train_loader(batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    val_loader_A1   = data_module_A1.get_val_loader()\n",
    "\n",
    "    train_loader_A2 = data_module_A2.get_train_loader(batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    val_loader_A2   = data_module_A2.get_val_loader()\n",
    "\n",
    "    inspect(train_loader_A0, \"A0\")\n",
    "    inspect(train_loader_A1, \"A1\")\n",
    "    inspect(train_loader_A2, \"A2\")\n",
    "\n",
    "    # ============================================\n",
    "    # 학습 실행 (LTB total_train 포맷 유지)\n",
    "    # - 여기서는 \"외생변수 A/B/C\"만 비교하므로 use_ssl_pretrain=False로 고정\n",
    "    # Walmart처럼 항상 판매량이 있는(continuous) 데이터에서 “스파이크”를 잡는 규칙이:\n",
    "    # 스파이크 마스크가 과도하게 넓게 잡히거나(사실상 대부분 True)\n",
    "    # spike-loss가 MSE/제곱오차 기반인데 reduction이 sum 또는 정규화 없이 누적되어\n",
    "    # sales 스케일(1e4~1e5)에서 제곱오차가 1e8급으로 바로 올라가\n",
    "    # → 결과적으로 delta가 1e8 수준으로 튄다.\n",
    "    # → 그래서 최종적으로 이 상황에서는 spike_epoch를 0으로 잡아준다.\n",
    "    # ============================================\n",
    "    # ,\n",
    "    print('run result_A0')\n",
    "\n",
    "    # results_A0 = run_total_train_weekly(\n",
    "    #     train_loader_A0,\n",
    "    #     val_loader_A0,\n",
    "    #     device=device,\n",
    "    #     lookback=lookback,\n",
    "    #     horizon=horizon,\n",
    "    #     warmup_epochs=20,\n",
    "    #     spike_epochs=0,\n",
    "    #     save_dir=save_root_A0,\n",
    "    #     # loss = DistributionLoss(distribution=\"Normal\", level=[80, 90]),\n",
    "    #     loss = MAE(),\n",
    "    #     loss_quantile=MQLoss(quantiles=[0.1,0.5,0.9]),  # Quantile 학습용\n",
    "    #     models_to_run=[\"patchtst\"],\n",
    "    #     use_ssl_mode = 'full',\n",
    "    # )\n",
    "    #\n",
    "    # print('run result_A1')\n",
    "    # results_A1 = run_total_train_weekly(\n",
    "    #     train_loader_A1,\n",
    "    #     val_loader_A1,\n",
    "    #     device=device,\n",
    "    #     lookback=lookback,\n",
    "    #     horizon=horizon,\n",
    "    #     warmup_epochs=20,\n",
    "    #     spike_epochs=0,\n",
    "    #     save_dir=save_root_A1,\n",
    "    #     # loss = DistributionLoss(distribution=\"Normal\", level=[80, 90]),\n",
    "    #     loss = MAE(),\n",
    "    #     loss_quantile=MQLoss(quantiles=[0.1,0.5,0.9]),  # Quantile 학습용\n",
    "    #     models_to_run=[\"patchtst\"],\n",
    "    #     use_ssl_mode = 'full',\n",
    "    # )\n",
    "    #\n",
    "    # print('run result_A2')\n",
    "    # results_A2 = run_total_train_weekly(\n",
    "    #     train_loader_A2,\n",
    "    #     val_loader_A2,\n",
    "    #     device=device,\n",
    "    #     lookback=lookback,\n",
    "    #     horizon=horizon,\n",
    "    #     warmup_epochs=20,\n",
    "    #     spike_epochs=0,\n",
    "    #     save_dir=save_root_A2,\n",
    "    #     # loss = DistributionLoss(distribution=\"Normal\", level=[80, 90]),\n",
    "    #     loss = MAE(),\n",
    "    #     loss_quantile=MQLoss(quantiles=[0.1,0.5,0.9]),  # Quantile 학습용\n",
    "    #     models_to_run=[\"patchtst\"],\n",
    "    #     use_ssl_mode = 'full',\n",
    "    # )\n",
    "\n",
    "    builders = {\n",
    "    \"patchtst_quantile\": build_patchTST_quantile,\n",
    "    \"patchtst\": build_patchTST_base,\n",
    "    'patchtst_dist': build_patchTST_dist,\n",
    "    }\n",
    "\n",
    "    print(builders)\n",
    "\n",
    "    print(load_model_dict(save_root_A0, builders, device = device))\n",
    "\n",
    "\n",
    "    def to_point_pred(y_hat, *, quantiles=(0.1,0.5,0.9), q_star=0.5):\n",
    "        # dict 처리\n",
    "        if isinstance(y_hat, dict):\n",
    "            if \"y_hat\" in y_hat:\n",
    "                y_hat = y_hat[\"y_hat\"]\n",
    "            elif \"loc\" in y_hat:         # dist output\n",
    "                return y_hat[\"loc\"]\n",
    "\n",
    "        y_hat = np.asarray(y_hat)\n",
    "\n",
    "        # dist tensor: [B,H,2] -> loc\n",
    "        if y_hat.ndim >= 3 and y_hat.shape[-1] == 2:\n",
    "            return y_hat[..., 0]\n",
    "\n",
    "        # quantile: [B,H,Q] or [B,Q,H]\n",
    "        if y_hat.ndim == 3:\n",
    "            qs = list(quantiles)\n",
    "            if y_hat.shape[-1] == len(qs):     # [B,H,Q]\n",
    "                return y_hat[..., qs.index(q_star)]\n",
    "            if y_hat.shape[1] == len(qs):      # [B,Q,H]\n",
    "                return y_hat[:, qs.index(q_star), :]\n",
    "\n",
    "        # point: [B,H] or [B,H,1]\n",
    "        if y_hat.ndim == 3 and y_hat.shape[-1] == 1:\n",
    "            return y_hat[..., 0]\n",
    "        return y_hat\n",
    "\n",
    "    def squeeze_y(y):\n",
    "        y = np.asarray(y)\n",
    "        if y.ndim == 3 and y.shape[-1] == 1:\n",
    "            return y[..., 0]\n",
    "        return y\n",
    "    model_A0_b = load_model_dict(save_root_A0, builders, device = device)['patchtst']\n",
    "    model_A1_b = load_model_dict(save_root_A1, builders, device = device)['patchtst']\n",
    "    model_A2_b = load_model_dict(save_root_A2, builders, device = device)['patchtst']\n",
    "    y0, yhat = eval_on_loader(model_A0_b, val_loader_A0, device=device)\n",
    "    y1, yhat1 = eval_on_loader(model_A1_b, val_loader_A1, device=device, future_exo_cb = future_exo_cb_time)\n",
    "    y2, yhat2 = eval_on_loader(model_A2_b, val_loader_A2, device=device, future_exo_cb = future_exo_cb_time_plus_holiday)\n",
    "\n",
    "    metric_A0 = {\n",
    "    \"MAE\": float(mae(y0.reshape(-1), yhat0.reshape(-1))),\n",
    "    \"RMSE\": float(rmse(y0.reshape(-1), yhat0.reshape(-1))),\n",
    "    \"SMAPE\": float(smape(y0.reshape(-1), yhat0.reshape(-1))),\n",
    "    }\n",
    "    metric_A1 = {\n",
    "        \"MAE\": float(mae(y1.reshape(-1), yhat1.reshape(-1))),\n",
    "        \"RMSE\": float(rmse(y1.reshape(-1), yhat1.reshape(-1))),\n",
    "        \"SMAPE\": float(smape(y1.reshape(-1), yhat1.reshape(-1))),\n",
    "    }\n",
    "    metric_A2 = {\n",
    "        \"MAE\": float(mae(y2.reshape(-1), yhat2.reshape(-1))),\n",
    "        \"RMSE\": float(rmse(y2.reshape(-1), yhat2.reshape(-1))),\n",
    "        \"SMAPE\": float(smape(y2.reshape(-1), yhat2.reshape(-1))),\n",
    "    }\n",
    "\n",
    "    model_A0_d = load_model_dict(save_root_A0, builders, device = device)['patchtst_dist']\n",
    "    model_A1_d = load_model_dict(save_root_A1, builders, device = device)['patchtst_dist']\n",
    "    model_A2_d = load_model_dict(save_root_A2, builders, device = device)['patchtst_dist']\n",
    "\n",
    "    y0_d, yhat0_d = eval_on_loader(model_A0_d, val_loader_A0, device=device)\n",
    "    y0_d = squeeze_y(y0_d); yhat0_d = to_point_pred(yhat0_d, quantiles = (0.1, 0.5, 0.9), q_star = 0.5)\n",
    "    y1_d, yhat1_d = eval_on_loader(model_A1_d, val_loader_A1, device=device, future_exo_cb = future_exo_cb_time)\n",
    "    y1_d = squeeze_y(y1_d); yhat1_d = to_point_pred(yhat1_d, quantiles = (0.1, 0.5, 0.9), q_star = 0.5)\n",
    "    y2_d, yhat2_d = eval_on_loader(model_A2_d, val_loader_A2, device=device, future_exo_cb = future_exo_cb_time_plus_holiday)\n",
    "    y2_d = squeeze_y(y2_d); yhat2_d = to_point_pred(yhat2_d, quantiles = (0.1, 0.5, 0.9), q_star = 0.5)\n",
    "\n",
    "    metric_A0 = {\n",
    "    \"MAE\": float(mae(y0_d.reshape(-1), yhat0_d.reshape(-1))),\n",
    "    \"RMSE\": float(rmse(y0_d.reshape(-1), yhat0_d.reshape(-1))),\n",
    "    \"SMAPE\": float(smape(y0_d.reshape(-1), yhat0_d.reshape(-1))),\n",
    "    }\n",
    "    metric_A1 = {\n",
    "        \"MAE\": float(mae(y1_d.reshape(-1), yhat1_d.reshape(-1))),\n",
    "        \"RMSE\": float(rmse(y1_d.reshape(-1), yhat1_d.reshape(-1))),\n",
    "        \"SMAPE\": float(smape(y1_d.reshape(-1), yhat1_d.reshape(-1))),\n",
    "    }\n",
    "    metric_A2 = {\n",
    "        \"MAE\": float(mae(y2_d.reshape(-1), yhat2_d.reshape(-1))),\n",
    "        \"RMSE\": float(rmse(y2_d.reshape(-1), yhat2_d.reshape(-1))),\n",
    "        \"SMAPE\": float(smape(y2_d.reshape(-1), yhat2_d.reshape(-1))),\n",
    "    }\n",
    "\n",
    "    model_A0_q = load_model_dict(save_root_A0, builders, device = device)['patchtst_quantile']\n",
    "    model_A1_q = load_model_dict(save_root_A1, builders, device = device)['patchtst_quantile']\n",
    "    model_A2_q = load_model_dict(save_root_A2, builders, device = device)['patchtst_quantile']\n",
    "\n",
    "    y0_q, yhat0_q = eval_on_loader_quantile(model_A0_q, val_loader_A0, device=device)\n",
    "    y1_q, yhat1_q = eval_on_loader_quantile(model_A1_q, val_loader_A1, device=device, future_exo_cb = future_exo_cb_time)\n",
    "    y2_q, yhat2_q = eval_on_loader_quantile(model_A2_q, val_loader_A2, device=device, future_exo_cb = future_exo_cb_time_plus_holiday)\n",
    "\n",
    "    metric_A0_q = {\n",
    "    \"MAE\": float(mae(y0_q.reshape(-1), yhat0_q.reshape(-1))),\n",
    "    \"RMSE\": float(rmse(y0_q.reshape(-1), yhat0_q.reshape(-1))),\n",
    "    \"SMAPE\": float(smape(y0_q.reshape(-1), yhat0_q.reshape(-1))),\n",
    "    }\n",
    "    metric_A1_q = {\n",
    "        \"MAE\": float(mae(y1_q.reshape(-1), yhat1_q.reshape(-1))),\n",
    "        \"RMSE\": float(rmse(y1_q.reshape(-1), yhat1_q.reshape(-1))),\n",
    "        \"SMAPE\": float(smape(y1_q.reshape(-1), yhat1_q.reshape(-1))),\n",
    "    }\n",
    "    metric_A2_q = {\n",
    "        \"MAE\": float(mae(y2_q.reshape(-1), yhat2_q.reshape(-1))),\n",
    "        \"RMSE\": float(rmse(y2_q.reshape(-1), yhat2_q.reshape(-1))),\n",
    "        \"SMAPE\": float(smape(y2_q.reshape(-1), yhat2_q.reshape(-1))),\n",
    "    }\n",
    "\n",
    "    # -------------------------\n",
    "    # Point metrics row append\n",
    "    # -------------------------\n",
    "    rows.append({\n",
    "        \"seed\": seed,\n",
    "        \"variant\": \"A0\",\n",
    "        \"model_type\": \"point\",\n",
    "        \"MAE\": metric_A0[\"MAE\"],\n",
    "        \"RMSE\": metric_A0[\"RMSE\"],\n",
    "        \"SMAPE\": metric_A0[\"SMAPE\"],\n",
    "        \"save_root\": save_root_A0,\n",
    "    })\n",
    "    rows.append({\n",
    "        \"seed\": seed,\n",
    "        \"variant\": \"A1\",\n",
    "        \"model_type\": \"point\",\n",
    "        \"MAE\": metric_A1[\"MAE\"],\n",
    "        \"RMSE\": metric_A1[\"RMSE\"],\n",
    "        \"SMAPE\": metric_A1[\"SMAPE\"],\n",
    "        \"save_root\": save_root_A1,\n",
    "    })\n",
    "    rows.append({\n",
    "        \"seed\": seed,\n",
    "        \"variant\": \"A2\",\n",
    "        \"model_type\": \"point\",\n",
    "        \"MAE\": metric_A2[\"MAE\"],\n",
    "        \"RMSE\": metric_A2[\"RMSE\"],\n",
    "        \"SMAPE\": metric_A2[\"SMAPE\"],\n",
    "        \"save_root\": save_root_A2,\n",
    "    })\n",
    "\n",
    "    # -------------------------\n",
    "    # Quantile metrics row append\n",
    "    # (주의: q50 등 기준이 명확해야 함)\n",
    "    # -------------------------\n",
    "    rows.append({\n",
    "        \"seed\": seed,\n",
    "        \"variant\": \"A0\",\n",
    "        \"model_type\": \"quantile(q50)\",\n",
    "        \"MAE\": metric_A0_q[\"MAE\"],\n",
    "        \"RMSE\": metric_A0_q[\"RMSE\"],\n",
    "        \"SMAPE\": metric_A0_q[\"SMAPE\"],\n",
    "        \"save_root\": save_root_A0,\n",
    "    })\n",
    "    rows.append({\n",
    "        \"seed\": seed,\n",
    "        \"variant\": \"A1\",\n",
    "        \"model_type\": \"quantile(q50)\",\n",
    "        \"MAE\": metric_A1_q[\"MAE\"],\n",
    "        \"RMSE\": metric_A1_q[\"RMSE\"],\n",
    "        \"SMAPE\": metric_A1_q[\"SMAPE\"],\n",
    "        \"save_root\": save_root_A1,\n",
    "    })\n",
    "    rows.append({\n",
    "        \"seed\": seed,\n",
    "        \"variant\": \"A2\",\n",
    "        \"model_type\": \"quantile(q50)\",\n",
    "        \"MAE\": metric_A2_q[\"MAE\"],\n",
    "        \"RMSE\": metric_A2_q[\"RMSE\"],\n",
    "        \"SMAPE\": metric_A2_q[\"SMAPE\"],\n",
    "        \"save_root\": save_root_A2,\n",
    "    })\n",
    "\n",
    "# loop 종료 후 저장\n",
    "df_out = pd.DataFrame(rows)\n",
    "\n",
    "out_csv = os.path.join(save_dir, \"ab_results_by_seed.csv\")\n",
    "df_out.to_csv(out_csv, index=False)\n",
    "\n",
    "# variant별 mean/std 요약도 같이 저장 추천\n",
    "summary = (\n",
    "    df_out.groupby([\"variant\", \"model_type\"])[[\"MAE\", \"RMSE\", \"SMAPE\"]]\n",
    "    .agg([\"mean\", \"std\"])\n",
    "    .reset_index()\n",
    ")\n",
    "out_sum = os.path.join(save_dir, \"ab_results_summary.csv\")\n",
    "summary.to_csv(out_sum, index=False)\n",
    "\n",
    "print(\"saved:\", out_csv, out_sum)\n"
   ],
   "id": "74ac8e2b5faa0b3d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "state_dict = model_A1_q.state_dict()\n",
    "\n",
    "print(\"Keys in the state_dict:\")\n",
    "for key in state_dict.keys():\n",
    "    print(key)"
   ],
   "id": "4121b271f273d63e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ============================================\n",
    "# CELL 6) 체크포인트 로드 + 평가 (첨부 노트북 흐름 유지)\n",
    "# - quantile 모델이면 eval_on_loader_quantile 사용\n",
    "# - point 모델이면 eval_on_loader 사용\n",
    "# ============================================\n",
    "\n",
    "# builders는 \"당신이 실제 저장한 모델 키\"에 맞추세요.\n",
    "# (첨부 노트북에서는 patchtst_quantile을 사용)\n",
    "# builders = {\n",
    "#     \"patchtst_quantile\": build_patchTST_quantile,\n",
    "#     \"patchtst\": build_patchTST_base,\n",
    "# }\n",
    "\n",
    "model_A0 = load_model_dict(save_root_A0, builders, device = device)['patchtst']\n",
    "model_A1 = load_model_dict(save_root_A1, builders, device = device)['patchtst']\n",
    "model_A2 = load_model_dict(save_root_A2, builders, device = device)['patchtst']\n",
    "y0, yhat0 = eval_on_loader_quantile(model_A0_q, val_loader_A0, device=device)\n",
    "y1, yhat1 = eval_on_loader_quantile(model_A1_q, val_loader_A1, device=device, future_exo_cb = future_exo_cb_time)\n",
    "y2, yhat2 = eval_on_loader_quantile(model_A2_q, val_loader_A2, device=device, future_exo_cb = future_exo_cb_time_plus_holiday)\n",
    "\n",
    "# model_A0 = load_model_dict(save_root_A0, builders, device = device)['patchtst_quantile']\n",
    "# model_A1 = load_model_dict(save_root_A1, builders, device = device)['patchtst_quantile']\n",
    "# model_A2 = load_model_dict(save_root_A2, builders, device = device)['patchtst_quantile']\n",
    "# y0, yhat0 = eval_on_loader_quantile(model_A0_q, val_loader_A0, device=device)\n",
    "# y1, yhat1 = eval_on_loader_quantile(model_A1_q, val_loader_A1, device=device, future_exo_cb = future_exo_cb_time)\n",
    "# y2, yhat2 = eval_on_loader_quantile(model_A2_q, val_loader_A2, device=device, future_exo_cb = future_exo_cb_time_plus_holiday)\n",
    "\n",
    "\n",
    "# model_A0 = load_model_dict(save_root_A0, builders, device = device)['patchtst_dist']\n",
    "# model_A1 = load_model_dict(save_root_A1, builders, device = device)['patchtst_dist']\n",
    "# model_A2 = load_model_dict(save_root_A2, builders, device = device)['patchtst_dist']\n",
    "# y0, yhat0 = eval_on_loader(model_A0, val_loader_A0, device=device)\n",
    "# y0 = squeeze_y(y0); yhat0 = to_point_pred(yhat0, quantiles = (0.1, 0.5, 0.9), q_star = 0.5)\n",
    "# y1, yhat1 = eval_on_loader(model_A1, val_loader_A1, device=device, future_exo_cb = future_exo_cb_time)\n",
    "# y1 = squeeze_y(y1); yhat1 = to_point_pred(yhat1, quantiles = (0.1, 0.5, 0.9), q_star = 0.5)\n",
    "# y2, yhat2 = eval_on_loader(model_A2, val_loader_A2, device=device, future_exo_cb = future_exo_cb_time_plus_holiday)\n",
    "# y2 = squeeze_y(y2); yhat2 = to_point_pred(yhat2, quantiles = (0.1, 0.5, 0.9), q_star = 0.5)\n",
    "\n",
    "\n",
    "metric_A0 = {\n",
    "    \"MAE\": float(mae(y0.reshape(-1), yhat0.reshape(-1))),\n",
    "    \"RMSE\": float(rmse(y0.reshape(-1), yhat0.reshape(-1))),\n",
    "    \"SMAPE\": float(smape(y0.reshape(-1), yhat0.reshape(-1))),\n",
    "}\n",
    "metric_A1 = {\n",
    "    \"MAE\": float(mae(y1.reshape(-1), yhat1.reshape(-1))),\n",
    "    \"RMSE\": float(rmse(y1.reshape(-1), yhat1.reshape(-1))),\n",
    "    \"SMAPE\": float(smape(y1.reshape(-1), yhat1.reshape(-1))),\n",
    "}\n",
    "metric_A2 = {\n",
    "    \"MAE\": float(mae(y2.reshape(-1), yhat2.reshape(-1))),\n",
    "    \"RMSE\": float(rmse(y2.reshape(-1), yhat2.reshape(-1))),\n",
    "    \"SMAPE\": float(smape(y2.reshape(-1), yhat2.reshape(-1))),\n",
    "}\n",
    "print(f'metric_A0: {metric_A0}')\n",
    "print(f'metric_A1: {metric_A1}')\n",
    "print(f'metric_A2: {metric_A2}')\n",
    "# metric_A0, metric_A1, metric_A2"
   ],
   "id": "7a02b5c85b381dfe",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ============================================\n",
    "# CELL 7) (선택) 예측 시각화 (첨부 노트북 스타일)\n",
    "# ============================================\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_samples(y_true, preds: dict, max_n: int = 64):\n",
    "    n = min(max_n, y_true.shape[0])\n",
    "    fig, axes = plt.subplots(nrows=n, ncols=1, figsize=(12, 2.2*n), sharex=True)\n",
    "\n",
    "    if n == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for i in range(n):\n",
    "        ax = axes[i]\n",
    "        ax.plot(y_true[i], label=\"true\")\n",
    "        for k, v in preds.items():\n",
    "            ax.plot(v[i], label=k)\n",
    "        ax.set_title(f\"sample={i}\", fontsize=9)\n",
    "        ax.legend(loc=\"upper right\", fontsize=8)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_samples(\n",
    "    y_true=y0,\n",
    "    preds={\n",
    "        \"A0_y_only\": yhat0,\n",
    "        \"A1_time\": yhat1,\n",
    "        \"A2_time+holiday\": yhat2,\n",
    "    },\n",
    "    max_n=32,\n",
    ")"
   ],
   "id": "66989d185bda7f1e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from modeling_module.training.forecater import DMSForecaster\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from modeling_module.utils.date_util import DateUtil\n",
    "\n",
    "from neuralforecast import NeuralForecast\n",
    "from neuralforecast.models import PatchTST as NixtlaPatchTST\n",
    "from neuralforecast.losses.pytorch import DistributionLoss\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 0) Reproducibility\n",
    "# =========================\n",
    "def set_seed(seed: int = 22):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(55)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 1) Config\n",
    "# =========================\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "H = 27\n",
    "LOOKBACK = 52\n",
    "PATCH_LEN = 24\n",
    "STRIDE = 24\n",
    "FREQ = \"W-MON\"\n",
    "\n",
    "PARQUET_PATH = DIR + \"train_data/walmart_best_feature_train.parquet\"\n",
    "\n",
    "# our_model\n",
    "our_model = model_A0\n",
    "our_model.eval()\n",
    "forecaster = DMSForecaster(our_model)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 2) Load polars -> pandas (Nixtla format)\n",
    "# =========================\n",
    "df = pl.read_parquet(PARQUET_PATH)\n",
    "\n",
    "# DateUtil을 통일해서 사용 (DateUtil.yyyyww_to_date)\n",
    "nixtla_df = (\n",
    "    df.select([\"unique_id\", \"date\", \"y\"])\n",
    "      .with_columns([\n",
    "          pl.col(\"unique_id\").cast(pl.Utf8),\n",
    "          pl.col(\"date\").map_elements(DateUtil.yyyyww_to_date, return_dtype=pl.Date).alias(\"ds\"),\n",
    "          pl.col(\"y\").cast(pl.Float64),\n",
    "      ])\n",
    "      .select([\"unique_id\", \"ds\", \"y\"])\n",
    "      .sort([\"unique_id\", \"ds\"])\n",
    "      .to_pandas()\n",
    ")\n",
    "nixtla_df[\"ds\"] = pd.to_datetime(nixtla_df[\"ds\"])\n",
    "nixtla_df[\"unique_id\"] = nixtla_df[\"unique_id\"].astype(str)\n",
    "\n",
    "print(\"[debug] raw unique_id:\", nixtla_df[\"unique_id\"].nunique())\n",
    "print(\"[debug] raw ds range:\", nixtla_df[\"ds\"].min(), \"~\", nixtla_df[\"ds\"].max())\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 3) Weekly reindex (안전 버전)\n",
    "#    - 같은 ds에 중복 row가 있으면 sum으로 집계 후 reindex\n",
    "# =========================\n",
    "def reindex_weekly_panel_sum(df_: pd.DataFrame, freq: str = \"W-MON\") -> pd.DataFrame:\n",
    "    out = []\n",
    "    for uid, g in df_.groupby(\"unique_id\", sort=False):\n",
    "        g = g.sort_values(\"ds\")\n",
    "\n",
    "        # (중요) 중복 ds를 sum으로 집계 (데이터에 중복 주차가 있으면 reindex가 꼬임)\n",
    "        g = g.groupby(\"ds\", as_index=False)[\"y\"].sum()\n",
    "\n",
    "        idx = pd.date_range(start=g[\"ds\"].min(), end=g[\"ds\"].max(), freq=freq)\n",
    "        gg = g.set_index(\"ds\").reindex(idx)\n",
    "        gg.index.name = \"ds\"\n",
    "        gg = gg.reset_index()\n",
    "        gg[\"unique_id\"] = uid\n",
    "        gg[\"y\"] = gg[\"y\"].fillna(0.0)\n",
    "\n",
    "        out.append(gg[[\"unique_id\", \"ds\", \"y\"]])\n",
    "    return pd.concat(out, ignore_index=True)\n",
    "\n",
    "nixtla_df = reindex_weekly_panel_sum(nixtla_df, freq=FREQ)\n",
    "\n",
    "print(\"[debug] after reindex unique_id:\", nixtla_df[\"unique_id\"].nunique())\n",
    "print(\"[debug] after reindex ds freq check sample:\")\n",
    "tmp = nixtla_df[nixtla_df[\"unique_id\"] == nixtla_df[\"unique_id\"].iloc[0]].sort_values(\"ds\")\n",
    "print(tmp[\"ds\"].diff().dropna().value_counts().head(3))\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 4) Length filter (여기가 0 만들기 가장 흔한 포인트)\n",
    "# =========================\n",
    "# val_size=H를 쓰면 train에 최소 (LOOKBACK + H)가 필요하고\n",
    "# holdout test로 H를 따로 빼니 전체는 LOOKBACK + 2H가 \"권장\"이지만,\n",
    "# 데이터가 짧다면 우선 완화해서 돌리는 게 맞습니다.\n",
    "MIN_REQUIRED = LOOKBACK + H  # <-- 완화 (기존 LOOKBACK + 2H -> 너무 빡빡해서 0될 가능성 큼)\n",
    "\n",
    "lens = nixtla_df.groupby(\"unique_id\")[\"y\"].size().sort_values()\n",
    "valid_ids = lens[lens >= (MIN_REQUIRED + H)].index  # train에 LOOKBACK+H, test에 H => total >= LOOKBACK+2H\n",
    "# 만약 여기서도 0이면, 아래 줄을 더 완화:\n",
    "# valid_ids = lens[lens >= (LOOKBACK + H)].index\n",
    "\n",
    "panel = nixtla_df[nixtla_df[\"unique_id\"].isin(valid_ids)].copy()\n",
    "\n",
    "print(\"[debug] valid_ids:\", len(valid_ids))\n",
    "if len(valid_ids) == 0:\n",
    "    print(\"[debug] shortest series length top5:\\n\", lens.head(5))\n",
    "    print(\"[debug] longest series length top5:\\n\", lens.tail(5))\n",
    "    raise RuntimeError(\n",
    "        \"No valid_ids remain after length filtering. \"\n",
    "        \"Lower LOOKBACK/H or relax MIN_REQUIRED condition.\"\n",
    "    )\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 5) Holdout split (last H as test)\n",
    "# =========================\n",
    "def split_last_h(df_: pd.DataFrame, h: int) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    df_ = df_.sort_values([\"unique_id\", \"ds\"]).reset_index(drop=True)\n",
    "    test = df_.groupby(\"unique_id\", group_keys=False).tail(h)\n",
    "    train = df_.drop(test.index)\n",
    "    return train.reset_index(drop=True), test.reset_index(drop=True)\n",
    "\n",
    "train_df, test_df = split_last_h(panel, h=H)\n",
    "\n",
    "print(\"[debug] train unique_id:\", train_df[\"unique_id\"].nunique(), \"rows:\", len(train_df))\n",
    "print(\"[debug] test  unique_id:\", test_df[\"unique_id\"].nunique(), \"rows:\", len(test_df))\n",
    "\n",
    "if train_df[\"unique_id\"].nunique() == 0:\n",
    "    raise RuntimeError(\"train_df has zero groups. Check filtering/splitting logic.\")\n",
    "\n",
    "futr_df = test_df[[\"unique_id\", \"ds\"]].copy()\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 6) Nixtla PatchTST train + predict (holdout ds 사용)\n",
    "# =========================\n",
    "nixtla_model = NixtlaPatchTST(\n",
    "    h=H,\n",
    "    input_size=LOOKBACK,\n",
    "    patch_len=PATCH_LEN,\n",
    "    stride=STRIDE,\n",
    "    hidden_size=16,\n",
    "    n_heads=4,\n",
    "    start_padding_enabled=True,\n",
    "\n",
    "    scaler_type=\"standard\",\n",
    "    revin=True,\n",
    "\n",
    "    loss=DistributionLoss(distribution=\"StudentT\", level=[80, 90]),\n",
    "    learning_rate=1e-3,\n",
    "    max_steps=500,\n",
    "    val_check_steps=50,\n",
    "    early_stop_patience_steps=2,\n",
    "\n",
    "    accelerator=\"gpu\" if DEVICE == \"cuda\" else \"cpu\",\n",
    "    devices=1,\n",
    "    enable_progress_bar=False,\n",
    ")\n",
    "\n",
    "nf = NeuralForecast(models=[nixtla_model], freq=FREQ)\n",
    "\n",
    "# (중요) train이 짧다면 val_size=H가 다시 문제를 만들 수 있음 -> 안전하게 0으로도 가능\n",
    "val_size = H if (train_df.groupby(\"unique_id\")[\"y\"].size().min() >= (LOOKBACK + H)) else 0\n",
    "print(\"[debug] using val_size =\", val_size)\n",
    "\n",
    "nf.fit(df=train_df, val_size=val_size)\n",
    "\n",
    "fcst_nf = nf.predict(futr_df=futr_df).reset_index(drop=False)\n",
    "\n",
    "# 예측 컬럼명 추출 (DistributionLoss면 PatchTST-median이 있을 수도 있음)\n",
    "pred_cols = [c for c in fcst_nf.columns if c.startswith(\"PatchTST\")]\n",
    "NI_COL = \"PatchTST-median\" if \"PatchTST-median\" in pred_cols else \"PatchTST\"\n",
    "print(\"[debug] nixtla pred col:\", NI_COL)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 7) Our predict (DMSForecaster)\n",
    "# =========================\n",
    "our_rows = []\n",
    "for uid, g_tr in train_df.groupby(\"unique_id\", sort=False):\n",
    "    g_tr = g_tr.sort_values(\"ds\")\n",
    "    y_hist = g_tr[\"y\"].to_numpy(dtype=np.float32)\n",
    "\n",
    "    # 마지막 LOOKBACK window\n",
    "    x_win = y_hist[-LOOKBACK:]\n",
    "    x_t = torch.from_numpy(x_win).unsqueeze(0)  # (1, LOOKBACK)\n",
    "\n",
    "    out = forecaster.predict(x_t, horizon=H, device=DEVICE, mode=\"eval\", future_exo_cb = future_exo_cb_time)\n",
    "    y_hat = np.asarray(out[\"point\"], dtype=np.float32).reshape(-1)\n",
    "\n",
    "    g_te = test_df[test_df[\"unique_id\"] == uid].sort_values(\"ds\")\n",
    "    ds_te = g_te[\"ds\"].to_list()\n",
    "    y_te = g_te[\"y\"].to_numpy(dtype=np.float32)\n",
    "\n",
    "    if len(ds_te) != H:\n",
    "        raise RuntimeError(f\"uid={uid} test length != H. got {len(ds_te)}\")\n",
    "\n",
    "    for d, yt, yp in zip(ds_te, y_te, y_hat):\n",
    "        our_rows.append((uid, d, float(yt), float(yp)))\n",
    "\n",
    "our_df = pd.DataFrame(our_rows, columns=[\"unique_id\", \"ds\", \"y_true\", \"y_pred_ours\"])\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 8) Merge\n",
    "# =========================\n",
    "test_true = test_df.rename(columns={\"y\": \"y_true\"})\n",
    "nixtla_pred = fcst_nf[[\"unique_id\", \"ds\", NI_COL]].rename(columns={NI_COL: \"y_pred_nixtla\"})\n",
    "\n",
    "merged = (\n",
    "    test_true.merge(nixtla_pred, on=[\"unique_id\", \"ds\"], how=\"inner\")\n",
    "             .merge(our_df, on=[\"unique_id\", \"ds\", \"y_true\"], how=\"inner\")\n",
    ")\n",
    "\n",
    "merged_pl = pl.from_pandas(merged).sort([\"unique_id\", \"ds\"])\n",
    "print(merged_pl.head())\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 9) Metrics\n",
    "# =========================\n",
    "def mae(a, b): return float(np.mean(np.abs(a - b)))\n",
    "def rmse(a, b): return float(np.sqrt(np.mean((a - b) ** 2)))\n",
    "def smape(a, b):\n",
    "    denom = (np.abs(a) + np.abs(b)) + 1e-9\n",
    "    return float(np.mean(2.0 * np.abs(a - b) / denom))\n",
    "\n",
    "y = merged[\"y_true\"].to_numpy(np.float32)\n",
    "p_n = merged[\"y_pred_nixtla\"].to_numpy(np.float32)\n",
    "p_o = merged[\"y_pred_ours\"].to_numpy(np.float32)\n",
    "\n",
    "print(\"\\n[Overall Metrics]\")\n",
    "print(f\"Nixtla  MAE={mae(y,p_n):.4f} | RMSE={rmse(y,p_n):.4f} | sMAPE={smape(y,p_n):.4f}\")\n",
    "print(f\"Our     MAE={mae(y,p_o):.4f} | RMSE={rmse(y,p_o):.4f} | sMAPE={smape(y,p_o):.4f}\")\n",
    "\n",
    "per_id_mae = (\n",
    "    merged_pl.group_by(\"unique_id\")\n",
    "      .agg([\n",
    "          (pl.col(\"y_true\") - pl.col(\"y_pred_nixtla\")).abs().mean().alias(\"mae_nixtla\"),\n",
    "          (pl.col(\"y_true\") - pl.col(\"y_pred_ours\")).abs().mean().alias(\"mae_ours\"),\n",
    "      ])\n",
    "      .sort(\"mae_ours\")\n",
    ")\n",
    "print(\"\\n[Per-ID MAE]\")\n",
    "print(per_id_mae.head(10))\n",
    "\n",
    "per_id = (\n",
    "    merged_pl.group_by(\"unique_id\")\n",
    "    .agg([\n",
    "        (pl.col(\"y_true\") - pl.col(\"y_pred_nixtla\")).abs().mean().alias(\"mae_nixtla\"),\n",
    "        (pl.col(\"y_true\") - pl.col(\"y_pred_ours\")).abs().mean().alias(\"mae_ours\"),\n",
    "    ])\n",
    "    .with_columns((pl.col(\"mae_nixtla\") - pl.col(\"mae_ours\")).alias(\"mae_diff\"))  # +면 ours 승\n",
    ")\n",
    "\n",
    "print(\"ours wins:\", per_id.filter(pl.col(\"mae_diff\") > 0).height)\n",
    "print(\"nixtla wins:\", per_id.filter(pl.col(\"mae_diff\") < 0).height)\n",
    "print(\"ties:\", per_id.filter(pl.col(\"mae_diff\") == 0).height)\n",
    "\n",
    "print(\"\\n[Nixtla best 10]\")\n",
    "print(per_id.sort(\"mae_diff\").head(10))\n",
    "\n",
    "print(\"\\n[Ours best 10]\")\n",
    "print(per_id.sort(\"mae_diff\", descending=True).head(10))\n",
    "\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 10) Plot sample id\n",
    "# =========================\n",
    "def plot_all_uids_with_error(\n",
    "    merged_pl: pl.DataFrame,\n",
    "    uids,\n",
    "    *,\n",
    "    ncols: int = 1,\n",
    "    figsize_per_uid=(12, 6),\n",
    "    sharex: bool = False,\n",
    "):\n",
    "    \"\"\"\n",
    "    각 uid마다 (Forecast, Absolute Error) 2행 subplot을 만들고,\n",
    "    전체 uid를 하나의 fig에 그려서 복사/저장하기 쉽게 구성.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    merged_pl : pl.DataFrame\n",
    "        columns: unique_id, ds, y_true, y_pred_nixtla, y_pred_ours\n",
    "    uids : Iterable[str|int]\n",
    "        그릴 unique_id 목록\n",
    "    ncols : int\n",
    "        uid 패널을 가로로 몇 개 놓을지 (1이면 세로로만 쌓임)\n",
    "    figsize_per_uid : tuple\n",
    "        uid 1개(2행짜리)당 기본 figsize\n",
    "    sharex : bool\n",
    "        전체 축 sharex 여부 (uid별 기간이 다르면 False 권장)\n",
    "    \"\"\"\n",
    "    uids = [str(u) for u in uids]\n",
    "    n = len(uids)\n",
    "    if n == 0:\n",
    "        raise ValueError(\"uids is empty.\")\n",
    "\n",
    "    ncols = max(1, int(ncols))\n",
    "    nrows_uid = int(np.ceil(n / ncols))\n",
    "\n",
    "    # uid 1개당 2행이므로 전체 행 수는 2 * nrows_uid\n",
    "    total_rows = 2 * nrows_uid\n",
    "    fig_w = figsize_per_uid[0] * ncols\n",
    "    fig_h = figsize_per_uid[1] * nrows_uid\n",
    "\n",
    "    fig, axes = plt.subplots(\n",
    "        total_rows, ncols,\n",
    "        figsize=(fig_w, fig_h),\n",
    "        sharex=sharex,\n",
    "        squeeze=False\n",
    "    )\n",
    "\n",
    "    # axes indexing helper: uid index i -> (block_row, col)\n",
    "    for i, uid in enumerate(uids):\n",
    "        block_r = (i // ncols)  # uid block row\n",
    "        c = (i % ncols)\n",
    "\n",
    "        ax_fore = axes[2 * block_r][c]\n",
    "        ax_err  = axes[2 * block_r + 1][c]\n",
    "\n",
    "        p = (\n",
    "            merged_pl\n",
    "            .filter(pl.col(\"unique_id\") == uid)\n",
    "            .sort(\"ds\")\n",
    "            .to_pandas()\n",
    "        )\n",
    "        if len(p) == 0:\n",
    "            ax_fore.set_title(f\"Forecast (uid={uid}) - NO DATA\")\n",
    "            ax_fore.axis(\"off\")\n",
    "            ax_err.axis(\"off\")\n",
    "            continue\n",
    "\n",
    "        err_n = np.abs(p[\"y_true\"] - p[\"y_pred_nixtla\"])\n",
    "        err_o = np.abs(p[\"y_true\"] - p[\"y_pred_ours\"])\n",
    "\n",
    "        # --- Forecast\n",
    "        ax_fore.plot(p[\"ds\"], p[\"y_true\"], label=\"true\")\n",
    "        ax_fore.plot(p[\"ds\"], p[\"y_pred_nixtla\"], label=\"nixtla\")\n",
    "        ax_fore.plot(p[\"ds\"], p[\"y_pred_ours\"], label=\"ours\")\n",
    "        ax_fore.set_title(f\"Forecast (uid={uid})\")\n",
    "        ax_fore.legend(loc=\"best\")\n",
    "\n",
    "        # --- Error\n",
    "        ax_err.plot(p[\"ds\"], err_n, label=\"abs_err_nixtla\")\n",
    "        ax_err.plot(p[\"ds\"], err_o, label=\"abs_err_ours\")\n",
    "        ax_err.set_title(\"Absolute Error\")\n",
    "        ax_err.legend(loc=\"best\")\n",
    "\n",
    "    # 남는 subplot(빈칸) 끄기\n",
    "    for j in range(n, nrows_uid * ncols):\n",
    "        block_r = (j // ncols)\n",
    "        c = (j % ncols)\n",
    "        axes[2 * block_r][c].axis(\"off\")\n",
    "        axes[2 * block_r + 1][c].axis(\"off\")\n",
    "\n",
    "    fig.tight_layout()\n",
    "    return fig, axes\n",
    "\n",
    "# 사용 예시 1) 세로로 전부 쌓기 (복사하기 가장 직관적)\n",
    "fig, axes = plot_all_uids_with_error(merged_pl, valid_ids, ncols=1, sharex=False)"
   ],
   "id": "fb4b467a04d0f6c",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
