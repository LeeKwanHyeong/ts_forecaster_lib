{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# PatchTST AB Test (Walmart) — Refactored Notebook\n",
    "\n",
    "Variants:\n",
    "- **A0**: y-only (no exogenous)\n",
    "- **A1**: time/calendar exogenous (sin/cos)\n",
    "- **A2**: time/calendar + holiday (vectorized, batch-safe)\n",
    "\n",
    "Key fixes:\n",
    "- `compose_exo_calendar_cb` supports **batched** `start_idx` so the DataLoader collate can call it once per batch.\n",
    "- `get_train_loader(batch_size=...)` is called explicitly to avoid silent fallback to the default 32.\n"
   ],
   "id": "70944070f96a4c2f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T00:53:10.842869Z",
     "start_time": "2026-01-28T00:53:09.607406Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "SRC = \"/Users/igwanhyeong/PycharmProjects/ts_forecaster_lib/src\"\n",
    "if SRC not in sys.path:\n",
    "    sys.path.insert(0, SRC)\n",
    "\n",
    "\n",
    "# LTB modules (expected to exist in your repo)\n",
    "\n",
    "# optional\n",
    "\n",
    "'''\n",
    "pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128\n",
    "https://developer.nvidia.com/cuda-12-8-0-download-archive\n",
    "'''\n",
    "\n",
    "MAC_DIR = \"/Users/igwanhyeong/PycharmProjects/ts_forecaster_lib/raw_data/\"\n",
    "WINDOW_DIR = \"C:/Users/USER/PycharmProjects/ts_forecaster_lib/raw_data/\"\n",
    "\n",
    "DIR = WINDOW_DIR if sys.platform == \"win32\" else MAC_DIR\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(\"DIR:\", DIR)\n",
    "print(\"device:\", device)\n",
    "if device == \"cuda\":\n",
    "    print(\"cuda:\", torch.version.cuda, \"gpu_count:\", torch.cuda.device_count())"
   ],
   "id": "bc5f589c0a2d8ae0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DIR: C:/Users/USER/PycharmProjects/ts_forecaster_lib/raw_data/\n",
      "device: cuda\n",
      "cuda: 12.8 gpu_count: 1\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T00:53:11.722731Z",
     "start_time": "2026-01-28T00:53:11.705967Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = pl.read_parquet(DIR + 'train_data/walmart_best_feature_train.parquet')\n",
    "df"
   ],
   "id": "902b46160d02277c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "shape: (6_435, 38)\n",
       "┌───────────┬──────────┬────────┬────────────┬───┬────────────┬────────────┬───────────┬───────────┐\n",
       "│ unique_id ┆ date_idx ┆ date   ┆ y          ┆ … ┆ exo_p_mark ┆ exo_p_mark ┆ exo_p_mar ┆ exo_c_woy │\n",
       "│ ---       ┆ ---      ┆ ---    ┆ ---        ┆   ┆ down3      ┆ down4      ┆ kdown5    ┆ _bucket   │\n",
       "│ str       ┆ i64      ┆ i32    ┆ f32        ┆   ┆ ---        ┆ ---        ┆ ---       ┆ ---       │\n",
       "│           ┆          ┆        ┆            ┆   ┆ f32        ┆ f32        ┆ f32       ┆ i32       │\n",
       "╞═══════════╪══════════╪════════╪════════════╪═══╪════════════╪════════════╪═══════════╪═══════════╡\n",
       "│ 1         ┆ 14641    ┆ 201005 ┆ 1.6437e6   ┆ … ┆ 0.0        ┆ 0.0        ┆ 0.0       ┆ 1         │\n",
       "│ 1         ┆ 14648    ┆ 201006 ┆ 1641957.5  ┆ … ┆ 0.0        ┆ 0.0        ┆ 0.0       ┆ 1         │\n",
       "│ 1         ┆ 14655    ┆ 201007 ┆ 1.6120e6   ┆ … ┆ 0.0        ┆ 0.0        ┆ 0.0       ┆ 1         │\n",
       "│ 1         ┆ 14662    ┆ 201008 ┆ 1.4097e6   ┆ … ┆ 0.0        ┆ 0.0        ┆ 0.0       ┆ 2         │\n",
       "│ 1         ┆ 14669    ┆ 201009 ┆ 1.5548e6   ┆ … ┆ 0.0        ┆ 0.0        ┆ 0.0       ┆ 2         │\n",
       "│ …         ┆ …        ┆ …      ┆ …          ┆ … ┆ …          ┆ …          ┆ …         ┆ …         │\n",
       "│ 9         ┆ 15607    ┆ 201239 ┆ 516361.062 ┆ … ┆ 0.55       ┆ 190.380005 ┆ 1819.1500 ┆ 9         │\n",
       "│           ┆          ┆        ┆ 5          ┆   ┆            ┆            ┆ 24        ┆           │\n",
       "│ 9         ┆ 15614    ┆ 201240 ┆ 606755.312 ┆ … ┆ 3.01       ┆ 1107.79003 ┆ 1560.5500 ┆ 10        │\n",
       "│           ┆          ┆        ┆ 5          ┆   ┆            ┆ 9          ┆ 49        ┆           │\n",
       "│ 9         ┆ 15621    ┆ 201241 ┆ 558464.812 ┆ … ┆ 6.01       ┆ 0.0        ┆ 2839.8400 ┆ 10        │\n",
       "│           ┆          ┆        ┆ 5          ┆   ┆            ┆            ┆ 88        ┆           │\n",
       "│ 9         ┆ 15628    ┆ 201242 ┆ 542009.437 ┆ … ┆ 8.0        ┆ 28.940001  ┆ 3098.8701 ┆ 10        │\n",
       "│           ┆          ┆        ┆ 5          ┆   ┆            ┆            ┆ 17        ┆           │\n",
       "│ 9         ┆ 15635    ┆ 201243 ┆ 549731.5   ┆ … ┆ 8.0        ┆ 0.0        ┆ 1666.3800 ┆ 10        │\n",
       "│           ┆          ┆        ┆            ┆   ┆            ┆            ┆ 05        ┆           │\n",
       "└───────────┴──────────┴────────┴────────────┴───┴────────────┴────────────┴───────────┴───────────┘"
      ],
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (6_435, 38)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>unique_id</th><th>date_idx</th><th>date</th><th>y</th><th>exo_is_holiday</th><th>exo_temperature</th><th>exo_fuel_price</th><th>exo_cpi</th><th>exo_unemployment</th><th>exo_markdown_sum</th><th>exo_markdown1</th><th>exo_markdown2</th><th>exo_markdown3</th><th>exo_markdown4</th><th>exo_markdown5</th><th>exo_markdown1_isnull</th><th>exo_markdown2_isnull</th><th>exo_markdown3_isnull</th><th>exo_markdown4_isnull</th><th>exo_markdown5_isnull</th><th>exo_p_y_lag_1w</th><th>exo_p_y_lag_2w</th><th>exo_p_y_lag_52w</th><th>exo_p_y_rollmean_4w</th><th>exo_p_y_rollmean_12w</th><th>exo_p_y_rollstd_4w</th><th>exo_p_weeks_since_holiday</th><th>exo_p_temperature</th><th>exo_p_fuel_price</th><th>exo_p_cpi</th><th>exo_p_unemployment</th><th>exo_p_markdown_sum</th><th>exo_p_markdown1</th><th>exo_p_markdown2</th><th>exo_p_markdown3</th><th>exo_p_markdown4</th><th>exo_p_markdown5</th><th>exo_c_woy_bucket</th></tr><tr><td>str</td><td>i64</td><td>i32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>i32</td></tr></thead><tbody><tr><td>&quot;1&quot;</td><td>14641</td><td>201005</td><td>1.6437e6</td><td>0.0</td><td>42.310001</td><td>2.572</td><td>211.096359</td><td>8.106</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>1.0</td><td>1.0</td><td>1.0</td><td>1.0</td><td>1.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>999.0</td><td>42.310001</td><td>2.572</td><td>211.096359</td><td>8.106</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>1</td></tr><tr><td>&quot;1&quot;</td><td>14648</td><td>201006</td><td>1641957.5</td><td>1.0</td><td>38.509998</td><td>2.548</td><td>211.242172</td><td>8.106</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>1.0</td><td>1.0</td><td>1.0</td><td>1.0</td><td>1.0</td><td>1.6437e6</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>38.509998</td><td>2.548</td><td>211.242172</td><td>8.106</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>1</td></tr><tr><td>&quot;1&quot;</td><td>14655</td><td>201007</td><td>1.6120e6</td><td>0.0</td><td>39.93</td><td>2.514</td><td>211.289139</td><td>8.106</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>1.0</td><td>1.0</td><td>1.0</td><td>1.0</td><td>1.0</td><td>1641957.5</td><td>1.6437e6</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>1.0</td><td>39.93</td><td>2.514</td><td>211.289139</td><td>8.106</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>1</td></tr><tr><td>&quot;1&quot;</td><td>14662</td><td>201008</td><td>1.4097e6</td><td>0.0</td><td>46.630001</td><td>2.561</td><td>211.319641</td><td>8.106</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>1.0</td><td>1.0</td><td>1.0</td><td>1.0</td><td>1.0</td><td>1.6120e6</td><td>1641957.5</td><td>0.0</td><td>1.576836e6</td><td>0.0</td><td>112353.398438</td><td>2.0</td><td>46.630001</td><td>2.561</td><td>211.319641</td><td>8.106</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>2</td></tr><tr><td>&quot;1&quot;</td><td>14669</td><td>201009</td><td>1.5548e6</td><td>0.0</td><td>46.5</td><td>2.625</td><td>211.350143</td><td>8.106</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>1.0</td><td>1.0</td><td>1.0</td><td>1.0</td><td>1.0</td><td>1.4097e6</td><td>1.6120e6</td><td>0.0</td><td>1.554615e6</td><td>0.0</td><td>103135.0</td><td>3.0</td><td>46.5</td><td>2.625</td><td>211.350143</td><td>8.106</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>2</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>&quot;9&quot;</td><td>15607</td><td>201239</td><td>516361.0625</td><td>0.0</td><td>76.800003</td><td>3.666</td><td>226.763077</td><td>5.277</td><td>3711.670166</td><td>1699.680054</td><td>1.91</td><td>0.55</td><td>190.380005</td><td>1819.150024</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>533756.875</td><td>523427.34375</td><td>508567.03125</td><td>534839.375</td><td>536946.5625</td><td>21849.310547</td><td>3.0</td><td>76.800003</td><td>3.666</td><td>226.763077</td><td>5.277</td><td>3711.670166</td><td>1699.680054</td><td>1.91</td><td>0.55</td><td>190.380005</td><td>1819.150024</td><td>9</td></tr><tr><td>&quot;9&quot;</td><td>15614</td><td>201240</td><td>606755.3125</td><td>0.0</td><td>66.610001</td><td>3.617</td><td>226.966232</td><td>4.954</td><td>5328.919922</td><td>2657.570068</td><td>0.0</td><td>3.01</td><td>1107.790039</td><td>1560.550049</td><td>0.0</td><td>1.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>516361.0625</td><td>533756.875</td><td>553837.0</td><td>545075.125</td><td>542798.0625</td><td>41735.964844</td><td>4.0</td><td>66.610001</td><td>3.617</td><td>226.966232</td><td>4.954</td><td>5328.919922</td><td>2657.570068</td><td>0.0</td><td>3.01</td><td>1107.790039</td><td>1560.550049</td><td>10</td></tr><tr><td>&quot;9&quot;</td><td>15621</td><td>201241</td><td>558464.8125</td><td>0.0</td><td>60.09</td><td>3.601</td><td>227.169388</td><td>4.954</td><td>3366.26001</td><td>520.409973</td><td>0.0</td><td>6.01</td><td>0.0</td><td>2839.840088</td><td>0.0</td><td>1.0</td><td>0.0</td><td>1.0</td><td>0.0</td><td>606755.3125</td><td>516361.0625</td><td>529515.6875</td><td>553834.5</td><td>546504.1875</td><td>39282.828125</td><td>5.0</td><td>60.09</td><td>3.601</td><td>227.169388</td><td>4.954</td><td>3366.26001</td><td>520.409973</td><td>0.0</td><td>6.01</td><td>0.0</td><td>2839.840088</td><td>10</td></tr><tr><td>&quot;9&quot;</td><td>15628</td><td>201242</td><td>542009.4375</td><td>0.0</td><td>68.010002</td><td>3.594</td><td>227.214294</td><td>4.954</td><td>3681.530029</td><td>545.719971</td><td>0.0</td><td>8.0</td><td>28.940001</td><td>3098.870117</td><td>0.0</td><td>1.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>558464.8125</td><td>606755.3125</td><td>557075.1875</td><td>555897.625</td><td>550342.375</td><td>38074.996094</td><td>6.0</td><td>68.010002</td><td>3.594</td><td>227.214294</td><td>4.954</td><td>3681.530029</td><td>545.719971</td><td>0.0</td><td>8.0</td><td>28.940001</td><td>3098.870117</td><td>10</td></tr><tr><td>&quot;9&quot;</td><td>15635</td><td>201243</td><td>549731.5</td><td>0.0</td><td>69.519997</td><td>3.506</td><td>227.232803</td><td>4.954</td><td>2189.609863</td><td>512.22998</td><td>3.0</td><td>8.0</td><td>0.0</td><td>1666.380005</td><td>0.0</td><td>0.0</td><td>0.0</td><td>1.0</td><td>0.0</td><td>542009.4375</td><td>558464.8125</td><td>548527.5</td><td>564240.25</td><td>551662.6875</td><td>29129.589844</td><td>7.0</td><td>69.519997</td><td>3.506</td><td>227.232803</td><td>4.954</td><td>2189.609863</td><td>512.22998</td><td>3.0</td><td>8.0</td><td>0.0</td><td>1666.380005</td><td>10</td></tr></tbody></table></div>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T00:53:12.166037Z",
     "start_time": "2026-01-28T00:53:12.162385Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import random\n",
    "\n",
    "past_exo_cont_cols = (\n",
    "    # \"exo_p_y_lag_1w\",\n",
    "    \"exo_p_y_lag_2w\",\n",
    "    # \"exo_p_y_lag_52w\",\n",
    "    \"exo_p_y_rollmean_4w\",\"exo_p_y_rollmean_12w\",\"exo_p_y_rollstd_4w\",\n",
    "    # \"exo_p_weeks_since_holiday`\",\n",
    "    # \"exo_p_temperature\",\n",
    "    # \"exo_p_fuel_price\",\n",
    "    # \"exo_p_cpi\",\n",
    "    # \"exo_p_unemployment\",\n",
    "    # \"exo_p_markdown_sum\",\n",
    "    # \"exo_p_markdown1\",\n",
    "    # \"exo_p_markdown2\",\n",
    "    # \"exo_p_markdown3\",\n",
    "    # \"exo_p_markdown4\",\n",
    "    # \"exo_p_markdown5\",\n",
    "    # \"exo_markdown1_isnull\",\n",
    "    # \"exo_markdown2_isnull\",\n",
    "    # \"exo_markdown3_isnull\",\n",
    "    # \"exo_markdown4_isnull\",\n",
    "    # \"exo_markdown5_isnull\",\n",
    ")\n",
    "past_exo_cat_cols = (\n",
    "    # \"exo_c_woy_bucket\",\n",
    ")\n",
    "\n",
    "lookback = 52\n",
    "horizon = 27\n",
    "batch_size = 256\n",
    "\n",
    "freq = \"weekly\"          # walmart dt is weekly\n",
    "split_mode = \"multi\"     # id-disjoint split (leakage-safe)\n",
    "shuffle = True\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "print(\"device:\", device)\n"
   ],
   "id": "551c7de19d52e0bb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T00:53:12.733576Z",
     "start_time": "2026-01-28T00:53:12.720572Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "from modeling_module.utils.exogenous_utils import compose_exo_calendar_cb\n",
    "\n",
    "future_exo_cb_time = compose_exo_calendar_cb(date_type=freq)\n",
    "\n",
    "# ============================================================\n",
    "# Holiday lookup (vectorized) + FutureExo callback (time + holiday)\n",
    "# ============================================================\n",
    "holiday_map_dayidx = {\n",
    "    int(row[0]): float(row[1])\n",
    "    for row in (\n",
    "        df.select([\"date_idx\", \"exo_is_holiday\"])\n",
    "          .group_by(\"date_idx\")\n",
    "          .agg(pl.max(\"exo_is_holiday\").alias(\"exo_is_holiday\"))\n",
    "          .sort(\"date_idx\")\n",
    "          .iter_rows()\n",
    "    )\n",
    "}\n",
    "\n",
    "def build_holiday_array(holiday_map_dayidx: dict[int, float], *, pad: int = 0) -> np.ndarray:\n",
    "    if not holiday_map_dayidx:\n",
    "        return np.zeros((1,), dtype=np.float32)\n",
    "    max_k = max(int(k) for k in holiday_map_dayidx.keys())\n",
    "    arr = np.zeros((max_k + 1 + int(pad),), dtype=np.float32)\n",
    "    for k, v in holiday_map_dayidx.items():\n",
    "        kk = int(k)\n",
    "        if kk >= 0:\n",
    "            arr[kk] = float(v)\n",
    "    return arr\n",
    "\n",
    "class FutureExoTimePlusHoliday:\n",
    "    def __init__(self, holiday_by_dayidx: np.ndarray, *, step_days: int = 7):\n",
    "        self.holiday = holiday_by_dayidx.astype(np.float32, copy=False)\n",
    "        self.step_days = int(step_days)\n",
    "\n",
    "    def __call__(self, start_idx, H: int, device: str = \"cpu\"):\n",
    "        # 1) calendar exo (batch-safe)\n",
    "        cal = future_exo_cb_time(start_idx, H, device=device)  # scalar: (H,E) | batch: (B,H,E)\n",
    "\n",
    "        # 2) holiday exo (vectorized in numpy)\n",
    "        is_scalar = isinstance(start_idx, (int, np.integer))\n",
    "        if is_scalar:\n",
    "            s = np.asarray([int(start_idx)], dtype=np.int64)\n",
    "        else:\n",
    "            s = np.asarray(start_idx, dtype=np.int64).reshape(-1)\n",
    "\n",
    "        B = s.shape[0]\n",
    "        H = int(H)\n",
    "\n",
    "        offsets = (self.step_days * np.arange(H, dtype=np.int64))[None, :]  # (1,H)\n",
    "        idx = s[:, None] + offsets                                          # (B,H)\n",
    "\n",
    "        hol = np.zeros((B, H), dtype=np.float32)\n",
    "        valid = (idx >= 0) & (idx < self.holiday.shape[0])\n",
    "        hol[valid] = self.holiday[idx[valid]]\n",
    "        hol_t = torch.from_numpy(hol).unsqueeze(-1)                         # (B,H,1), CPU\n",
    "\n",
    "        target_device = cal.device  # cal이 이미 cuda일 수 있음\n",
    "        cal = cal.to(target_device, dtype=torch.float32)\n",
    "        hol_t = hol_t.to(target_device, dtype=torch.float32)\n",
    "\n",
    "        # 3) concat\n",
    "        if is_scalar:\n",
    "            out = torch.cat([cal.to(torch.float32).unsqueeze(0), hol_t], dim=-1)[0]  # (H,E+1)\n",
    "        else:\n",
    "            out = torch.cat([cal.to(torch.float32), hol_t], dim=-1)                  # (B,H,E+1)\n",
    "\n",
    "        return out\n",
    "\n",
    "holiday_by_dayidx = build_holiday_array(holiday_map_dayidx, pad=7 * (horizon + 2))\n",
    "future_exo_cb_time_plus_holiday = FutureExoTimePlusHoliday(holiday_by_dayidx, step_days=7)\n"
   ],
   "id": "acfbc0170b17f162",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T00:53:13.354427Z",
     "start_time": "2026-01-28T00:53:13.329430Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import importlib, inspect\n",
    "m = importlib.import_module(\"modeling_module.models.PatchTST.supervised.PatchTST\")\n",
    "\n",
    "print(\"[PatchTST module file]\", m.__file__)\n",
    "print(\"[DistHeadWithExo class]\", m.DistHeadWithExo)\n",
    "print(\"[DistHeadWithExo forward file]\", inspect.getsourcefile(m.DistHeadWithExo.forward))\n",
    "print(\"[DistHeadWithExo forward sig ]\", inspect.signature(m.DistHeadWithExo.forward))"
   ],
   "id": "562d5ca8830c45af",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PatchTST module file] C:\\Users\\USER\\PycharmProjects\\ts_forecaster_lib\\src\\modeling_module\\models\\PatchTST\\supervised\\PatchTST.py\n",
      "[DistHeadWithExo class] <class 'modeling_module.models.PatchTST.supervised.PatchTST.DistHeadWithExo'>\n",
      "[DistHeadWithExo forward file] C:\\Users\\USER\\PycharmProjects\\ts_forecaster_lib\\src\\modeling_module\\models\\PatchTST\\supervised\\PatchTST.py\n",
      "[DistHeadWithExo forward sig ] (self, h: torch.Tensor, *, future_exo: Optional[torch.Tensor] = None) -> torch.Tensor\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T00:53:14.004922Z",
     "start_time": "2026-01-28T00:53:14.000464Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import inspect\n",
    "import modeling_module.models.PatchTST.supervised.PatchTST as M\n",
    "\n",
    "print(\"PatchTST module file:\", M.__file__)\n",
    "print(\"Dist forward signature:\", inspect.signature(M.PatchTSTDistModel.forward))\n",
    "print(\"Dist forward head_out checker snippet:\\n\", \"\\n\".join(inspect.getsource(M.PatchTSTDistModel.forward).splitlines()[:40]))\n"
   ],
   "id": "61339651e409bbbb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PatchTST module file: C:\\Users\\USER\\PycharmProjects\\ts_forecaster_lib\\src\\modeling_module\\models\\PatchTST\\supervised\\PatchTST.py\n",
      "Dist forward signature: (self, x: torch.Tensor, future_exo: torch.Tensor | None = None, past_exo_cont: torch.Tensor | None = None, past_exo_cat: torch.Tensor | None = None, part_ids=None, mode: str | None = None, fe_cont: torch.Tensor | None = None, pe_cont: torch.Tensor | None = None, pe_cat: torch.Tensor | None = None, **kwargs)\n",
      "Dist forward head_out checker snippet:\n",
      "     def forward(\n",
      "        self,\n",
      "        x: torch.Tensor,\n",
      "        # 신규 인터페이스 (Trainer/Adapter 호환)\n",
      "        future_exo: torch.Tensor | None = None,\n",
      "        past_exo_cont: torch.Tensor | None = None,\n",
      "        past_exo_cat: torch.Tensor | None = None,\n",
      "        part_ids=None,\n",
      "        mode: str | None = None,\n",
      "        # 레거시 인터페이스 (하위 호환)\n",
      "        fe_cont: torch.Tensor | None = None,\n",
      "        pe_cont: torch.Tensor | None = None,\n",
      "        pe_cat: torch.Tensor | None = None,\n",
      "        **kwargs,\n",
      "    ):\n",
      "        # 0) alias 통일\n",
      "        fe = future_exo if future_exo is not None else fe_cont\n",
      "        p_cont = past_exo_cont if past_exo_cont is not None else pe_cont\n",
      "        p_cat  = past_exo_cat  if past_exo_cat  is not None else pe_cat\n",
      "\n",
      "        # 1) RevIN norm\n",
      "        x_n = self.revin_layer(x, \"norm\") if self.use_revin else x\n",
      "\n",
      "        # 2) Backbone에 과거 외생 주입 (중요)\n",
      "        h = self.backbone(x_n, p_cont=p_cont, p_cat=p_cat)\n",
      "\n",
      "        # 3) Head에 미래 외생 주입\n",
      "        head_out = self.head(h, future_exo=fe)\n",
      "\n",
      "        if not torch.is_tensor(head_out) or head_out.dim() != 3 or head_out.size(-1) != self.out_mult:\n",
      "            raise TypeError(\n",
      "                f\"[PatchTSTDistModel] head_out must be (B,H,{self.out_mult}), got {type(head_out)} {getattr(head_out, 'shape', None)}\")\n",
      "\n",
      "            # param_names 순서 그대로 분해\n",
      "            # e.g. Normal: [\"-loc\",\"-scale\"]\n",
      "            #      StudentT: [\"-df\",\"-loc\",\"-scale\"]\n",
      "        params = {name: head_out[..., i] for i, name in enumerate(self.param_names)}\n",
      "\n",
      "        # ---- loc 처리 (기존과 동일) ----\n",
      "        loc_n = params.get(\"-loc\")\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T00:53:33.022487Z",
     "start_time": "2026-01-28T00:53:15.587745Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from modeling_module.models.model_builder import build_patchTST_dist\n",
    "from modeling_module.training.model_losses.loss_module import DistributionLoss, MQLoss, MAE, HuberLoss, MSE\n",
    "from modeling_module.utils.metrics import mae, rmse, smape\n",
    "from modeling_module.utils.eval_utils import eval_on_loader, eval_on_loader_quantile\n",
    "from modeling_module.utils.checkpoint import load_model_dict\n",
    "from modeling_module.models import build_patchTST_quantile, build_patchTST_base\n",
    "from modeling_module.training.model_trainers.total_train import run_total_train_weekly\n",
    "from modeling_module.data_loader import MultiPartExoDataModule\n",
    "import pandas as pd\n",
    "rows = []  # seed loop 밖에서 선언\n",
    "\n",
    "def set_seed(seed: int = 11):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.backends.mps.is_available():\n",
    "        torch.mps.manual_seed(seed)\n",
    "    elif torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def build_datamodule(variant: str, seed: int) -> MultiPartExoDataModule:\n",
    "    if variant == \"A0\":\n",
    "        future_exo_cb = None\n",
    "    elif variant == \"A1\":\n",
    "        future_exo_cb = future_exo_cb_time\n",
    "    elif variant == \"A2\":\n",
    "        future_exo_cb = future_exo_cb_time_plus_holiday\n",
    "    else:\n",
    "        raise ValueError(variant)\n",
    "\n",
    "    return MultiPartExoDataModule(\n",
    "        df=df,\n",
    "        id_col=\"unique_id\",\n",
    "        date_col=\"date\",\n",
    "        y_col=\"y\",\n",
    "        lookback=lookback,\n",
    "        horizon=horizon,\n",
    "        batch_size=batch_size,\n",
    "        past_exo_cont_cols=past_exo_cont_cols,\n",
    "        past_exo_cat_cols=past_exo_cat_cols,\n",
    "        future_exo_cb=future_exo_cb,\n",
    "        freq=freq,\n",
    "        shuffle=shuffle,\n",
    "        split_mode=split_mode,\n",
    "        seed=seed,\n",
    "    )\n",
    "\n",
    "def inspect(loader, name):\n",
    "    b = next(iter(loader))\n",
    "    x, y, uid, fe, pe_cont, pe_cat = b\n",
    "    print(f\"[{name}] x:\", x.shape, x.device, x.dtype)\n",
    "    print(f\"[{name}] fe:\", fe.shape, fe.device, fe.dtype)\n",
    "    print(f\"[{name}] pe:\", pe_cont.shape, pe_cont.device, pe_cont.dtype)\n",
    "    print(f\"[{name}] future_exo_cb is None?\", loader.collate_fn.future_exo_cb is None)\n",
    "    if fe.shape[-1] > 0:\n",
    "        print(f\"[{name}] fe sample:\", fe[0, :3, :])\n",
    "\n",
    "# seed_list = [11, 22, 33, 44, 55]\n",
    "seed_list = [55]\n",
    "for seed in seed_list:\n",
    "    set_seed(seed)\n",
    "\n",
    "    save_dir = os.path.join(DIR, \"fit\", \"walmart_patchtst_ab\")\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    save_root_A0 = os.path.join(save_dir, \"A0_y_only\", f\"seed_{seed}\")\n",
    "    save_root_A1 = os.path.join(save_dir, \"A1_time_exog\", f\"seed_{seed}\")\n",
    "    save_root_A2 = os.path.join(save_dir, \"A2_time_holiday\", f\"seed_{seed}\")\n",
    "\n",
    "    data_module_A0 = build_datamodule(\"A0\", seed)\n",
    "    data_module_A1 = build_datamodule(\"A1\", seed)\n",
    "    data_module_A2 = build_datamodule(\"A2\", seed)\n",
    "\n",
    "    train_loader_A0 = data_module_A0.get_train_loader(batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    val_loader_A0   = data_module_A0.get_val_loader()\n",
    "\n",
    "    train_loader_A1 = data_module_A1.get_train_loader(batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    val_loader_A1   = data_module_A1.get_val_loader()\n",
    "\n",
    "    train_loader_A2 = data_module_A2.get_train_loader(batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    val_loader_A2   = data_module_A2.get_val_loader()\n",
    "\n",
    "    inspect(train_loader_A0, \"A0\")\n",
    "    inspect(train_loader_A1, \"A1\")\n",
    "    inspect(train_loader_A2, \"A2\")\n",
    "\n",
    "    # ============================================\n",
    "    # 학습 실행 (LTB total_train 포맷 유지)\n",
    "    # - 여기서는 \"외생변수 A/B/C\"만 비교하므로 use_ssl_pretrain=False로 고정\n",
    "    # Walmart처럼 항상 판매량이 있는(continuous) 데이터에서 “스파이크”를 잡는 규칙이:\n",
    "    # 스파이크 마스크가 과도하게 넓게 잡히거나(사실상 대부분 True)\n",
    "    # spike-loss가 MSE/제곱오차 기반인데 reduction이 sum 또는 정규화 없이 누적되어\n",
    "    # sales 스케일(1e4~1e5)에서 제곱오차가 1e8급으로 바로 올라가\n",
    "    # → 결과적으로 delta가 1e8 수준으로 튄다.\n",
    "    # → 그래서 최종적으로 이 상황에서는 spike_epoch를 0으로 잡아준다.\n",
    "    # ============================================\n",
    "    # ,\n",
    "    print('run result_A0')\n",
    "\n",
    "    results_A0 = run_total_train_weekly(\n",
    "        train_loader_A0,\n",
    "        val_loader_A0,\n",
    "        device=device,\n",
    "        lookback=lookback,\n",
    "        horizon=horizon,\n",
    "        warmup_epochs=20,\n",
    "        spike_epochs=0,\n",
    "        save_dir=save_root_A0,\n",
    "        loss = DistributionLoss(distribution=\"StudentT\", level=[80, 90]),\n",
    "        # loss = HuberLoss(delta = 5.0),\n",
    "        loss_quantile=MQLoss(quantiles=[0.1,0.5,0.99]),  # Quantile 학습용\n",
    "        models_to_run=[\"patchtst\"],\n",
    "        use_exogenous_mode=False,\n",
    "        use_ssl_mode = 'full',\n",
    "    )\n",
    "\n",
    "    print('run result_A1')\n",
    "    results_A1 = run_total_train_weekly(\n",
    "        train_loader_A1,\n",
    "        val_loader_A1,\n",
    "        device=device,\n",
    "        lookback=lookback,\n",
    "        horizon=horizon,\n",
    "        warmup_epochs=20,\n",
    "        spike_epochs=0,\n",
    "        save_dir=save_root_A1,\n",
    "        loss = DistributionLoss(distribution=\"StudentT\", level=[80, 90]),\n",
    "        # loss = HuberLoss(delta = 5.0),\n",
    "        loss_quantile=MQLoss(quantiles=[0.1,0.5,0.99]),  # Quantile 학습용\n",
    "        models_to_run=[\"patchtst\"],\n",
    "        use_exogenous_mode=True,\n",
    "        use_ssl_mode = 'full',\n",
    "    )\n",
    "\n",
    "    print('run result_A2')\n",
    "    results_A2 = run_total_train_weekly(\n",
    "        train_loader_A2,\n",
    "        val_loader_A2,\n",
    "        device=device,\n",
    "        lookback=lookback,\n",
    "        horizon=horizon,\n",
    "        warmup_epochs=20,\n",
    "        spike_epochs=0,\n",
    "        save_dir=save_root_A2,\n",
    "        loss = DistributionLoss(distribution=\"StudentT\", level=[80, 90]),\n",
    "        # loss = HuberLoss(delta = 5.0),\n",
    "        loss_quantile=MQLoss(quantiles=[0.1,0.5,0.99]),  # Quantile 학습용\n",
    "        models_to_run=[\"patchtst\"],\n",
    "        use_exogenous_mode=True,\n",
    "        use_ssl_mode = 'full',\n",
    "    )\n",
    "\n",
    "    builders = {\n",
    "    \"patchtst_quantile\": build_patchTST_quantile,\n",
    "    \"patchtst\": build_patchTST_base,\n",
    "    'patchtst_dist': build_patchTST_dist,\n",
    "    }\n",
    "\n",
    "    print(builders)\n",
    "\n",
    "    print(load_model_dict(save_root_A0, builders, device = device))\n",
    "\n",
    "\n",
    "    def to_point_pred(y_hat, *, quantiles=(0.1,0.5,0.9), q_star=0.5):\n",
    "        # dict 처리\n",
    "        if isinstance(y_hat, dict):\n",
    "            if \"y_hat\" in y_hat:\n",
    "                y_hat = y_hat[\"y_hat\"]\n",
    "            elif \"loc\" in y_hat:         # dist output\n",
    "                return y_hat[\"loc\"]\n",
    "\n",
    "        y_hat = np.asarray(y_hat)\n",
    "\n",
    "        # dist tensor: [B,H,2] -> loc\n",
    "        if y_hat.ndim >= 3 and y_hat.shape[-1] == 2:\n",
    "            return y_hat[..., 0]\n",
    "\n",
    "        # quantile: [B,H,Q] or [B,Q,H]\n",
    "        if y_hat.ndim == 3:\n",
    "            qs = list(quantiles)\n",
    "            if y_hat.shape[-1] == len(qs):     # [B,H,Q]\n",
    "                return y_hat[..., qs.index(q_star)]\n",
    "            if y_hat.shape[1] == len(qs):      # [B,Q,H]\n",
    "                return y_hat[:, qs.index(q_star), :]\n",
    "\n",
    "        # point: [B,H] or [B,H,1]\n",
    "        if y_hat.ndim == 3 and y_hat.shape[-1] == 1:\n",
    "            return y_hat[..., 0]\n",
    "        return y_hat\n",
    "\n",
    "    def squeeze_y(y):\n",
    "        y = np.asarray(y)\n",
    "        if y.ndim == 3 and y.shape[-1] == 1:\n",
    "            return y[..., 0]\n",
    "        return y\n",
    "    model_A0_b = load_model_dict(save_root_A0, builders, device = device)['patchtst']\n",
    "    model_A1_b = load_model_dict(save_root_A1, builders, device = device)['patchtst']\n",
    "    model_A2_b = load_model_dict(save_root_A2, builders, device = device)['patchtst']\n",
    "    y0, yhat0 = eval_on_loader(model_A0_b, val_loader_A0, device=device)\n",
    "    y1, yhat1 = eval_on_loader(model_A1_b, val_loader_A1, device=device, future_exo_cb = future_exo_cb_time)\n",
    "    y2, yhat2 = eval_on_loader(model_A2_b, val_loader_A2, device=device, future_exo_cb = future_exo_cb_time_plus_holiday)\n",
    "\n",
    "    metric_A0 = {\n",
    "    \"MAE\": float(mae(y0.reshape(-1), yhat0.reshape(-1))),\n",
    "    \"RMSE\": float(rmse(y0.reshape(-1), yhat0.reshape(-1))),\n",
    "    \"SMAPE\": float(smape(y0.reshape(-1), yhat0.reshape(-1))),\n",
    "    }\n",
    "    metric_A1 = {\n",
    "        \"MAE\": float(mae(y1.reshape(-1), yhat1.reshape(-1))),\n",
    "        \"RMSE\": float(rmse(y1.reshape(-1), yhat1.reshape(-1))),\n",
    "        \"SMAPE\": float(smape(y1.reshape(-1), yhat1.reshape(-1))),\n",
    "    }\n",
    "    metric_A2 = {\n",
    "        \"MAE\": float(mae(y2.reshape(-1), yhat2.reshape(-1))),\n",
    "        \"RMSE\": float(rmse(y2.reshape(-1), yhat2.reshape(-1))),\n",
    "        \"SMAPE\": float(smape(y2.reshape(-1), yhat2.reshape(-1))),\n",
    "    }\n",
    "\n",
    "    model_A0_d = load_model_dict(save_root_A0, builders, device = device)['patchtst_dist']\n",
    "    model_A1_d = load_model_dict(save_root_A1, builders, device = device)['patchtst_dist']\n",
    "    model_A2_d = load_model_dict(save_root_A2, builders, device = device)['patchtst_dist']\n",
    "\n",
    "    y0_d, yhat0_d = eval_on_loader(model_A0_d, val_loader_A0, device=device)\n",
    "    y0_d = squeeze_y(y0_d); yhat0_d = to_point_pred(yhat0_d, quantiles = (0.1, 0.5, 0.9), q_star = 0.5)\n",
    "    y1_d, yhat1_d = eval_on_loader(model_A1_d, val_loader_A1, device=device, future_exo_cb = future_exo_cb_time)\n",
    "    y1_d = squeeze_y(y1_d); yhat1_d = to_point_pred(yhat1_d, quantiles = (0.1, 0.5, 0.9), q_star = 0.5)\n",
    "    y2_d, yhat2_d = eval_on_loader(model_A2_d, val_loader_A2, device=device, future_exo_cb = future_exo_cb_time_plus_holiday)\n",
    "    y2_d = squeeze_y(y2_d); yhat2_d = to_point_pred(yhat2_d, quantiles = (0.1, 0.5, 0.9), q_star = 0.5)\n",
    "\n",
    "    metric_A0 = {\n",
    "    \"MAE\": float(mae(y0_d.reshape(-1), yhat0_d.reshape(-1))),\n",
    "    \"RMSE\": float(rmse(y0_d.reshape(-1), yhat0_d.reshape(-1))),\n",
    "    \"SMAPE\": float(smape(y0_d.reshape(-1), yhat0_d.reshape(-1))),\n",
    "    }\n",
    "    metric_A1 = {\n",
    "        \"MAE\": float(mae(y1_d.reshape(-1), yhat1_d.reshape(-1))),\n",
    "        \"RMSE\": float(rmse(y1_d.reshape(-1), yhat1_d.reshape(-1))),\n",
    "        \"SMAPE\": float(smape(y1_d.reshape(-1), yhat1_d.reshape(-1))),\n",
    "    }\n",
    "    metric_A2 = {\n",
    "        \"MAE\": float(mae(y2_d.reshape(-1), yhat2_d.reshape(-1))),\n",
    "        \"RMSE\": float(rmse(y2_d.reshape(-1), yhat2_d.reshape(-1))),\n",
    "        \"SMAPE\": float(smape(y2_d.reshape(-1), yhat2_d.reshape(-1))),\n",
    "    }\n",
    "\n",
    "    model_A0_q = load_model_dict(save_root_A0, builders, device = device)['patchtst_quantile']\n",
    "    model_A1_q = load_model_dict(save_root_A1, builders, device = device)['patchtst_quantile']\n",
    "    model_A2_q = load_model_dict(save_root_A2, builders, device = device)['patchtst_quantile']\n",
    "\n",
    "    y0_q, yhat0_q = eval_on_loader_quantile(model_A0_q, val_loader_A0, device=device)\n",
    "    y1_q, yhat1_q = eval_on_loader_quantile(model_A1_q, val_loader_A1, device=device, future_exo_cb = future_exo_cb_time)\n",
    "    y2_q, yhat2_q = eval_on_loader_quantile(model_A2_q, val_loader_A2, device=device, future_exo_cb = future_exo_cb_time_plus_holiday)\n",
    "\n",
    "    metric_A0_q = {\n",
    "    \"MAE\": float(mae(y0_q.reshape(-1), yhat0_q.reshape(-1))),\n",
    "    \"RMSE\": float(rmse(y0_q.reshape(-1), yhat0_q.reshape(-1))),\n",
    "    \"SMAPE\": float(smape(y0_q.reshape(-1), yhat0_q.reshape(-1))),\n",
    "    }\n",
    "    metric_A1_q = {\n",
    "        \"MAE\": float(mae(y1_q.reshape(-1), yhat1_q.reshape(-1))),\n",
    "        \"RMSE\": float(rmse(y1_q.reshape(-1), yhat1_q.reshape(-1))),\n",
    "        \"SMAPE\": float(smape(y1_q.reshape(-1), yhat1_q.reshape(-1))),\n",
    "    }\n",
    "    metric_A2_q = {\n",
    "        \"MAE\": float(mae(y2_q.reshape(-1), yhat2_q.reshape(-1))),\n",
    "        \"RMSE\": float(rmse(y2_q.reshape(-1), yhat2_q.reshape(-1))),\n",
    "        \"SMAPE\": float(smape(y2_q.reshape(-1), yhat2_q.reshape(-1))),\n",
    "    }\n",
    "\n",
    "    # -------------------------\n",
    "    # Point metrics row append\n",
    "    # -------------------------\n",
    "    rows.append({\n",
    "        \"seed\": seed,\n",
    "        \"variant\": \"A0\",\n",
    "        \"model_type\": \"point\",\n",
    "        \"MAE\": metric_A0[\"MAE\"],\n",
    "        \"RMSE\": metric_A0[\"RMSE\"],\n",
    "        \"SMAPE\": metric_A0[\"SMAPE\"],\n",
    "        \"save_root\": save_root_A0,\n",
    "    })\n",
    "    rows.append({\n",
    "        \"seed\": seed,\n",
    "        \"variant\": \"A1\",\n",
    "        \"model_type\": \"point\",\n",
    "        \"MAE\": metric_A1[\"MAE\"],\n",
    "        \"RMSE\": metric_A1[\"RMSE\"],\n",
    "        \"SMAPE\": metric_A1[\"SMAPE\"],\n",
    "        \"save_root\": save_root_A1,\n",
    "    })\n",
    "    rows.append({\n",
    "        \"seed\": seed,\n",
    "        \"variant\": \"A2\",\n",
    "        \"model_type\": \"point\",\n",
    "        \"MAE\": metric_A2[\"MAE\"],\n",
    "        \"RMSE\": metric_A2[\"RMSE\"],\n",
    "        \"SMAPE\": metric_A2[\"SMAPE\"],\n",
    "        \"save_root\": save_root_A2,\n",
    "    })\n",
    "\n",
    "    # -------------------------\n",
    "    # Quantile metrics row append\n",
    "    # (주의: q50 등 기준이 명확해야 함)\n",
    "    # -------------------------\n",
    "    rows.append({\n",
    "        \"seed\": seed,\n",
    "        \"variant\": \"A0\",\n",
    "        \"model_type\": \"quantile(q50)\",\n",
    "        \"MAE\": metric_A0_q[\"MAE\"],\n",
    "        \"RMSE\": metric_A0_q[\"RMSE\"],\n",
    "        \"SMAPE\": metric_A0_q[\"SMAPE\"],\n",
    "        \"save_root\": save_root_A0,\n",
    "    })\n",
    "    rows.append({\n",
    "        \"seed\": seed,\n",
    "        \"variant\": \"A1\",\n",
    "        \"model_type\": \"quantile(q50)\",\n",
    "        \"MAE\": metric_A1_q[\"MAE\"],\n",
    "        \"RMSE\": metric_A1_q[\"RMSE\"],\n",
    "        \"SMAPE\": metric_A1_q[\"SMAPE\"],\n",
    "        \"save_root\": save_root_A1,\n",
    "    })\n",
    "    rows.append({\n",
    "        \"seed\": seed,\n",
    "        \"variant\": \"A2\",\n",
    "        \"model_type\": \"quantile(q50)\",\n",
    "        \"MAE\": metric_A2_q[\"MAE\"],\n",
    "        \"RMSE\": metric_A2_q[\"RMSE\"],\n",
    "        \"SMAPE\": metric_A2_q[\"SMAPE\"],\n",
    "        \"save_root\": save_root_A2,\n",
    "    })\n",
    "\n",
    "# loop 종료 후 저장\n",
    "df_out = pd.DataFrame(rows)\n",
    "\n",
    "out_csv = os.path.join(save_dir, \"ab_results_by_seed.csv\")\n",
    "df_out.to_csv(out_csv, index=False)\n",
    "\n",
    "# variant별 mean/std 요약도 같이 저장 추천\n",
    "summary = (\n",
    "    df_out.groupby([\"variant\", \"model_type\"])[[\"MAE\", \"RMSE\", \"SMAPE\"]]\n",
    "    .agg([\"mean\", \"std\"])\n",
    "    .reset_index()\n",
    ")\n",
    "out_sum = os.path.join(save_dir, \"ab_results_summary.csv\")\n",
    "summary.to_csv(out_sum, index=False)\n",
    "\n",
    "print(\"saved:\", out_csv, out_sum)\n"
   ],
   "id": "74ac8e2b5faa0b3d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[A0] x: torch.Size([256, 52, 1]) cpu torch.float32\n",
      "[A0] fe: torch.Size([256, 27, 0]) cpu torch.float32\n",
      "[A0] pe: torch.Size([256, 52, 4]) cpu torch.float32\n",
      "[A0] future_exo_cb is None? True\n",
      "[A1] x: torch.Size([256, 52, 1]) cpu torch.float32\n",
      "[A1] fe: torch.Size([256, 27, 2]) cpu torch.float32\n",
      "[A1] pe: torch.Size([256, 52, 4]) cpu torch.float32\n",
      "[A1] future_exo_cb is None? False\n",
      "[A1] fe sample: tensor([[-0.7468,  0.6651],\n",
      "        [-0.6624,  0.7491],\n",
      "        [-0.5671,  0.8237]])\n",
      "[A2] x: torch.Size([256, 52, 1]) cpu torch.float32\n",
      "[A2] fe: torch.Size([256, 27, 3]) cpu torch.float32\n",
      "[A2] pe: torch.Size([256, 52, 4]) cpu torch.float32\n",
      "[A2] future_exo_cb is None? False\n",
      "[A2] fe sample: tensor([[0.8859, 0.4639, 0.0000],\n",
      "        [0.9355, 0.3534, 0.0000],\n",
      "        [0.9708, 0.2398, 0.0000]])\n",
      "run result_A0\n",
      "[total_train] use_exogenous_mode: False has_fe: True, fe_dim: 0\n",
      "\n",
      "[total_train] === RUN: patchtst (weekly) ===\n",
      "[SSL] PatchTST Pretrain (Weekly) -> C:\\Users\\USER\\PycharmProjects\\ts_forecaster_lib\\raw_data\\fit\\walmart_patchtst_ab\\A0_y_only\\seed_55\\pretrain\\patchtst_pretrain_best.pt\n",
      "[Pretrain] stage=0 epochs=10 lr=0.0001 wd=0.001 mask_ratio=0.3 loss=mse\n",
      "[Pretrain][stage=0 ep=1/10] train=1.063381 val=0.996034\n",
      "[Pretrain][stage=0 ep=2/10] train=0.997781 val=0.924337\n",
      "[Pretrain][stage=0 ep=3/10] train=0.918552 val=0.921649\n",
      "[Pretrain][stage=0 ep=4/10] train=0.874426 val=0.879438\n",
      "[Pretrain][stage=0 ep=5/10] train=0.884045 val=0.900793\n",
      "[Pretrain][stage=0 ep=6/10] train=0.867202 val=0.837803\n",
      "[Pretrain][stage=0 ep=7/10] train=0.832832 val=0.813097\n",
      "[Pretrain][stage=0 ep=8/10] train=0.783768 val=0.790240\n",
      "[Pretrain][stage=0 ep=9/10] train=0.797896 val=0.792132\n",
      "[Pretrain][stage=0 ep=10/10] train=0.765156 val=0.746070\n",
      "[Pretrain] done | best_val=0.746070\n",
      "[run_patchtst] mode:: dist\n",
      "[DBG-backbone-init] d_past_cont=0 cont_input_dim=0 target_input_dim=12 total_input_dim=12\n",
      "PatchTST Dist (Weekly)\n",
      "[Finetune] loaded pretrain ckpt: C:\\Users\\USER\\PycharmProjects\\ts_forecaster_lib\\raw_data\\fit\\walmart_patchtst_ab\\A0_y_only\\seed_55\\pretrain\\patchtst_pretrain_best.pt\n",
      "[Finetune] ckpt meta keys: ['best_val']\n",
      "[Finetune] matched: 73\n",
      "[Finetune] missing: 4 / total: 77 ratio=0.052\n",
      "[Finetune] missing sample (first 30): ['head.net.0.bias', 'head.net.0.weight', 'head.net.2.bias', 'head.net.2.weight']\n",
      "[Finetune] load_strict=False\n",
      "[Finetune] load_state_dict -> missing_keys=4 unexpected_keys=0\n",
      "  - missing (first 30): ['head.net.0.weight', 'head.net.0.bias', 'head.net.2.weight', 'head.net.2.bias']\n",
      "[train_patchtst] loss_mode: dist\n",
      "[train_patchtst] exogenous_mode: True\n",
      "[train_patchtst] dist head rebuilt: d_future 0 -> 0\n",
      "\n",
      "[train_patchtst] ===== Stage 1/1 =====\n",
      "  - spike: OFF\n",
      "  - epochs: 20 | lr=0.0001 | horizon_decay=False\n",
      "[train_patchtst] Effective TrainingConfig:\n",
      "{\n",
      "  \"device\": \"cuda\",\n",
      "  \"log_every\": 100,\n",
      "  \"use_amp\": true,\n",
      "  \"lookback\": 52,\n",
      "  \"horizon\": 27,\n",
      "  \"epochs\": 20,\n",
      "  \"lr\": 0.0001,\n",
      "  \"weight_decay\": 0.001,\n",
      "  \"t_max\": 40,\n",
      "  \"patience\": 100,\n",
      "  \"max_grad_norm\": 30.0,\n",
      "  \"amp_device\": \"cuda\",\n",
      "  \"loss\": \"DistributionLoss()\",\n",
      "  \"loss_mode\": \"auto\",\n",
      "  \"point_loss\": \"mse\",\n",
      "  \"huber_delta\": 0.8,\n",
      "  \"q_star\": 0.5,\n",
      "  \"use_cost_q_star\": false,\n",
      "  \"Cu\": 2.0,\n",
      "  \"Co\": 1.0,\n",
      "  \"quantiles\": [\n",
      "    0.1,\n",
      "    0.5,\n",
      "    0.9\n",
      "  ],\n",
      "  \"dist_name\": \"normal\",\n",
      "  \"dist_scale_transform\": \"softplus\",\n",
      "  \"dist_min_scale\": 1000,\n",
      "  \"dist_use_weights\": true,\n",
      "  \"dist_family\": \"normal\",\n",
      "  \"dist_eps\": 1e-08,\n",
      "  \"dist_scale_is_positive\": true,\n",
      "  \"use_intermittent\": true,\n",
      "  \"alpha_zero\": 0.3,\n",
      "  \"alpha_pos\": 1.0,\n",
      "  \"gamma_run\": 0.5,\n",
      "  \"cap\": null,\n",
      "  \"use_horizon_decay\": false,\n",
      "  \"tau_h\": 1.0,\n",
      "  \"val_use_weights\": true,\n",
      "  \"use_exogenous_mode\": false,\n",
      "  \"spike_loss\": {\n",
      "    \"enabled\": false,\n",
      "    \"strategy\": \"mix\",\n",
      "    \"huber_delta\": 0.8,\n",
      "    \"asym_up_weight\": 2.0,\n",
      "    \"asym_down_weight\": 1.0,\n",
      "    \"mad_k\": 3.5,\n",
      "    \"w_spike\": 6.0,\n",
      "    \"w_norm\": 1.0,\n",
      "    \"alpha_huber\": 1.0,\n",
      "    \"beta_asym\": 1.0,\n",
      "    \"mix_with_baseline\": false,\n",
      "    \"gamma_baseline\": 0.1\n",
      "  },\n",
      "  \"lambda_hist_scale\": 0.1,\n",
      "  \"lambda_hist_var\": 0.03,\n",
      "  \"hist_window\": 12,\n",
      "  \"anchor_last_k\": 8,\n",
      "  \"anchor_weight\": 0.05\n",
      "}\n",
      "[CommonTrainer] TrainingConfig (final)\n",
      "{\n",
      "  \"device\": \"cuda\",\n",
      "  \"log_every\": 100,\n",
      "  \"use_amp\": true,\n",
      "  \"lookback\": 52,\n",
      "  \"horizon\": 27,\n",
      "  \"epochs\": 20,\n",
      "  \"lr\": 0.0001,\n",
      "  \"weight_decay\": 0.001,\n",
      "  \"t_max\": 40,\n",
      "  \"patience\": 100,\n",
      "  \"max_grad_norm\": 30.0,\n",
      "  \"amp_device\": \"cuda\",\n",
      "  \"loss\": \"DistributionLoss()\",\n",
      "  \"loss_mode\": \"auto\",\n",
      "  \"point_loss\": \"mse\",\n",
      "  \"huber_delta\": 0.8,\n",
      "  \"q_star\": 0.5,\n",
      "  \"use_cost_q_star\": false,\n",
      "  \"Cu\": 2.0,\n",
      "  \"Co\": 1.0,\n",
      "  \"quantiles\": [\n",
      "    0.1,\n",
      "    0.5,\n",
      "    0.9\n",
      "  ],\n",
      "  \"dist_name\": \"normal\",\n",
      "  \"dist_scale_transform\": \"softplus\",\n",
      "  \"dist_min_scale\": 1000,\n",
      "  \"dist_use_weights\": true,\n",
      "  \"dist_family\": \"normal\",\n",
      "  \"dist_eps\": 1e-08,\n",
      "  \"dist_scale_is_positive\": true,\n",
      "  \"use_intermittent\": true,\n",
      "  \"alpha_zero\": 0.3,\n",
      "  \"alpha_pos\": 1.0,\n",
      "  \"gamma_run\": 0.5,\n",
      "  \"cap\": null,\n",
      "  \"use_horizon_decay\": false,\n",
      "  \"tau_h\": 1.0,\n",
      "  \"val_use_weights\": true,\n",
      "  \"use_exogenous_mode\": false,\n",
      "  \"spike_loss\": {\n",
      "    \"enabled\": false,\n",
      "    \"strategy\": \"mix\",\n",
      "    \"huber_delta\": 0.8,\n",
      "    \"asym_up_weight\": 2.0,\n",
      "    \"asym_down_weight\": 1.0,\n",
      "    \"mad_k\": 3.5,\n",
      "    \"w_spike\": 6.0,\n",
      "    \"w_norm\": 1.0,\n",
      "    \"alpha_huber\": 1.0,\n",
      "    \"beta_asym\": 1.0,\n",
      "    \"mix_with_baseline\": false,\n",
      "    \"gamma_baseline\": 0.1\n",
      "  },\n",
      "  \"lambda_hist_scale\": 0.1,\n",
      "  \"lambda_hist_var\": 0.03,\n",
      "  \"hist_window\": 12,\n",
      "  \"anchor_last_k\": 8,\n",
      "  \"anchor_weight\": 0.05\n",
      "}\n",
      "Epoch 1/20 | LR 0.000100 | Train 38.361605 | Val 39.634581\n",
      "Epoch 2/20 | LR 0.000099 | Train 38.276968 | Val 39.458452\n",
      "Epoch 3/20 | LR 0.000099 | Train 38.146361 | Val 39.207047\n",
      "Epoch 4/20 | LR 0.000098 | Train 37.888077 | Val 38.885190\n",
      "Epoch 5/20 | LR 0.000096 | Train 37.546780 | Val 38.532778\n",
      "Epoch 6/20 | LR 0.000095 | Train 37.200188 | Val 38.258685\n",
      "Epoch 7/20 | LR 0.000093 | Train 36.971723 | Val 38.025056\n",
      "Epoch 8/20 | LR 0.000090 | Train 36.667813 | Val 37.667371\n",
      "Epoch 9/20 | LR 0.000088 | Train 36.352242 | Val 37.371202\n",
      "Epoch 10/20 | LR 0.000085 | Train 36.125134 | Val 37.212134\n",
      "Epoch 11/20 | LR 0.000082 | Train 35.919814 | Val 36.993229\n",
      "Epoch 12/20 | LR 0.000079 | Train 35.760949 | Val 36.832492\n",
      "Epoch 13/20 | LR 0.000076 | Train 35.634283 | Val 36.683336\n",
      "Epoch 14/20 | LR 0.000073 | Train 35.524243 | Val 36.543462\n",
      "Epoch 15/20 | LR 0.000069 | Train 35.366797 | Val 36.359044\n",
      "Epoch 16/20 | LR 0.000065 | Train 35.208012 | Val 36.243693\n",
      "Epoch 17/20 | LR 0.000062 | Train 35.142680 | Val 36.176882\n",
      "Epoch 18/20 | LR 0.000058 | Train 35.042428 | Val 36.008274\n",
      "Epoch 19/20 | LR 0.000054 | Train 34.891986 | Val 35.916999\n",
      "Epoch 20/20 | LR 0.000050 | Train 34.805560 | Val 35.827257\n",
      "[EXO-train] inferred E=0 | future_exo_cb? False | exo_is_normalized=True\n",
      "PatchTSTDistModel(\n",
      "  (backbone): SupervisedBackbone(\n",
      "    (cat_embs): ModuleList()\n",
      "    (input_proj): Linear(in_features=12, out_features=256, bias=True)\n",
      "    (attn_core): FullAttention(\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (encoder): TSTEncoder(\n",
      "      (layers): ModuleList(\n",
      "        (0-2): 3 x TSTEncoderLayer(\n",
      "          (mha): MultiHeadAttention(\n",
      "            (W_Q): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (W_K): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (W_V): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (core): FullAttentionWithLogits(\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (to_out): Sequential(\n",
      "              (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (1): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (dropout_attn): Dropout(p=0.0, inplace=False)\n",
      "          (norm_attn): Sequential(\n",
      "            (0): Transpose()\n",
      "            (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): Transpose()\n",
      "          )\n",
      "          (ff): Sequential(\n",
      "            (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (1): GELU(approximate='none')\n",
      "            (2): Dropout(p=0.0, inplace=False)\n",
      "            (3): Linear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (dropout_ffn): Dropout(p=0.0, inplace=False)\n",
      "          (norm_ffn): Sequential(\n",
      "            (0): Transpose()\n",
      "            (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): Transpose()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (norm_out): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (revin_layer): RevIN()\n",
      "  (head): DistHeadWithExo(\n",
      "    (net): Sequential(\n",
      "      (0): Linear(in_features=256, out_features=128, bias=True)\n",
      "      (1): GELU(approximate='none')\n",
      "      (2): Linear(in_features=128, out_features=81, bias=True)\n",
      "    )\n",
      "  )\n",
      ") save success! C:\\Users\\USER\\PycharmProjects\\ts_forecaster_lib\\raw_data\\fit\\walmart_patchtst_ab\\A0_y_only\\seed_55\\weekly_PatchTSTDist_L52_H27.pt\n",
      "[DBG-backbone-init] d_past_cont=0 cont_input_dim=0 target_input_dim=12 total_input_dim=12\n",
      "PatchTST Quantile (Weekly)\n",
      "[Finetune] loaded pretrain ckpt: C:\\Users\\USER\\PycharmProjects\\ts_forecaster_lib\\raw_data\\fit\\walmart_patchtst_ab\\A0_y_only\\seed_55\\pretrain\\patchtst_pretrain_best.pt\n",
      "[Finetune] ckpt meta keys: ['best_val']\n",
      "[Finetune] matched: 73\n",
      "[Finetune] missing: 4 / total: 77 ratio=0.052\n",
      "[Finetune] missing sample (first 30): ['head.net.0.bias', 'head.net.0.weight', 'head.net.2.bias', 'head.net.2.weight']\n",
      "[Finetune] load_strict=False\n",
      "[Finetune] load_state_dict -> missing_keys=4 unexpected_keys=0\n",
      "  - missing (first 30): ['head.net.0.weight', 'head.net.0.bias', 'head.net.2.weight', 'head.net.2.bias']\n",
      "[train_patchtst] loss_mode: quantile\n",
      "[train_patchtst] exogenous_mode: True\n",
      "[train_patchtst] quantile head rebuilt: d_future 0 -> 0\n",
      "\n",
      "[train_patchtst] ===== Stage 1/1 =====\n",
      "  - spike: OFF\n",
      "  - epochs: 20 | lr=0.0001 | horizon_decay=False\n",
      "[train_patchtst] Effective TrainingConfig:\n",
      "{\n",
      "  \"device\": \"cuda\",\n",
      "  \"log_every\": 100,\n",
      "  \"use_amp\": true,\n",
      "  \"lookback\": 52,\n",
      "  \"horizon\": 27,\n",
      "  \"epochs\": 20,\n",
      "  \"lr\": 0.0001,\n",
      "  \"weight_decay\": 0.001,\n",
      "  \"t_max\": 40,\n",
      "  \"patience\": 100,\n",
      "  \"max_grad_norm\": 30.0,\n",
      "  \"amp_device\": \"cuda\",\n",
      "  \"loss\": \"MQLoss()\",\n",
      "  \"loss_mode\": \"auto\",\n",
      "  \"point_loss\": \"mse\",\n",
      "  \"huber_delta\": 0.8,\n",
      "  \"q_star\": 0.5,\n",
      "  \"use_cost_q_star\": false,\n",
      "  \"Cu\": 2.0,\n",
      "  \"Co\": 1.0,\n",
      "  \"quantiles\": [\n",
      "    0.1,\n",
      "    0.5,\n",
      "    0.9\n",
      "  ],\n",
      "  \"dist_name\": \"normal\",\n",
      "  \"dist_scale_transform\": \"softplus\",\n",
      "  \"dist_min_scale\": 1000,\n",
      "  \"dist_use_weights\": true,\n",
      "  \"dist_family\": \"normal\",\n",
      "  \"dist_eps\": 1e-08,\n",
      "  \"dist_scale_is_positive\": true,\n",
      "  \"use_intermittent\": true,\n",
      "  \"alpha_zero\": 0.3,\n",
      "  \"alpha_pos\": 1.0,\n",
      "  \"gamma_run\": 0.5,\n",
      "  \"cap\": null,\n",
      "  \"use_horizon_decay\": false,\n",
      "  \"tau_h\": 1.0,\n",
      "  \"val_use_weights\": true,\n",
      "  \"use_exogenous_mode\": false,\n",
      "  \"spike_loss\": {\n",
      "    \"enabled\": false,\n",
      "    \"strategy\": \"mix\",\n",
      "    \"huber_delta\": 2.0,\n",
      "    \"asym_up_weight\": 2.0,\n",
      "    \"asym_down_weight\": 1.0,\n",
      "    \"mad_k\": 3.5,\n",
      "    \"w_spike\": 6.0,\n",
      "    \"w_norm\": 1.0,\n",
      "    \"alpha_huber\": 0.7,\n",
      "    \"beta_asym\": 0.3,\n",
      "    \"mix_with_baseline\": false,\n",
      "    \"gamma_baseline\": 0.2\n",
      "  },\n",
      "  \"lambda_hist_scale\": 0.1,\n",
      "  \"lambda_hist_var\": 0.03,\n",
      "  \"hist_window\": 12,\n",
      "  \"anchor_last_k\": 8,\n",
      "  \"anchor_weight\": 0.05\n",
      "}\n",
      "[CommonTrainer] TrainingConfig (final)\n",
      "{\n",
      "  \"device\": \"cuda\",\n",
      "  \"log_every\": 100,\n",
      "  \"use_amp\": true,\n",
      "  \"lookback\": 52,\n",
      "  \"horizon\": 27,\n",
      "  \"epochs\": 20,\n",
      "  \"lr\": 0.0001,\n",
      "  \"weight_decay\": 0.001,\n",
      "  \"t_max\": 40,\n",
      "  \"patience\": 100,\n",
      "  \"max_grad_norm\": 30.0,\n",
      "  \"amp_device\": \"cuda\",\n",
      "  \"loss\": \"MQLoss()\",\n",
      "  \"loss_mode\": \"auto\",\n",
      "  \"point_loss\": \"mse\",\n",
      "  \"huber_delta\": 0.8,\n",
      "  \"q_star\": 0.5,\n",
      "  \"use_cost_q_star\": false,\n",
      "  \"Cu\": 2.0,\n",
      "  \"Co\": 1.0,\n",
      "  \"quantiles\": [\n",
      "    0.1,\n",
      "    0.5,\n",
      "    0.9\n",
      "  ],\n",
      "  \"dist_name\": \"normal\",\n",
      "  \"dist_scale_transform\": \"softplus\",\n",
      "  \"dist_min_scale\": 1000,\n",
      "  \"dist_use_weights\": true,\n",
      "  \"dist_family\": \"normal\",\n",
      "  \"dist_eps\": 1e-08,\n",
      "  \"dist_scale_is_positive\": true,\n",
      "  \"use_intermittent\": true,\n",
      "  \"alpha_zero\": 0.3,\n",
      "  \"alpha_pos\": 1.0,\n",
      "  \"gamma_run\": 0.5,\n",
      "  \"cap\": null,\n",
      "  \"use_horizon_decay\": false,\n",
      "  \"tau_h\": 1.0,\n",
      "  \"val_use_weights\": true,\n",
      "  \"use_exogenous_mode\": false,\n",
      "  \"spike_loss\": {\n",
      "    \"enabled\": false,\n",
      "    \"strategy\": \"mix\",\n",
      "    \"huber_delta\": 2.0,\n",
      "    \"asym_up_weight\": 2.0,\n",
      "    \"asym_down_weight\": 1.0,\n",
      "    \"mad_k\": 3.5,\n",
      "    \"w_spike\": 6.0,\n",
      "    \"w_norm\": 1.0,\n",
      "    \"alpha_huber\": 0.7,\n",
      "    \"beta_asym\": 0.3,\n",
      "    \"mix_with_baseline\": false,\n",
      "    \"gamma_baseline\": 0.2\n",
      "  },\n",
      "  \"lambda_hist_scale\": 0.1,\n",
      "  \"lambda_hist_var\": 0.03,\n",
      "  \"hist_window\": 12,\n",
      "  \"anchor_last_k\": 8,\n",
      "  \"anchor_weight\": 0.05\n",
      "}\n",
      "Epoch 1/20 | LR 0.000100 | Train 33880.963542 | Val 39344.858724\n",
      "Epoch 2/20 | LR 0.000099 | Train 28711.630425 | Val 35195.257812\n",
      "Epoch 3/20 | LR 0.000099 | Train 26507.960286 | Val 32810.671875\n",
      "Epoch 4/20 | LR 0.000098 | Train 25162.730252 | Val 31164.406250\n",
      "Epoch 5/20 | LR 0.000096 | Train 24245.802951 | Val 30076.595703\n",
      "Epoch 6/20 | LR 0.000095 | Train 23335.994792 | Val 29195.900391\n",
      "Epoch 7/20 | LR 0.000093 | Train 22745.025174 | Val 28561.942708\n",
      "Epoch 8/20 | LR 0.000090 | Train 22204.490017 | Val 27927.415365\n",
      "Epoch 9/20 | LR 0.000088 | Train 21800.987196 | Val 27360.960286\n",
      "Epoch 10/20 | LR 0.000085 | Train 21247.699653 | Val 26935.131510\n",
      "Epoch 11/20 | LR 0.000082 | Train 20828.623481 | Val 26493.733724\n",
      "Epoch 12/20 | LR 0.000079 | Train 20491.668186 | Val 26154.634115\n",
      "Epoch 13/20 | LR 0.000076 | Train 20190.018663 | Val 25799.589844\n",
      "Epoch 14/20 | LR 0.000073 | Train 19862.112630 | Val 25561.873698\n",
      "Epoch 15/20 | LR 0.000069 | Train 19646.789714 | Val 25317.283854\n",
      "Epoch 16/20 | LR 0.000065 | Train 19437.510417 | Val 25154.977214\n",
      "Epoch 17/20 | LR 0.000062 | Train 19163.883030 | Val 24907.453125\n",
      "Epoch 18/20 | LR 0.000058 | Train 18977.518663 | Val 24794.184245\n",
      "Epoch 19/20 | LR 0.000054 | Train 18788.501519 | Val 24634.265625\n",
      "Epoch 20/20 | LR 0.000050 | Train 18593.861762 | Val 24500.539062\n",
      "[EXO-train] inferred E=0 | future_exo_cb? False | exo_is_normalized=True\n",
      "PatchTSTQuantileModel(\n",
      "  (backbone): SupervisedBackbone(\n",
      "    (cat_embs): ModuleList()\n",
      "    (input_proj): Linear(in_features=12, out_features=256, bias=True)\n",
      "    (attn_core): FullAttention(\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (encoder): TSTEncoder(\n",
      "      (layers): ModuleList(\n",
      "        (0-2): 3 x TSTEncoderLayer(\n",
      "          (mha): MultiHeadAttention(\n",
      "            (W_Q): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (W_K): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (W_V): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (core): FullAttentionWithLogits(\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (to_out): Sequential(\n",
      "              (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (1): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (dropout_attn): Dropout(p=0.0, inplace=False)\n",
      "          (norm_attn): Sequential(\n",
      "            (0): Transpose()\n",
      "            (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): Transpose()\n",
      "          )\n",
      "          (ff): Sequential(\n",
      "            (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (1): GELU(approximate='none')\n",
      "            (2): Dropout(p=0.0, inplace=False)\n",
      "            (3): Linear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (dropout_ffn): Dropout(p=0.0, inplace=False)\n",
      "          (norm_ffn): Sequential(\n",
      "            (0): Transpose()\n",
      "            (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): Transpose()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (norm_out): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (head): QuantileHeadWithExo(\n",
      "    (net): Sequential(\n",
      "      (0): Linear(in_features=256, out_features=128, bias=True)\n",
      "      (1): GELU(approximate='none')\n",
      "      (2): Linear(in_features=128, out_features=81, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (revin_layer): RevIN()\n",
      ") save success! C:\\Users\\USER\\PycharmProjects\\ts_forecaster_lib\\raw_data\\fit\\walmart_patchtst_ab\\A0_y_only\\seed_55\\weekly_PatchTSTQuantile_L52_H27.pt\n",
      "run result_A1\n",
      "[total_train] use_exogenous_mode: True has_fe: True, fe_dim: 2\n",
      "[total_train] future exo from loader: fe_dim=2 (freq=weekly)\n",
      "\n",
      "[total_train] === RUN: patchtst (weekly) ===\n",
      "[SSL] PatchTST Pretrain (Weekly) -> C:\\Users\\USER\\PycharmProjects\\ts_forecaster_lib\\raw_data\\fit\\walmart_patchtst_ab\\A1_time_exog\\seed_55\\pretrain\\patchtst_pretrain_best.pt\n",
      "[Pretrain] stage=0 epochs=10 lr=0.0001 wd=0.001 mask_ratio=0.3 loss=mse\n",
      "[Pretrain][stage=0 ep=1/10] train=1.088141 val=0.988179\n",
      "[Pretrain][stage=0 ep=2/10] train=0.966096 val=0.971045\n",
      "[Pretrain][stage=0 ep=3/10] train=0.941556 val=0.921492\n",
      "[Pretrain][stage=0 ep=4/10] train=0.917030 val=0.850420\n",
      "[Pretrain][stage=0 ep=5/10] train=0.921253 val=0.863453\n",
      "[Pretrain][stage=0 ep=6/10] train=0.863040 val=0.856899\n",
      "[Pretrain][stage=0 ep=7/10] train=0.833534 val=0.851841\n",
      "[Pretrain][stage=0 ep=8/10] train=0.819764 val=0.846633\n",
      "[Pretrain][stage=0 ep=9/10] train=0.811297 val=0.830333\n",
      "[Pretrain][stage=0 ep=10/10] train=0.805671 val=0.767630\n",
      "[Pretrain] done | best_val=0.767630\n",
      "[run_patchtst] mode:: dist\n",
      "[DBG-backbone-init] d_past_cont=0 cont_input_dim=0 target_input_dim=12 total_input_dim=12\n",
      "PatchTST Dist (Weekly)\n",
      "[Finetune] loaded pretrain ckpt: C:\\Users\\USER\\PycharmProjects\\ts_forecaster_lib\\raw_data\\fit\\walmart_patchtst_ab\\A1_time_exog\\seed_55\\pretrain\\patchtst_pretrain_best.pt\n",
      "[Finetune] ckpt meta keys: ['best_val']\n",
      "[Finetune] matched: 73\n",
      "[Finetune] missing: 6 / total: 79 ratio=0.076\n",
      "[Finetune] missing sample (first 30): ['head.future_proj.bias', 'head.future_proj.weight', 'head.net.0.bias', 'head.net.0.weight', 'head.net.2.bias', 'head.net.2.weight']\n",
      "[Finetune] load_strict=False\n",
      "[Finetune] load_state_dict -> missing_keys=6 unexpected_keys=0\n",
      "  - missing (first 30): ['head.future_proj.weight', 'head.future_proj.bias', 'head.net.0.weight', 'head.net.0.bias', 'head.net.2.weight', 'head.net.2.bias']\n",
      "[train_patchtst] loss_mode: dist\n",
      "[train_patchtst] exogenous_mode: True\n",
      "[train_patchtst] dist head rebuilt: d_future 2 -> 2\n",
      "[train_patchtst] loader provides fe_cont(E=2), so future_exo_cb disabled.\n",
      "\n",
      "[train_patchtst] ===== Stage 1/1 =====\n",
      "  - spike: OFF\n",
      "  - epochs: 20 | lr=0.0001 | horizon_decay=False\n",
      "[train_patchtst] Effective TrainingConfig:\n",
      "{\n",
      "  \"device\": \"cuda\",\n",
      "  \"log_every\": 100,\n",
      "  \"use_amp\": true,\n",
      "  \"lookback\": 52,\n",
      "  \"horizon\": 27,\n",
      "  \"epochs\": 20,\n",
      "  \"lr\": 0.0001,\n",
      "  \"weight_decay\": 0.001,\n",
      "  \"t_max\": 40,\n",
      "  \"patience\": 100,\n",
      "  \"max_grad_norm\": 30.0,\n",
      "  \"amp_device\": \"cuda\",\n",
      "  \"loss\": \"DistributionLoss()\",\n",
      "  \"loss_mode\": \"auto\",\n",
      "  \"point_loss\": \"mse\",\n",
      "  \"huber_delta\": 0.8,\n",
      "  \"q_star\": 0.5,\n",
      "  \"use_cost_q_star\": false,\n",
      "  \"Cu\": 2.0,\n",
      "  \"Co\": 1.0,\n",
      "  \"quantiles\": [\n",
      "    0.1,\n",
      "    0.5,\n",
      "    0.9\n",
      "  ],\n",
      "  \"dist_name\": \"normal\",\n",
      "  \"dist_scale_transform\": \"softplus\",\n",
      "  \"dist_min_scale\": 1000,\n",
      "  \"dist_use_weights\": true,\n",
      "  \"dist_family\": \"normal\",\n",
      "  \"dist_eps\": 1e-08,\n",
      "  \"dist_scale_is_positive\": true,\n",
      "  \"use_intermittent\": true,\n",
      "  \"alpha_zero\": 0.3,\n",
      "  \"alpha_pos\": 1.0,\n",
      "  \"gamma_run\": 0.5,\n",
      "  \"cap\": null,\n",
      "  \"use_horizon_decay\": false,\n",
      "  \"tau_h\": 1.0,\n",
      "  \"val_use_weights\": true,\n",
      "  \"use_exogenous_mode\": true,\n",
      "  \"spike_loss\": {\n",
      "    \"enabled\": false,\n",
      "    \"strategy\": \"mix\",\n",
      "    \"huber_delta\": 0.8,\n",
      "    \"asym_up_weight\": 2.0,\n",
      "    \"asym_down_weight\": 1.0,\n",
      "    \"mad_k\": 3.5,\n",
      "    \"w_spike\": 6.0,\n",
      "    \"w_norm\": 1.0,\n",
      "    \"alpha_huber\": 1.0,\n",
      "    \"beta_asym\": 1.0,\n",
      "    \"mix_with_baseline\": false,\n",
      "    \"gamma_baseline\": 0.1\n",
      "  },\n",
      "  \"lambda_hist_scale\": 0.1,\n",
      "  \"lambda_hist_var\": 0.03,\n",
      "  \"hist_window\": 12,\n",
      "  \"anchor_last_k\": 8,\n",
      "  \"anchor_weight\": 0.05\n",
      "}\n",
      "[CommonTrainer] TrainingConfig (final)\n",
      "{\n",
      "  \"device\": \"cuda\",\n",
      "  \"log_every\": 100,\n",
      "  \"use_amp\": true,\n",
      "  \"lookback\": 52,\n",
      "  \"horizon\": 27,\n",
      "  \"epochs\": 20,\n",
      "  \"lr\": 0.0001,\n",
      "  \"weight_decay\": 0.001,\n",
      "  \"t_max\": 40,\n",
      "  \"patience\": 100,\n",
      "  \"max_grad_norm\": 30.0,\n",
      "  \"amp_device\": \"cuda\",\n",
      "  \"loss\": \"DistributionLoss()\",\n",
      "  \"loss_mode\": \"auto\",\n",
      "  \"point_loss\": \"mse\",\n",
      "  \"huber_delta\": 0.8,\n",
      "  \"q_star\": 0.5,\n",
      "  \"use_cost_q_star\": false,\n",
      "  \"Cu\": 2.0,\n",
      "  \"Co\": 1.0,\n",
      "  \"quantiles\": [\n",
      "    0.1,\n",
      "    0.5,\n",
      "    0.9\n",
      "  ],\n",
      "  \"dist_name\": \"normal\",\n",
      "  \"dist_scale_transform\": \"softplus\",\n",
      "  \"dist_min_scale\": 1000,\n",
      "  \"dist_use_weights\": true,\n",
      "  \"dist_family\": \"normal\",\n",
      "  \"dist_eps\": 1e-08,\n",
      "  \"dist_scale_is_positive\": true,\n",
      "  \"use_intermittent\": true,\n",
      "  \"alpha_zero\": 0.3,\n",
      "  \"alpha_pos\": 1.0,\n",
      "  \"gamma_run\": 0.5,\n",
      "  \"cap\": null,\n",
      "  \"use_horizon_decay\": false,\n",
      "  \"tau_h\": 1.0,\n",
      "  \"val_use_weights\": true,\n",
      "  \"use_exogenous_mode\": true,\n",
      "  \"spike_loss\": {\n",
      "    \"enabled\": false,\n",
      "    \"strategy\": \"mix\",\n",
      "    \"huber_delta\": 0.8,\n",
      "    \"asym_up_weight\": 2.0,\n",
      "    \"asym_down_weight\": 1.0,\n",
      "    \"mad_k\": 3.5,\n",
      "    \"w_spike\": 6.0,\n",
      "    \"w_norm\": 1.0,\n",
      "    \"alpha_huber\": 1.0,\n",
      "    \"beta_asym\": 1.0,\n",
      "    \"mix_with_baseline\": false,\n",
      "    \"gamma_baseline\": 0.1\n",
      "  },\n",
      "  \"lambda_hist_scale\": 0.1,\n",
      "  \"lambda_hist_var\": 0.03,\n",
      "  \"hist_window\": 12,\n",
      "  \"anchor_last_k\": 8,\n",
      "  \"anchor_weight\": 0.05\n",
      "}\n",
      "Epoch 1/20 | LR 0.000100 | Train 38.396817 | Val 39.648876\n",
      "Epoch 2/20 | LR 0.000099 | Train 38.210437 | Val 39.429451\n",
      "Epoch 3/20 | LR 0.000099 | Train 37.988019 | Val 39.219735\n",
      "Epoch 4/20 | LR 0.000098 | Train 37.745757 | Val 38.950808\n",
      "Epoch 5/20 | LR 0.000096 | Train 37.498889 | Val 38.686008\n",
      "Epoch 6/20 | LR 0.000095 | Train 37.226705 | Val 38.434900\n",
      "Epoch 7/20 | LR 0.000093 | Train 37.021537 | Val 38.228339\n",
      "Epoch 8/20 | LR 0.000090 | Train 36.847629 | Val 38.040114\n",
      "Epoch 9/20 | LR 0.000088 | Train 36.754166 | Val 37.896797\n",
      "Epoch 10/20 | LR 0.000085 | Train 36.647263 | Val 37.697740\n",
      "Epoch 11/20 | LR 0.000082 | Train 36.441833 | Val 37.472753\n",
      "Epoch 12/20 | LR 0.000079 | Train 36.224076 | Val 37.278618\n",
      "Epoch 13/20 | LR 0.000076 | Train 36.011519 | Val 37.091516\n",
      "Epoch 14/20 | LR 0.000073 | Train 35.837141 | Val 36.907018\n",
      "Epoch 15/20 | LR 0.000069 | Train 35.653123 | Val 36.744415\n",
      "Epoch 16/20 | LR 0.000065 | Train 35.494686 | Val 36.584454\n",
      "Epoch 17/20 | LR 0.000062 | Train 35.326854 | Val 36.445449\n",
      "Epoch 18/20 | LR 0.000058 | Train 35.224739 | Val 36.301618\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[7]\u001B[39m\u001B[32m, line 118\u001B[39m\n\u001B[32m    100\u001B[39m results_A0 = run_total_train_weekly(\n\u001B[32m    101\u001B[39m     train_loader_A0,\n\u001B[32m    102\u001B[39m     val_loader_A0,\n\u001B[32m   (...)\u001B[39m\u001B[32m    114\u001B[39m     use_ssl_mode = \u001B[33m'\u001B[39m\u001B[33mfull\u001B[39m\u001B[33m'\u001B[39m,\n\u001B[32m    115\u001B[39m )\n\u001B[32m    117\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m'\u001B[39m\u001B[33mrun result_A1\u001B[39m\u001B[33m'\u001B[39m)\n\u001B[32m--> \u001B[39m\u001B[32m118\u001B[39m results_A1 = \u001B[43mrun_total_train_weekly\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    119\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtrain_loader_A1\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    120\u001B[39m \u001B[43m    \u001B[49m\u001B[43mval_loader_A1\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    121\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    122\u001B[39m \u001B[43m    \u001B[49m\u001B[43mlookback\u001B[49m\u001B[43m=\u001B[49m\u001B[43mlookback\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    123\u001B[39m \u001B[43m    \u001B[49m\u001B[43mhorizon\u001B[49m\u001B[43m=\u001B[49m\u001B[43mhorizon\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    124\u001B[39m \u001B[43m    \u001B[49m\u001B[43mwarmup_epochs\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m20\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m    125\u001B[39m \u001B[43m    \u001B[49m\u001B[43mspike_epochs\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m0\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m    126\u001B[39m \u001B[43m    \u001B[49m\u001B[43msave_dir\u001B[49m\u001B[43m=\u001B[49m\u001B[43msave_root_A1\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    127\u001B[39m \u001B[43m    \u001B[49m\u001B[43mloss\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[43mDistributionLoss\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdistribution\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mStudentT\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlevel\u001B[49m\u001B[43m=\u001B[49m\u001B[43m[\u001B[49m\u001B[32;43m80\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m90\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    128\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m# loss = HuberLoss(delta = 5.0),\u001B[39;49;00m\n\u001B[32m    129\u001B[39m \u001B[43m    \u001B[49m\u001B[43mloss_quantile\u001B[49m\u001B[43m=\u001B[49m\u001B[43mMQLoss\u001B[49m\u001B[43m(\u001B[49m\u001B[43mquantiles\u001B[49m\u001B[43m=\u001B[49m\u001B[43m[\u001B[49m\u001B[32;43m0.1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[32;43m0.5\u001B[39;49m\u001B[43m,\u001B[49m\u001B[32;43m0.99\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Quantile 학습용\u001B[39;49;00m\n\u001B[32m    130\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmodels_to_run\u001B[49m\u001B[43m=\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mpatchtst\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    131\u001B[39m \u001B[43m    \u001B[49m\u001B[43muse_exogenous_mode\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    132\u001B[39m \u001B[43m    \u001B[49m\u001B[43muse_ssl_mode\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mfull\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m    133\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    135\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m'\u001B[39m\u001B[33mrun result_A2\u001B[39m\u001B[33m'\u001B[39m)\n\u001B[32m    136\u001B[39m results_A2 = run_total_train_weekly(\n\u001B[32m    137\u001B[39m     train_loader_A2,\n\u001B[32m    138\u001B[39m     val_loader_A2,\n\u001B[32m   (...)\u001B[39m\u001B[32m    150\u001B[39m     use_ssl_mode = \u001B[33m'\u001B[39m\u001B[33mfull\u001B[39m\u001B[33m'\u001B[39m,\n\u001B[32m    151\u001B[39m )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\ts_forecaster_lib\\src\\modeling_module\\training\\model_trainers\\total_train.py:1136\u001B[39m, in \u001B[36mrun_total_train_weekly\u001B[39m\u001B[34m(train_loader, val_loader, device, lookback, horizon, warmup_epochs, spike_epochs, base_lr, save_dir, use_exogenous_mode, models_to_run, loss_point, loss_quantile, loss, use_ssl_mode, ssl_pretrain_epochs, ssl_mask_ratio, ssl_loss_type, ssl_freeze_encoder_before_ft, ssl_pretrained_ckpt_path)\u001B[39m\n\u001B[32m   1111\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mrun_total_train_weekly\u001B[39m(\n\u001B[32m   1112\u001B[39m     train_loader,\n\u001B[32m   1113\u001B[39m     val_loader,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1134\u001B[39m     ssl_pretrained_ckpt_path: Optional[\u001B[38;5;28mstr\u001B[39m] = \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[32m   1135\u001B[39m ):\n\u001B[32m-> \u001B[39m\u001B[32m1136\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_run_total_train_generic\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1137\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1138\u001B[39m \u001B[43m        \u001B[49m\u001B[43mval_loader\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1139\u001B[39m \u001B[43m        \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1140\u001B[39m \u001B[43m        \u001B[49m\u001B[43mlookback\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1141\u001B[39m \u001B[43m        \u001B[49m\u001B[43mhorizon\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1142\u001B[39m \u001B[43m        \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mweekly\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m   1143\u001B[39m \u001B[43m        \u001B[49m\u001B[43msave_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1144\u001B[39m \u001B[43m        \u001B[49m\u001B[43muse_exogenous_mode\u001B[49m\u001B[43m=\u001B[49m\u001B[43muse_exogenous_mode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1145\u001B[39m \u001B[43m        \u001B[49m\u001B[43mwarmup_epochs\u001B[49m\u001B[43m=\u001B[49m\u001B[43mwarmup_epochs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1146\u001B[39m \u001B[43m        \u001B[49m\u001B[43mspike_epochs\u001B[49m\u001B[43m=\u001B[49m\u001B[43mspike_epochs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1147\u001B[39m \u001B[43m        \u001B[49m\u001B[43mbase_lr\u001B[49m\u001B[43m=\u001B[49m\u001B[43mbase_lr\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1148\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmodels_to_run\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmodels_to_run\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1149\u001B[39m \u001B[43m        \u001B[49m\u001B[43mloss_point\u001B[49m\u001B[43m=\u001B[49m\u001B[43mloss_point\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1150\u001B[39m \u001B[43m        \u001B[49m\u001B[43mloss_quantile\u001B[49m\u001B[43m=\u001B[49m\u001B[43mloss_quantile\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1151\u001B[39m \u001B[43m        \u001B[49m\u001B[43mloss\u001B[49m\u001B[43m=\u001B[49m\u001B[43mloss\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1152\u001B[39m \u001B[43m        \u001B[49m\u001B[43muse_ssl_mode\u001B[49m\u001B[43m=\u001B[49m\u001B[43muse_ssl_mode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1153\u001B[39m \u001B[43m        \u001B[49m\u001B[43mssl_pretrain_epochs\u001B[49m\u001B[43m=\u001B[49m\u001B[43mssl_pretrain_epochs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1154\u001B[39m \u001B[43m        \u001B[49m\u001B[43mssl_mask_ratio\u001B[49m\u001B[43m=\u001B[49m\u001B[43mssl_mask_ratio\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1155\u001B[39m \u001B[43m        \u001B[49m\u001B[43mssl_loss_type\u001B[49m\u001B[43m=\u001B[49m\u001B[43mssl_loss_type\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1156\u001B[39m \u001B[43m        \u001B[49m\u001B[43mssl_freeze_encoder_before_ft\u001B[49m\u001B[43m=\u001B[49m\u001B[43mssl_freeze_encoder_before_ft\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1157\u001B[39m \u001B[43m        \u001B[49m\u001B[43mssl_pretrained_ckpt_path\u001B[49m\u001B[43m=\u001B[49m\u001B[43mssl_pretrained_ckpt_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1158\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\ts_forecaster_lib\\src\\modeling_module\\training\\model_trainers\\total_train.py:1102\u001B[39m, in \u001B[36m_run_total_train_generic\u001B[39m\u001B[34m(train_loader, val_loader, device, lookback, horizon, freq, save_dir, use_exogenous_mode, models_to_run, warmup_epochs, spike_epochs, base_lr, loss_point, loss_quantile, loss, use_ssl_mode, ssl_pretrain_epochs, ssl_mask_ratio, ssl_loss_type, ssl_freeze_encoder_before_ft, ssl_pretrained_ckpt_path)\u001B[39m\n\u001B[32m   1099\u001B[39m         kwargs.pop(\u001B[33m\"\u001B[39m\u001B[33mquantile_train_cfg\u001B[39m\u001B[33m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[32m   1100\u001B[39m         kwargs.pop(\u001B[33m\"\u001B[39m\u001B[33mloss_quantile\u001B[39m\u001B[33m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[32m-> \u001B[39m\u001B[32m1102\u001B[39m     \u001B[43mMODEL_REGISTRY\u001B[49m\u001B[43m[\u001B[49m\u001B[43mm\u001B[49m\u001B[43m]\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1104\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m results\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\ts_forecaster_lib\\src\\modeling_module\\training\\model_trainers\\total_train.py:606\u001B[39m, in \u001B[36m_run_patchtst\u001B[39m\u001B[34m(results, freq, train_loader, val_loader, save_root, lookback, horizon, future_exo_cb, exo_dim, patch_len, stride, point_train_cfg, quantile_train_cfg, stages, device, loss_point, loss_quantile, use_exogenous_mode, use_ssl_mode, ssl_pretrain_epochs, ssl_mask_ratio, ssl_loss_type, ssl_freeze_encoder_before_ft, ssl_pretrained_ckpt_path)\u001B[39m\n\u001B[32m    603\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mname_base\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m (\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfreq.capitalize()\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m)\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m    605\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m (use_ssl_mode == \u001B[33m\"\u001B[39m\u001B[33mfull\u001B[39m\u001B[33m\"\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m (pretrain_ckpt_path \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[32m--> \u001B[39m\u001B[32m606\u001B[39m     best_pt_base = \u001B[43mtrain_patchtst_finetune\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    607\u001B[39m \u001B[43m        \u001B[49m\u001B[43mpt_base\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    608\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    609\u001B[39m \u001B[43m        \u001B[49m\u001B[43mval_loader\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    610\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtrain_cfg\u001B[49m\u001B[43m=\u001B[49m\u001B[43mpoint_train_cfg\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    611\u001B[39m \u001B[43m        \u001B[49m\u001B[43mstages\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mlist\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mstages\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    612\u001B[39m \u001B[43m        \u001B[49m\u001B[43mfuture_exo_cb\u001B[49m\u001B[43m=\u001B[49m\u001B[43mfuture_exo_cb\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    613\u001B[39m \u001B[43m        \u001B[49m\u001B[43mexo_is_normalized\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    614\u001B[39m \u001B[43m        \u001B[49m\u001B[43mpretrain_ckpt_path\u001B[49m\u001B[43m=\u001B[49m\u001B[43mpretrain_ckpt_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    615\u001B[39m \u001B[43m        \u001B[49m\u001B[43mload_strict\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    616\u001B[39m \u001B[43m        \u001B[49m\u001B[43mfreeze_encoder_before_ft\u001B[49m\u001B[43m=\u001B[49m\u001B[43mssl_freeze_encoder_before_ft\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    617\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    618\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    619\u001B[39m     best_pt_base = train_patchtst(\n\u001B[32m    620\u001B[39m         pt_base,\n\u001B[32m    621\u001B[39m         train_loader,\n\u001B[32m   (...)\u001B[39m\u001B[32m    626\u001B[39m         use_exogenous_mode=use_exogenous_mode,\n\u001B[32m    627\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\ts_forecaster_lib\\src\\modeling_module\\training\\model_trainers\\patchtst_finetune.py:435\u001B[39m, in \u001B[36mtrain_patchtst_finetune\u001B[39m\u001B[34m(model, train_loader, val_loader, train_cfg, stages, pretrain_ckpt_path, load_strict, freeze_encoder_before_ft, unfreeze_after_stage0, future_exo_cb, exo_is_normalized)\u001B[39m\n\u001B[32m    432\u001B[39m     \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33m[Finetune] encoder blocks frozen (stage0).\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m    434\u001B[39m \u001B[38;5;66;03m# 3) 지도학습 파이프라인 실행\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m435\u001B[39m out = \u001B[43mtrain_patchtst\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    436\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    437\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    438\u001B[39m \u001B[43m    \u001B[49m\u001B[43mval_loader\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    439\u001B[39m \u001B[43m    \u001B[49m\u001B[43mstages\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstages\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    440\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtrain_cfg\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtrain_cfg\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    441\u001B[39m \u001B[43m    \u001B[49m\u001B[43mfuture_exo_cb\u001B[49m\u001B[43m=\u001B[49m\u001B[43mfuture_exo_cb\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    442\u001B[39m \u001B[43m    \u001B[49m\u001B[43mexo_is_normalized\u001B[49m\u001B[43m=\u001B[49m\u001B[43mexo_is_normalized\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    443\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    445\u001B[39m \u001B[38;5;66;03m# 4) 학습 후 동결 해제 (옵션)\u001B[39;00m\n\u001B[32m    446\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m freeze_encoder_before_ft \u001B[38;5;129;01mand\u001B[39;00m unfreeze_after_stage0:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\ts_forecaster_lib\\src\\modeling_module\\training\\model_trainers\\patchtst_train.py:252\u001B[39m, in \u001B[36mtrain_patchtst\u001B[39m\u001B[34m(model, train_loader, val_loader, stages, train_cfg, future_exo_cb, exo_is_normalized, use_exogenous_mode)\u001B[39m\n\u001B[32m    242\u001B[39m     \u001B[38;5;66;03m# 트레이너 초기화 및 학습 수행\u001B[39;00m\n\u001B[32m    243\u001B[39m     trainer = CommonTrainer(\n\u001B[32m    244\u001B[39m         cfg=cfg_i,\n\u001B[32m    245\u001B[39m         adapter=adapter,\n\u001B[32m   (...)\u001B[39m\u001B[32m    250\u001B[39m         use_exogenous_mode=use_exogenous_mode\n\u001B[32m    251\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m252\u001B[39m     model = \u001B[43mtrainer\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtl_i\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtta_steps\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m0\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m    253\u001B[39m     best = {\u001B[33m\"\u001B[39m\u001B[33mmodel\u001B[39m\u001B[33m\"\u001B[39m: model, \u001B[33m\"\u001B[39m\u001B[33mcfg\u001B[39m\u001B[33m\"\u001B[39m: cfg_i}\n\u001B[32m    255\u001B[39m \u001B[38;5;28mprint\u001B[39m(\n\u001B[32m    256\u001B[39m     \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m[EXO-train] inferred E=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mE\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m | future_exo_cb? \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfuture_exo_cb\u001B[38;5;250m \u001B[39m\u001B[38;5;129;01mis\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;129;01mnot\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m | exo_is_normalized=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mexo_is_normalized\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\ts_forecaster_lib\\src\\modeling_module\\training\\engine.py:421\u001B[39m, in \u001B[36mCommonTrainer.fit\u001B[39m\u001B[34m(self, model, train_loader, val_loader, tta_steps)\u001B[39m\n\u001B[32m    417\u001B[39m     \u001B[38;5;28mself\u001B[39m.adapter.tta_reset(model)\n\u001B[32m    419\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mself\u001B[39m.cfg.epochs):\n\u001B[32m    420\u001B[39m     \u001B[38;5;66;03m# 1. 학습 루프 실행\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m421\u001B[39m     train_loss = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_run_epoch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[32m    423\u001B[39m     \u001B[38;5;66;03m# 2. 검증 루프 진입\u001B[39;00m\n\u001B[32m    424\u001B[39m     model.eval()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\ts_forecaster_lib\\src\\modeling_module\\training\\engine.py:346\u001B[39m, in \u001B[36mCommonTrainer._run_epoch\u001B[39m\u001B[34m(self, model, loader, train)\u001B[39m\n\u001B[32m    331\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m autocast(\n\u001B[32m    332\u001B[39m         device_type=\u001B[38;5;28mself\u001B[39m.cfg.amp_device,\n\u001B[32m    333\u001B[39m         enabled=\u001B[38;5;28mself\u001B[39m.amp_enabled,\n\u001B[32m    334\u001B[39m         dtype=\u001B[38;5;28mself\u001B[39m.dtype \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.dtype \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[33m\"\u001B[39m\u001B[33mfp32\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    335\u001B[39m ):\n\u001B[32m    336\u001B[39m     \u001B[38;5;66;03m# 어댑터를 경유한 모델 순전파 실행\u001B[39;00m\n\u001B[32m    337\u001B[39m     pred = \u001B[38;5;28mself\u001B[39m._forward_with_adapter(\n\u001B[32m    338\u001B[39m         model,\n\u001B[32m    339\u001B[39m         x,\n\u001B[32m   (...)\u001B[39m\u001B[32m    344\u001B[39m         mode=(\u001B[33m\"\u001B[39m\u001B[33mtrain\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m train \u001B[38;5;28;01melse\u001B[39;00m \u001B[33m\"\u001B[39m\u001B[33meval\u001B[39m\u001B[33m\"\u001B[39m),\n\u001B[32m    345\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m346\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_nan_stat\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mpred\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpred\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    348\u001B[39m     \u001B[38;5;66;03m# 손실 함수 계산 (Validation 여부 반영)\u001B[39;00m\n\u001B[32m    349\u001B[39m     loss = \u001B[38;5;28mself\u001B[39m.loss_comp.compute(pred, y, is_val=(\u001B[38;5;129;01mnot\u001B[39;00m train))\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\ts_forecaster_lib\\src\\modeling_module\\training\\engine.py:225\u001B[39m, in \u001B[36mCommonTrainer._nan_stat\u001B[39m\u001B[34m(self, name, t)\u001B[39m\n\u001B[32m    223\u001B[39m has_nan = torch.isnan(t).any().item()\n\u001B[32m    224\u001B[39m has_inf = torch.isinf(t).any().item()\n\u001B[32m--> \u001B[39m\u001B[32m225\u001B[39m finite_mask = \u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43misfinite\u001B[49m\u001B[43m(\u001B[49m\u001B[43mt\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    226\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m finite_mask.any():\n\u001B[32m    227\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ============================================\n",
    "# CELL 6) 체크포인트 로드 + 평가 (첨부 노트북 흐름 유지)\n",
    "# - quantile 모델이면 eval_on_loader_quantile 사용\n",
    "# - point 모델이면 eval_on_loader 사용\n",
    "# ============================================\n",
    "\n",
    "# builders는 \"당신이 실제 저장한 모델 키\"에 맞추세요.\n",
    "# (첨부 노트북에서는 patchtst_quantile을 사용)\n",
    "# builders = {\n",
    "#     \"patchtst_quantile\": build_patchTST_quantile,\n",
    "#     \"patchtst\": build_patchTST_base,\n",
    "# }\n",
    "\n",
    "# model_A0 = load_model_dict(save_root_A0, builders, device = device)['patchtst']\n",
    "# model_A1 = load_model_dict(save_root_A1, builders, device = device)['patchtst']\n",
    "# model_A2 = load_model_dict(save_root_A2, builders, device = device)['patchtst']\n",
    "# y0, yhat0 = eval_on_loader_quantile(model_A0_q, val_loader_A0, device=device)\n",
    "# y1, yhat1 = eval_on_loader_quantile(model_A1_q, val_loader_A1, device=device, future_exo_cb = future_exo_cb_time)\n",
    "# y2, yhat2 = eval_on_loader_quantile(model_A2_q, val_loader_A2, device=device, future_exo_cb = future_exo_cb_time_plus_holiday)\n",
    "\n",
    "# model_A0 = load_model_dict(save_root_A0, builders, device = device)['patchtst_quantile']\n",
    "# model_A1 = load_model_dict(save_root_A1, builders, device = device)['patchtst_quantile']\n",
    "# model_A2 = load_model_dict(save_root_A2, builders, device = device)['patchtst_quantile']\n",
    "# y0, yhat0 = eval_on_loader_quantile(model_A0_q, val_loader_A0, device=device)\n",
    "# y1, yhat1 = eval_on_loader_quantile(model_A1_q, val_loader_A1, device=device, future_exo_cb = future_exo_cb_time)\n",
    "# y2, yhat2 = eval_on_loader_quantile(model_A2_q, val_loader_A2, device=device, future_exo_cb = future_exo_cb_time_plus_holiday)\n",
    "\n",
    "\n",
    "model_A0 = load_model_dict(save_root_A0, builders, device = device)['patchtst_dist']\n",
    "model_A1 = load_model_dict(save_root_A1, builders, device = device)['patchtst_dist']\n",
    "model_A2 = load_model_dict(save_root_A2, builders, device = device)['patchtst_dist']\n",
    "y0, yhat0 = eval_on_loader(model_A0, val_loader_A0, device=device)\n",
    "y0 = squeeze_y(y0); yhat0 = to_point_pred(yhat0, quantiles = (0.1, 0.5, 0.9), q_star = 0.5)\n",
    "y1, yhat1 = eval_on_loader(model_A1, val_loader_A1, device=device, future_exo_cb = future_exo_cb_time)\n",
    "y1 = squeeze_y(y1); yhat1 = to_point_pred(yhat1, quantiles = (0.1, 0.5, 0.9), q_star = 0.5)\n",
    "y2, yhat2 = eval_on_loader(model_A2, val_loader_A2, device=device, future_exo_cb = future_exo_cb_time_plus_holiday)\n",
    "y2 = squeeze_y(y2); yhat2 = to_point_pred(yhat2, quantiles = (0.1, 0.5, 0.9), q_star = 0.5)\n",
    "\n",
    "\n",
    "metric_A0 = {\n",
    "    \"MAE\": float(mae(y0.reshape(-1), yhat0.reshape(-1))),\n",
    "    \"RMSE\": float(rmse(y0.reshape(-1), yhat0.reshape(-1))),\n",
    "    \"SMAPE\": float(smape(y0.reshape(-1), yhat0.reshape(-1))),\n",
    "}\n",
    "metric_A1 = {\n",
    "    \"MAE\": float(mae(y1.reshape(-1), yhat1.reshape(-1))),\n",
    "    \"RMSE\": float(rmse(y1.reshape(-1), yhat1.reshape(-1))),\n",
    "    \"SMAPE\": float(smape(y1.reshape(-1), yhat1.reshape(-1))),\n",
    "}\n",
    "metric_A2 = {\n",
    "    \"MAE\": float(mae(y2.reshape(-1), yhat2.reshape(-1))),\n",
    "    \"RMSE\": float(rmse(y2.reshape(-1), yhat2.reshape(-1))),\n",
    "    \"SMAPE\": float(smape(y2.reshape(-1), yhat2.reshape(-1))),\n",
    "}\n",
    "print(f'metric_A0: {metric_A0}')\n",
    "print(f'metric_A1: {metric_A1}')\n",
    "print(f'metric_A2: {metric_A2}')\n",
    "# metric_A0, metric_A1, metric_A2"
   ],
   "id": "7a02b5c85b381dfe",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ============================================\n",
    "# CELL 7) (선택) 예측 시각화 (첨부 노트북 스타일)\n",
    "# ============================================\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_samples(y_true, preds: dict, max_n: int = 64):\n",
    "    n = min(max_n, y_true.shape[0])\n",
    "    fig, axes = plt.subplots(nrows=n, ncols=1, figsize=(12, 2.2*n), sharex=True)\n",
    "\n",
    "    if n == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for i in range(n):\n",
    "        ax = axes[i]\n",
    "        ax.plot(y_true[i], label=\"true\")\n",
    "        for k, v in preds.items():\n",
    "            ax.plot(v[i], label=k)\n",
    "        ax.set_title(f\"sample={i}\", fontsize=9)\n",
    "        ax.legend(loc=\"upper right\", fontsize=8)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_samples(\n",
    "    y_true=y0,\n",
    "    preds={\n",
    "        \"A0_y_only\": yhat0,\n",
    "        \"A1_time\": yhat1,\n",
    "        \"A2_time+holiday\": yhat2,\n",
    "    },\n",
    "    max_n=32,\n",
    ")"
   ],
   "id": "66989d185bda7f1e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from modeling_module.training.forecater import DMSForecaster\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from modeling_module.utils.date_util import DateUtil\n",
    "\n",
    "from neuralforecast import NeuralForecast\n",
    "from neuralforecast.models import PatchTST as NixtlaPatchTST\n",
    "from neuralforecast.losses.pytorch import DistributionLoss\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 0) Reproducibility\n",
    "# =========================\n",
    "def set_seed(seed: int = 22):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(55)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 1) Config\n",
    "# =========================\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "H = 27\n",
    "LOOKBACK = 52\n",
    "PATCH_LEN = 24\n",
    "STRIDE = 24\n",
    "FREQ = \"W-MON\"\n",
    "\n",
    "PARQUET_PATH = DIR + \"train_data/walmart_best_feature_train.parquet\"\n",
    "\n",
    "# our_model\n",
    "our_model = model_A0\n",
    "our_model.eval()\n",
    "forecaster = DMSForecaster(our_model)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 2) Load polars -> pandas (Nixtla format)\n",
    "# =========================\n",
    "df = pl.read_parquet(PARQUET_PATH)\n",
    "\n",
    "# DateUtil을 통일해서 사용 (DateUtil.yyyyww_to_date)\n",
    "nixtla_df = (\n",
    "    df.select([\"unique_id\", \"date\", \"y\"])\n",
    "      .with_columns([\n",
    "          pl.col(\"unique_id\").cast(pl.Utf8),\n",
    "          pl.col(\"date\").map_elements(DateUtil.yyyyww_to_date, return_dtype=pl.Date).alias(\"ds\"),\n",
    "          pl.col(\"y\").cast(pl.Float64),\n",
    "      ])\n",
    "      .select([\"unique_id\", \"ds\", \"y\"])\n",
    "      .sort([\"unique_id\", \"ds\"])\n",
    "      .to_pandas()\n",
    ")\n",
    "nixtla_df[\"ds\"] = pd.to_datetime(nixtla_df[\"ds\"])\n",
    "nixtla_df[\"unique_id\"] = nixtla_df[\"unique_id\"].astype(str)\n",
    "\n",
    "print(\"[debug] raw unique_id:\", nixtla_df[\"unique_id\"].nunique())\n",
    "print(\"[debug] raw ds range:\", nixtla_df[\"ds\"].min(), \"~\", nixtla_df[\"ds\"].max())\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 3) Weekly reindex (안전 버전)\n",
    "#    - 같은 ds에 중복 row가 있으면 sum으로 집계 후 reindex\n",
    "# =========================\n",
    "def reindex_weekly_panel_sum(df_: pd.DataFrame, freq: str = \"W-MON\") -> pd.DataFrame:\n",
    "    out = []\n",
    "    for uid, g in df_.groupby(\"unique_id\", sort=False):\n",
    "        g = g.sort_values(\"ds\")\n",
    "\n",
    "        # (중요) 중복 ds를 sum으로 집계 (데이터에 중복 주차가 있으면 reindex가 꼬임)\n",
    "        g = g.groupby(\"ds\", as_index=False)[\"y\"].sum()\n",
    "\n",
    "        idx = pd.date_range(start=g[\"ds\"].min(), end=g[\"ds\"].max(), freq=freq)\n",
    "        gg = g.set_index(\"ds\").reindex(idx)\n",
    "        gg.index.name = \"ds\"\n",
    "        gg = gg.reset_index()\n",
    "        gg[\"unique_id\"] = uid\n",
    "        gg[\"y\"] = gg[\"y\"].fillna(0.0)\n",
    "\n",
    "        out.append(gg[[\"unique_id\", \"ds\", \"y\"]])\n",
    "    return pd.concat(out, ignore_index=True)\n",
    "\n",
    "nixtla_df = reindex_weekly_panel_sum(nixtla_df, freq=FREQ)\n",
    "\n",
    "print(\"[debug] after reindex unique_id:\", nixtla_df[\"unique_id\"].nunique())\n",
    "print(\"[debug] after reindex ds freq check sample:\")\n",
    "tmp = nixtla_df[nixtla_df[\"unique_id\"] == nixtla_df[\"unique_id\"].iloc[0]].sort_values(\"ds\")\n",
    "print(tmp[\"ds\"].diff().dropna().value_counts().head(3))\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 4) Length filter (여기가 0 만들기 가장 흔한 포인트)\n",
    "# =========================\n",
    "# val_size=H를 쓰면 train에 최소 (LOOKBACK + H)가 필요하고\n",
    "# holdout test로 H를 따로 빼니 전체는 LOOKBACK + 2H가 \"권장\"이지만,\n",
    "# 데이터가 짧다면 우선 완화해서 돌리는 게 맞습니다.\n",
    "MIN_REQUIRED = LOOKBACK + H  # <-- 완화 (기존 LOOKBACK + 2H -> 너무 빡빡해서 0될 가능성 큼)\n",
    "\n",
    "lens = nixtla_df.groupby(\"unique_id\")[\"y\"].size().sort_values()\n",
    "valid_ids = lens[lens >= (MIN_REQUIRED + H)].index  # train에 LOOKBACK+H, test에 H => total >= LOOKBACK+2H\n",
    "# 만약 여기서도 0이면, 아래 줄을 더 완화:\n",
    "# valid_ids = lens[lens >= (LOOKBACK + H)].index\n",
    "\n",
    "panel = nixtla_df[nixtla_df[\"unique_id\"].isin(valid_ids)].copy()\n",
    "\n",
    "print(\"[debug] valid_ids:\", len(valid_ids))\n",
    "if len(valid_ids) == 0:\n",
    "    print(\"[debug] shortest series length top5:\\n\", lens.head(5))\n",
    "    print(\"[debug] longest series length top5:\\n\", lens.tail(5))\n",
    "    raise RuntimeError(\n",
    "        \"No valid_ids remain after length filtering. \"\n",
    "        \"Lower LOOKBACK/H or relax MIN_REQUIRED condition.\"\n",
    "    )\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 5) Holdout split (last H as test)\n",
    "# =========================\n",
    "def split_last_h(df_: pd.DataFrame, h: int) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    df_ = df_.sort_values([\"unique_id\", \"ds\"]).reset_index(drop=True)\n",
    "    test = df_.groupby(\"unique_id\", group_keys=False).tail(h)\n",
    "    train = df_.drop(test.index)\n",
    "    return train.reset_index(drop=True), test.reset_index(drop=True)\n",
    "\n",
    "train_df, test_df = split_last_h(panel, h=H)\n",
    "\n",
    "print(\"[debug] train unique_id:\", train_df[\"unique_id\"].nunique(), \"rows:\", len(train_df))\n",
    "print(\"[debug] test  unique_id:\", test_df[\"unique_id\"].nunique(), \"rows:\", len(test_df))\n",
    "\n",
    "if train_df[\"unique_id\"].nunique() == 0:\n",
    "    raise RuntimeError(\"train_df has zero groups. Check filtering/splitting logic.\")\n",
    "\n",
    "futr_df = test_df[[\"unique_id\", \"ds\"]].copy()\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 6) Nixtla PatchTST train + predict (holdout ds 사용)\n",
    "# =========================\n",
    "nixtla_model = NixtlaPatchTST(\n",
    "    h=H,\n",
    "    input_size=LOOKBACK,\n",
    "    patch_len=PATCH_LEN,\n",
    "    stride=STRIDE,\n",
    "    hidden_size=16,\n",
    "    n_heads=4,\n",
    "    start_padding_enabled=True,\n",
    "\n",
    "    scaler_type=\"standard\",\n",
    "    revin=True,\n",
    "\n",
    "    loss=DistributionLoss(distribution=\"StudentT\", level=[80, 90]),\n",
    "    learning_rate=1e-3,\n",
    "    max_steps=500,\n",
    "    val_check_steps=50,\n",
    "    early_stop_patience_steps=2,\n",
    "\n",
    "    accelerator=\"gpu\" if DEVICE == \"cuda\" else \"cpu\",\n",
    "    devices=1,\n",
    "    enable_progress_bar=False,\n",
    ")\n",
    "\n",
    "nf = NeuralForecast(models=[nixtla_model], freq=FREQ)\n",
    "\n",
    "# (중요) train이 짧다면 val_size=H가 다시 문제를 만들 수 있음 -> 안전하게 0으로도 가능\n",
    "val_size = H if (train_df.groupby(\"unique_id\")[\"y\"].size().min() >= (LOOKBACK + H)) else 0\n",
    "print(\"[debug] using val_size =\", val_size)\n",
    "\n",
    "nf.fit(df=train_df, val_size=val_size)\n",
    "\n",
    "fcst_nf = nf.predict(futr_df=futr_df).reset_index(drop=False)\n",
    "\n",
    "# 예측 컬럼명 추출 (DistributionLoss면 PatchTST-median이 있을 수도 있음)\n",
    "pred_cols = [c for c in fcst_nf.columns if c.startswith(\"PatchTST\")]\n",
    "NI_COL = \"PatchTST-median\" if \"PatchTST-median\" in pred_cols else \"PatchTST\"\n",
    "print(\"[debug] nixtla pred col:\", NI_COL)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 7) Our predict (DMSForecaster)\n",
    "# =========================\n",
    "our_rows = []\n",
    "for uid, g_tr in train_df.groupby(\"unique_id\", sort=False):\n",
    "    g_tr = g_tr.sort_values(\"ds\")\n",
    "    y_hist = g_tr[\"y\"].to_numpy(dtype=np.float32)\n",
    "\n",
    "    # 마지막 LOOKBACK window\n",
    "    x_win = y_hist[-LOOKBACK:]\n",
    "    x_t = torch.from_numpy(x_win).unsqueeze(0)  # (1, LOOKBACK)\n",
    "\n",
    "    out = forecaster.predict(x_t, horizon=H, device=DEVICE, mode=\"eval\", future_exo_cb = future_exo_cb_time)\n",
    "    y_hat = np.asarray(out[\"point\"], dtype=np.float32).reshape(-1)\n",
    "\n",
    "    g_te = test_df[test_df[\"unique_id\"] == uid].sort_values(\"ds\")\n",
    "    ds_te = g_te[\"ds\"].to_list()\n",
    "    y_te = g_te[\"y\"].to_numpy(dtype=np.float32)\n",
    "\n",
    "    if len(ds_te) != H:\n",
    "        raise RuntimeError(f\"uid={uid} test length != H. got {len(ds_te)}\")\n",
    "\n",
    "    for d, yt, yp in zip(ds_te, y_te, y_hat):\n",
    "        our_rows.append((uid, d, float(yt), float(yp)))\n",
    "\n",
    "our_df = pd.DataFrame(our_rows, columns=[\"unique_id\", \"ds\", \"y_true\", \"y_pred_ours\"])\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 8) Merge\n",
    "# =========================\n",
    "test_true = test_df.rename(columns={\"y\": \"y_true\"})\n",
    "nixtla_pred = fcst_nf[[\"unique_id\", \"ds\", NI_COL]].rename(columns={NI_COL: \"y_pred_nixtla\"})\n",
    "\n",
    "merged = (\n",
    "    test_true.merge(nixtla_pred, on=[\"unique_id\", \"ds\"], how=\"inner\")\n",
    "             .merge(our_df, on=[\"unique_id\", \"ds\", \"y_true\"], how=\"inner\")\n",
    ")\n",
    "\n",
    "merged_pl = pl.from_pandas(merged).sort([\"unique_id\", \"ds\"])\n",
    "print(merged_pl.head())\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 9) Metrics\n",
    "# =========================\n",
    "def mae(a, b): return float(np.mean(np.abs(a - b)))\n",
    "def rmse(a, b): return float(np.sqrt(np.mean((a - b) ** 2)))\n",
    "def smape(a, b):\n",
    "    denom = (np.abs(a) + np.abs(b)) + 1e-9\n",
    "    return float(np.mean(2.0 * np.abs(a - b) / denom))\n",
    "\n",
    "y = merged[\"y_true\"].to_numpy(np.float32)\n",
    "p_n = merged[\"y_pred_nixtla\"].to_numpy(np.float32)\n",
    "p_o = merged[\"y_pred_ours\"].to_numpy(np.float32)\n",
    "\n",
    "print(\"\\n[Overall Metrics]\")\n",
    "print(f\"Nixtla  MAE={mae(y,p_n):.4f} | RMSE={rmse(y,p_n):.4f} | sMAPE={smape(y,p_n):.4f}\")\n",
    "print(f\"Our     MAE={mae(y,p_o):.4f} | RMSE={rmse(y,p_o):.4f} | sMAPE={smape(y,p_o):.4f}\")\n",
    "\n",
    "per_id_mae = (\n",
    "    merged_pl.group_by(\"unique_id\")\n",
    "      .agg([\n",
    "          (pl.col(\"y_true\") - pl.col(\"y_pred_nixtla\")).abs().mean().alias(\"mae_nixtla\"),\n",
    "          (pl.col(\"y_true\") - pl.col(\"y_pred_ours\")).abs().mean().alias(\"mae_ours\"),\n",
    "      ])\n",
    "      .sort(\"mae_ours\")\n",
    ")\n",
    "print(\"\\n[Per-ID MAE]\")\n",
    "print(per_id_mae.head(10))\n",
    "\n",
    "per_id = (\n",
    "    merged_pl.group_by(\"unique_id\")\n",
    "    .agg([\n",
    "        (pl.col(\"y_true\") - pl.col(\"y_pred_nixtla\")).abs().mean().alias(\"mae_nixtla\"),\n",
    "        (pl.col(\"y_true\") - pl.col(\"y_pred_ours\")).abs().mean().alias(\"mae_ours\"),\n",
    "    ])\n",
    "    .with_columns((pl.col(\"mae_nixtla\") - pl.col(\"mae_ours\")).alias(\"mae_diff\"))  # +면 ours 승\n",
    ")\n",
    "\n",
    "print(\"ours wins:\", per_id.filter(pl.col(\"mae_diff\") > 0).height)\n",
    "print(\"nixtla wins:\", per_id.filter(pl.col(\"mae_diff\") < 0).height)\n",
    "print(\"ties:\", per_id.filter(pl.col(\"mae_diff\") == 0).height)\n",
    "\n",
    "print(\"\\n[Nixtla best 10]\")\n",
    "print(per_id.sort(\"mae_diff\").head(10))\n",
    "\n",
    "print(\"\\n[Ours best 10]\")\n",
    "print(per_id.sort(\"mae_diff\", descending=True).head(10))\n",
    "\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 10) Plot sample id\n",
    "# =========================\n",
    "def plot_all_uids_with_error(\n",
    "    merged_pl: pl.DataFrame,\n",
    "    uids,\n",
    "    *,\n",
    "    ncols: int = 1,\n",
    "    figsize_per_uid=(12, 6),\n",
    "    sharex: bool = False,\n",
    "):\n",
    "    \"\"\"\n",
    "    각 uid마다 (Forecast, Absolute Error) 2행 subplot을 만들고,\n",
    "    전체 uid를 하나의 fig에 그려서 복사/저장하기 쉽게 구성.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    merged_pl : pl.DataFrame\n",
    "        columns: unique_id, ds, y_true, y_pred_nixtla, y_pred_ours\n",
    "    uids : Iterable[str|int]\n",
    "        그릴 unique_id 목록\n",
    "    ncols : int\n",
    "        uid 패널을 가로로 몇 개 놓을지 (1이면 세로로만 쌓임)\n",
    "    figsize_per_uid : tuple\n",
    "        uid 1개(2행짜리)당 기본 figsize\n",
    "    sharex : bool\n",
    "        전체 축 sharex 여부 (uid별 기간이 다르면 False 권장)\n",
    "    \"\"\"\n",
    "    uids = [str(u) for u in uids]\n",
    "    n = len(uids)\n",
    "    if n == 0:\n",
    "        raise ValueError(\"uids is empty.\")\n",
    "\n",
    "    ncols = max(1, int(ncols))\n",
    "    nrows_uid = int(np.ceil(n / ncols))\n",
    "\n",
    "    # uid 1개당 2행이므로 전체 행 수는 2 * nrows_uid\n",
    "    total_rows = 2 * nrows_uid\n",
    "    fig_w = figsize_per_uid[0] * ncols\n",
    "    fig_h = figsize_per_uid[1] * nrows_uid\n",
    "\n",
    "    fig, axes = plt.subplots(\n",
    "        total_rows, ncols,\n",
    "        figsize=(fig_w, fig_h),\n",
    "        sharex=sharex,\n",
    "        squeeze=False\n",
    "    )\n",
    "\n",
    "    # axes indexing helper: uid index i -> (block_row, col)\n",
    "    for i, uid in enumerate(uids):\n",
    "        block_r = (i // ncols)  # uid block row\n",
    "        c = (i % ncols)\n",
    "\n",
    "        ax_fore = axes[2 * block_r][c]\n",
    "        ax_err  = axes[2 * block_r + 1][c]\n",
    "\n",
    "        p = (\n",
    "            merged_pl\n",
    "            .filter(pl.col(\"unique_id\") == uid)\n",
    "            .sort(\"ds\")\n",
    "            .to_pandas()\n",
    "        )\n",
    "        if len(p) == 0:\n",
    "            ax_fore.set_title(f\"Forecast (uid={uid}) - NO DATA\")\n",
    "            ax_fore.axis(\"off\")\n",
    "            ax_err.axis(\"off\")\n",
    "            continue\n",
    "\n",
    "        err_n = np.abs(p[\"y_true\"] - p[\"y_pred_nixtla\"])\n",
    "        err_o = np.abs(p[\"y_true\"] - p[\"y_pred_ours\"])\n",
    "\n",
    "        # --- Forecast\n",
    "        ax_fore.plot(p[\"ds\"], p[\"y_true\"], label=\"true\")\n",
    "        ax_fore.plot(p[\"ds\"], p[\"y_pred_nixtla\"], label=\"nixtla\")\n",
    "        ax_fore.plot(p[\"ds\"], p[\"y_pred_ours\"], label=\"ours\")\n",
    "        ax_fore.set_title(f\"Forecast (uid={uid})\")\n",
    "        ax_fore.legend(loc=\"best\")\n",
    "\n",
    "        # --- Error\n",
    "        ax_err.plot(p[\"ds\"], err_n, label=\"abs_err_nixtla\")\n",
    "        ax_err.plot(p[\"ds\"], err_o, label=\"abs_err_ours\")\n",
    "        ax_err.set_title(\"Absolute Error\")\n",
    "        ax_err.legend(loc=\"best\")\n",
    "\n",
    "    # 남는 subplot(빈칸) 끄기\n",
    "    for j in range(n, nrows_uid * ncols):\n",
    "        block_r = (j // ncols)\n",
    "        c = (j % ncols)\n",
    "        axes[2 * block_r][c].axis(\"off\")\n",
    "        axes[2 * block_r + 1][c].axis(\"off\")\n",
    "\n",
    "    fig.tight_layout()\n",
    "    return fig, axes\n",
    "\n",
    "# 사용 예시 1) 세로로 전부 쌓기 (복사하기 가장 직관적)\n",
    "fig, axes = plot_all_uids_with_error(merged_pl, valid_ids, ncols=1, sharex=False)"
   ],
   "id": "fb4b467a04d0f6c",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
