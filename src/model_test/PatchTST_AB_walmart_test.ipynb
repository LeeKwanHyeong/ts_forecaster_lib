{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# PatchTST AB Test (Walmart) — Refactored Notebook\n",
    "\n",
    "Variants:\n",
    "- **A0**: y-only (no exogenous)\n",
    "- **A1**: time/calendar exogenous (sin/cos)\n",
    "- **A2**: time/calendar + holiday (vectorized, batch-safe)\n",
    "\n",
    "Key fixes:\n",
    "- `compose_exo_calendar_cb` supports **batched** `start_idx` so the DataLoader collate can call it once per batch.\n",
    "- `get_train_loader(batch_size=...)` is called explicitly to avoid silent fallback to the default 32.\n"
   ],
   "id": "16ad9283735bdd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-24T01:43:19.710514Z",
     "start_time": "2026-01-24T01:43:18.432582Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "\n",
    "# LTB modules (expected to exist in your repo)\n",
    "\n",
    "# optional\n",
    "\n",
    "'''\n",
    "pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128\n",
    "https://developer.nvidia.com/cuda-12-8-0-download-archive\n",
    "'''\n",
    "\n",
    "MAC_DIR = \"/Users/igwanhyeong/PycharmProjects/ts_forecaster_lib/raw_data/\"\n",
    "WINDOW_DIR = \"C:/Users/USER/PycharmProjects/ts_forecaster_lib/raw_data/\"\n",
    "\n",
    "DIR = WINDOW_DIR if sys.platform == \"win32\" else MAC_DIR\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(\"DIR:\", DIR)\n",
    "print(\"device:\", device)\n",
    "if device == \"cuda\":\n",
    "    print(\"cuda:\", torch.version.cuda, \"gpu_count:\", torch.cuda.device_count())"
   ],
   "id": "490b15e474327ccd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DIR: C:/Users/USER/PycharmProjects/ts_forecaster_lib/raw_data/\n",
      "device: cuda\n",
      "cuda: 12.8 gpu_count: 1\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-24T01:43:19.733165Z",
     "start_time": "2026-01-24T01:43:19.716045Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = pl.read_parquet(DIR + 'train_data/walmart_best_feature_train.parquet')\n",
    "df"
   ],
   "id": "abbf742533efa0ba",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "shape: (6_435, 38)\n",
       "┌───────────┬──────────┬────────┬────────────┬───┬────────────┬────────────┬───────────┬───────────┐\n",
       "│ unique_id ┆ date_idx ┆ date   ┆ y          ┆ … ┆ exo_p_mark ┆ exo_p_mark ┆ exo_p_mar ┆ exo_c_woy │\n",
       "│ ---       ┆ ---      ┆ ---    ┆ ---        ┆   ┆ down3      ┆ down4      ┆ kdown5    ┆ _bucket   │\n",
       "│ str       ┆ i64      ┆ i32    ┆ f32        ┆   ┆ ---        ┆ ---        ┆ ---       ┆ ---       │\n",
       "│           ┆          ┆        ┆            ┆   ┆ f32        ┆ f32        ┆ f32       ┆ i32       │\n",
       "╞═══════════╪══════════╪════════╪════════════╪═══╪════════════╪════════════╪═══════════╪═══════════╡\n",
       "│ 1         ┆ 14641    ┆ 201005 ┆ 1.6437e6   ┆ … ┆ 0.0        ┆ 0.0        ┆ 0.0       ┆ 1         │\n",
       "│ 1         ┆ 14648    ┆ 201006 ┆ 1641957.5  ┆ … ┆ 0.0        ┆ 0.0        ┆ 0.0       ┆ 1         │\n",
       "│ 1         ┆ 14655    ┆ 201007 ┆ 1.6120e6   ┆ … ┆ 0.0        ┆ 0.0        ┆ 0.0       ┆ 1         │\n",
       "│ 1         ┆ 14662    ┆ 201008 ┆ 1.4097e6   ┆ … ┆ 0.0        ┆ 0.0        ┆ 0.0       ┆ 2         │\n",
       "│ 1         ┆ 14669    ┆ 201009 ┆ 1.5548e6   ┆ … ┆ 0.0        ┆ 0.0        ┆ 0.0       ┆ 2         │\n",
       "│ …         ┆ …        ┆ …      ┆ …          ┆ … ┆ …          ┆ …          ┆ …         ┆ …         │\n",
       "│ 9         ┆ 15607    ┆ 201239 ┆ 516361.062 ┆ … ┆ 0.55       ┆ 190.380005 ┆ 1819.1500 ┆ 9         │\n",
       "│           ┆          ┆        ┆ 5          ┆   ┆            ┆            ┆ 24        ┆           │\n",
       "│ 9         ┆ 15614    ┆ 201240 ┆ 606755.312 ┆ … ┆ 3.01       ┆ 1107.79003 ┆ 1560.5500 ┆ 10        │\n",
       "│           ┆          ┆        ┆ 5          ┆   ┆            ┆ 9          ┆ 49        ┆           │\n",
       "│ 9         ┆ 15621    ┆ 201241 ┆ 558464.812 ┆ … ┆ 6.01       ┆ 0.0        ┆ 2839.8400 ┆ 10        │\n",
       "│           ┆          ┆        ┆ 5          ┆   ┆            ┆            ┆ 88        ┆           │\n",
       "│ 9         ┆ 15628    ┆ 201242 ┆ 542009.437 ┆ … ┆ 8.0        ┆ 28.940001  ┆ 3098.8701 ┆ 10        │\n",
       "│           ┆          ┆        ┆ 5          ┆   ┆            ┆            ┆ 17        ┆           │\n",
       "│ 9         ┆ 15635    ┆ 201243 ┆ 549731.5   ┆ … ┆ 8.0        ┆ 0.0        ┆ 1666.3800 ┆ 10        │\n",
       "│           ┆          ┆        ┆            ┆   ┆            ┆            ┆ 05        ┆           │\n",
       "└───────────┴──────────┴────────┴────────────┴───┴────────────┴────────────┴───────────┴───────────┘"
      ],
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (6_435, 38)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>unique_id</th><th>date_idx</th><th>date</th><th>y</th><th>exo_is_holiday</th><th>exo_temperature</th><th>exo_fuel_price</th><th>exo_cpi</th><th>exo_unemployment</th><th>exo_markdown_sum</th><th>exo_markdown1</th><th>exo_markdown2</th><th>exo_markdown3</th><th>exo_markdown4</th><th>exo_markdown5</th><th>exo_markdown1_isnull</th><th>exo_markdown2_isnull</th><th>exo_markdown3_isnull</th><th>exo_markdown4_isnull</th><th>exo_markdown5_isnull</th><th>exo_p_y_lag_1w</th><th>exo_p_y_lag_2w</th><th>exo_p_y_lag_52w</th><th>exo_p_y_rollmean_4w</th><th>exo_p_y_rollmean_12w</th><th>exo_p_y_rollstd_4w</th><th>exo_p_weeks_since_holiday</th><th>exo_p_temperature</th><th>exo_p_fuel_price</th><th>exo_p_cpi</th><th>exo_p_unemployment</th><th>exo_p_markdown_sum</th><th>exo_p_markdown1</th><th>exo_p_markdown2</th><th>exo_p_markdown3</th><th>exo_p_markdown4</th><th>exo_p_markdown5</th><th>exo_c_woy_bucket</th></tr><tr><td>str</td><td>i64</td><td>i32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>i32</td></tr></thead><tbody><tr><td>&quot;1&quot;</td><td>14641</td><td>201005</td><td>1.6437e6</td><td>0.0</td><td>42.310001</td><td>2.572</td><td>211.096359</td><td>8.106</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>1.0</td><td>1.0</td><td>1.0</td><td>1.0</td><td>1.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>999.0</td><td>42.310001</td><td>2.572</td><td>211.096359</td><td>8.106</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>1</td></tr><tr><td>&quot;1&quot;</td><td>14648</td><td>201006</td><td>1641957.5</td><td>1.0</td><td>38.509998</td><td>2.548</td><td>211.242172</td><td>8.106</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>1.0</td><td>1.0</td><td>1.0</td><td>1.0</td><td>1.0</td><td>1.6437e6</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>38.509998</td><td>2.548</td><td>211.242172</td><td>8.106</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>1</td></tr><tr><td>&quot;1&quot;</td><td>14655</td><td>201007</td><td>1.6120e6</td><td>0.0</td><td>39.93</td><td>2.514</td><td>211.289139</td><td>8.106</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>1.0</td><td>1.0</td><td>1.0</td><td>1.0</td><td>1.0</td><td>1641957.5</td><td>1.6437e6</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>1.0</td><td>39.93</td><td>2.514</td><td>211.289139</td><td>8.106</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>1</td></tr><tr><td>&quot;1&quot;</td><td>14662</td><td>201008</td><td>1.4097e6</td><td>0.0</td><td>46.630001</td><td>2.561</td><td>211.319641</td><td>8.106</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>1.0</td><td>1.0</td><td>1.0</td><td>1.0</td><td>1.0</td><td>1.6120e6</td><td>1641957.5</td><td>0.0</td><td>1.576836e6</td><td>0.0</td><td>112353.398438</td><td>2.0</td><td>46.630001</td><td>2.561</td><td>211.319641</td><td>8.106</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>2</td></tr><tr><td>&quot;1&quot;</td><td>14669</td><td>201009</td><td>1.5548e6</td><td>0.0</td><td>46.5</td><td>2.625</td><td>211.350143</td><td>8.106</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>1.0</td><td>1.0</td><td>1.0</td><td>1.0</td><td>1.0</td><td>1.4097e6</td><td>1.6120e6</td><td>0.0</td><td>1.554615e6</td><td>0.0</td><td>103135.0</td><td>3.0</td><td>46.5</td><td>2.625</td><td>211.350143</td><td>8.106</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>2</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>&quot;9&quot;</td><td>15607</td><td>201239</td><td>516361.0625</td><td>0.0</td><td>76.800003</td><td>3.666</td><td>226.763077</td><td>5.277</td><td>3711.670166</td><td>1699.680054</td><td>1.91</td><td>0.55</td><td>190.380005</td><td>1819.150024</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>533756.875</td><td>523427.34375</td><td>508567.03125</td><td>534839.375</td><td>536946.5625</td><td>21849.310547</td><td>3.0</td><td>76.800003</td><td>3.666</td><td>226.763077</td><td>5.277</td><td>3711.670166</td><td>1699.680054</td><td>1.91</td><td>0.55</td><td>190.380005</td><td>1819.150024</td><td>9</td></tr><tr><td>&quot;9&quot;</td><td>15614</td><td>201240</td><td>606755.3125</td><td>0.0</td><td>66.610001</td><td>3.617</td><td>226.966232</td><td>4.954</td><td>5328.919922</td><td>2657.570068</td><td>0.0</td><td>3.01</td><td>1107.790039</td><td>1560.550049</td><td>0.0</td><td>1.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>516361.0625</td><td>533756.875</td><td>553837.0</td><td>545075.125</td><td>542798.0625</td><td>41735.964844</td><td>4.0</td><td>66.610001</td><td>3.617</td><td>226.966232</td><td>4.954</td><td>5328.919922</td><td>2657.570068</td><td>0.0</td><td>3.01</td><td>1107.790039</td><td>1560.550049</td><td>10</td></tr><tr><td>&quot;9&quot;</td><td>15621</td><td>201241</td><td>558464.8125</td><td>0.0</td><td>60.09</td><td>3.601</td><td>227.169388</td><td>4.954</td><td>3366.26001</td><td>520.409973</td><td>0.0</td><td>6.01</td><td>0.0</td><td>2839.840088</td><td>0.0</td><td>1.0</td><td>0.0</td><td>1.0</td><td>0.0</td><td>606755.3125</td><td>516361.0625</td><td>529515.6875</td><td>553834.5</td><td>546504.1875</td><td>39282.828125</td><td>5.0</td><td>60.09</td><td>3.601</td><td>227.169388</td><td>4.954</td><td>3366.26001</td><td>520.409973</td><td>0.0</td><td>6.01</td><td>0.0</td><td>2839.840088</td><td>10</td></tr><tr><td>&quot;9&quot;</td><td>15628</td><td>201242</td><td>542009.4375</td><td>0.0</td><td>68.010002</td><td>3.594</td><td>227.214294</td><td>4.954</td><td>3681.530029</td><td>545.719971</td><td>0.0</td><td>8.0</td><td>28.940001</td><td>3098.870117</td><td>0.0</td><td>1.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>558464.8125</td><td>606755.3125</td><td>557075.1875</td><td>555897.625</td><td>550342.375</td><td>38074.996094</td><td>6.0</td><td>68.010002</td><td>3.594</td><td>227.214294</td><td>4.954</td><td>3681.530029</td><td>545.719971</td><td>0.0</td><td>8.0</td><td>28.940001</td><td>3098.870117</td><td>10</td></tr><tr><td>&quot;9&quot;</td><td>15635</td><td>201243</td><td>549731.5</td><td>0.0</td><td>69.519997</td><td>3.506</td><td>227.232803</td><td>4.954</td><td>2189.609863</td><td>512.22998</td><td>3.0</td><td>8.0</td><td>0.0</td><td>1666.380005</td><td>0.0</td><td>0.0</td><td>0.0</td><td>1.0</td><td>0.0</td><td>542009.4375</td><td>558464.8125</td><td>548527.5</td><td>564240.25</td><td>551662.6875</td><td>29129.589844</td><td>7.0</td><td>69.519997</td><td>3.506</td><td>227.232803</td><td>4.954</td><td>2189.609863</td><td>512.22998</td><td>3.0</td><td>8.0</td><td>0.0</td><td>1666.380005</td><td>10</td></tr></tbody></table></div>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-24T01:43:19.835208Z",
     "start_time": "2026-01-24T01:43:19.831699Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import random\n",
    "\n",
    "past_exo_cont_cols = (\n",
    "    # \"exo_p_y_lag_1w\",\n",
    "    \"exo_p_y_lag_2w\",\n",
    "    # \"exo_p_y_lag_52w\",\n",
    "    \"exo_p_y_rollmean_4w\",\"exo_p_y_rollmean_12w\",\"exo_p_y_rollstd_4w\",\n",
    "    # \"exo_p_weeks_since_holiday`\",\n",
    "    # \"exo_p_temperature\",\n",
    "    # \"exo_p_fuel_price\",\n",
    "    # \"exo_p_cpi\",\n",
    "    # \"exo_p_unemployment\",\n",
    "    # \"exo_p_markdown_sum\",\n",
    "    # \"exo_p_markdown1\",\n",
    "    # \"exo_p_markdown2\",\n",
    "    # \"exo_p_markdown3\",\n",
    "    # \"exo_p_markdown4\",\n",
    "    # \"exo_p_markdown5\",\n",
    "    # \"exo_markdown1_isnull\",\n",
    "    # \"exo_markdown2_isnull\",\n",
    "    # \"exo_markdown3_isnull\",\n",
    "    # \"exo_markdown4_isnull\",\n",
    "    # \"exo_markdown5_isnull\",\n",
    ")\n",
    "past_exo_cat_cols = (\n",
    "    # \"exo_c_woy_bucket\",\n",
    ")\n",
    "\n",
    "lookback = 52\n",
    "horizon = 27\n",
    "batch_size = 256\n",
    "\n",
    "freq = \"weekly\"          # walmart dt is weekly\n",
    "split_mode = \"multi\"     # id-disjoint split (leakage-safe)\n",
    "shuffle = True\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "print(\"device:\", device)\n"
   ],
   "id": "390bbc89ff08b8d2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-24T01:43:20.301206Z",
     "start_time": "2026-01-24T01:43:20.287204Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "from modeling_module.utils.exogenous_utils import compose_exo_calendar_cb\n",
    "\n",
    "future_exo_cb_time = compose_exo_calendar_cb(date_type=freq)\n",
    "\n",
    "# ============================================================\n",
    "# Holiday lookup (vectorized) + FutureExo callback (time + holiday)\n",
    "# ============================================================\n",
    "holiday_map_dayidx = {\n",
    "    int(row[0]): float(row[1])\n",
    "    for row in (\n",
    "        df.select([\"date_idx\", \"exo_is_holiday\"])\n",
    "          .group_by(\"date_idx\")\n",
    "          .agg(pl.max(\"exo_is_holiday\").alias(\"exo_is_holiday\"))\n",
    "          .sort(\"date_idx\")\n",
    "          .iter_rows()\n",
    "    )\n",
    "}\n",
    "\n",
    "def build_holiday_array(holiday_map_dayidx: dict[int, float], *, pad: int = 0) -> np.ndarray:\n",
    "    if not holiday_map_dayidx:\n",
    "        return np.zeros((1,), dtype=np.float32)\n",
    "    max_k = max(int(k) for k in holiday_map_dayidx.keys())\n",
    "    arr = np.zeros((max_k + 1 + int(pad),), dtype=np.float32)\n",
    "    for k, v in holiday_map_dayidx.items():\n",
    "        kk = int(k)\n",
    "        if kk >= 0:\n",
    "            arr[kk] = float(v)\n",
    "    return arr\n",
    "\n",
    "class FutureExoTimePlusHoliday:\n",
    "    def __init__(self, holiday_by_dayidx: np.ndarray, *, step_days: int = 7):\n",
    "        self.holiday = holiday_by_dayidx.astype(np.float32, copy=False)\n",
    "        self.step_days = int(step_days)\n",
    "\n",
    "    def __call__(self, start_idx, H: int, device: str = \"cpu\"):\n",
    "        # 1) calendar exo (batch-safe)\n",
    "        cal = future_exo_cb_time(start_idx, H, device=device)  # scalar: (H,E) | batch: (B,H,E)\n",
    "\n",
    "        # 2) holiday exo (vectorized in numpy)\n",
    "        is_scalar = isinstance(start_idx, (int, np.integer))\n",
    "        if is_scalar:\n",
    "            s = np.asarray([int(start_idx)], dtype=np.int64)\n",
    "        else:\n",
    "            s = np.asarray(start_idx, dtype=np.int64).reshape(-1)\n",
    "\n",
    "        B = s.shape[0]\n",
    "        H = int(H)\n",
    "\n",
    "        offsets = (self.step_days * np.arange(H, dtype=np.int64))[None, :]  # (1,H)\n",
    "        idx = s[:, None] + offsets                                          # (B,H)\n",
    "\n",
    "        hol = np.zeros((B, H), dtype=np.float32)\n",
    "        valid = (idx >= 0) & (idx < self.holiday.shape[0])\n",
    "        hol[valid] = self.holiday[idx[valid]]\n",
    "        hol_t = torch.from_numpy(hol).unsqueeze(-1)                         # (B,H,1), CPU\n",
    "\n",
    "        target_device = cal.device  # cal이 이미 cuda일 수 있음\n",
    "        cal = cal.to(target_device, dtype=torch.float32)\n",
    "        hol_t = hol_t.to(target_device, dtype=torch.float32)\n",
    "\n",
    "        # 3) concat\n",
    "        if is_scalar:\n",
    "            out = torch.cat([cal.to(torch.float32).unsqueeze(0), hol_t], dim=-1)[0]  # (H,E+1)\n",
    "        else:\n",
    "            out = torch.cat([cal.to(torch.float32), hol_t], dim=-1)                  # (B,H,E+1)\n",
    "\n",
    "        return out\n",
    "\n",
    "holiday_by_dayidx = build_holiday_array(holiday_map_dayidx, pad=7 * (horizon + 2))\n",
    "future_exo_cb_time_plus_holiday = FutureExoTimePlusHoliday(holiday_by_dayidx, step_days=7)\n"
   ],
   "id": "9be7243d1de2d24f",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-24T01:43:20.854195Z",
     "start_time": "2026-01-24T01:43:20.829696Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import importlib, inspect\n",
    "m = importlib.import_module(\"modeling_module.models.PatchTST.supervised.PatchTST\")\n",
    "\n",
    "print(\"[PatchTST module file]\", m.__file__)\n",
    "print(\"[DistHeadWithExo class]\", m.DistHeadWithExo)\n",
    "print(\"[DistHeadWithExo forward file]\", inspect.getsourcefile(m.DistHeadWithExo.forward))\n",
    "print(\"[DistHeadWithExo forward sig ]\", inspect.signature(m.DistHeadWithExo.forward))"
   ],
   "id": "495178ac4679fd12",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PatchTST module file] C:\\Users\\USER\\PycharmProjects\\ts_forecaster_lib\\src\\modeling_module\\models\\PatchTST\\supervised\\PatchTST.py\n",
      "[DistHeadWithExo class] <class 'modeling_module.models.PatchTST.supervised.PatchTST.DistHeadWithExo'>\n",
      "[DistHeadWithExo forward file] C:\\Users\\USER\\PycharmProjects\\ts_forecaster_lib\\src\\modeling_module\\models\\PatchTST\\supervised\\PatchTST.py\n",
      "[DistHeadWithExo forward sig ] (self, h: torch.Tensor, future_exo: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, torch.Tensor]\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-24T01:44:43.534584Z",
     "start_time": "2026-01-24T01:44:41.736347Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from modeling_module.training.model_losses.loss_module import DistributionLoss\n",
    "from modeling_module.utils.metrics import mae, rmse, smape\n",
    "from modeling_module.utils.eval_utils import eval_on_loader, eval_on_loader_quantile\n",
    "from modeling_module.utils.checkpoint import load_model_dict\n",
    "from modeling_module.models import build_patchTST_quantile, build_patchTST_base\n",
    "from modeling_module.training.model_trainers.total_train import run_total_train_weekly\n",
    "from modeling_module.data_loader import MultiPartExoDataModule\n",
    "import pandas as pd\n",
    "rows = []  # seed loop 밖에서 선언\n",
    "\n",
    "def set_seed(seed: int = 11):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.backends.mps.is_available():\n",
    "        torch.mps.manual_seed(seed)\n",
    "    elif torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def build_datamodule(variant: str, seed: int) -> MultiPartExoDataModule:\n",
    "    if variant == \"A0\":\n",
    "        future_exo_cb = None\n",
    "    elif variant == \"A1\":\n",
    "        future_exo_cb = future_exo_cb_time\n",
    "    elif variant == \"A2\":\n",
    "        future_exo_cb = future_exo_cb_time_plus_holiday\n",
    "    else:\n",
    "        raise ValueError(variant)\n",
    "\n",
    "    return MultiPartExoDataModule(\n",
    "        df=df,\n",
    "        id_col=\"unique_id\",\n",
    "        date_col=\"date\",\n",
    "        y_col=\"y\",\n",
    "        lookback=lookback,\n",
    "        horizon=horizon,\n",
    "        batch_size=batch_size,\n",
    "        past_exo_cont_cols=past_exo_cont_cols,\n",
    "        past_exo_cat_cols=past_exo_cat_cols,\n",
    "        future_exo_cb=future_exo_cb,\n",
    "        freq=freq,\n",
    "        shuffle=shuffle,\n",
    "        split_mode=split_mode,\n",
    "        seed=seed,\n",
    "    )\n",
    "\n",
    "def inspect(loader, name):\n",
    "    b = next(iter(loader))\n",
    "    x, y, uid, fe, pe_cont, pe_cat = b\n",
    "    print(f\"[{name}] x:\", x.shape, x.device, x.dtype)\n",
    "    print(f\"[{name}] fe:\", fe.shape, fe.device, fe.dtype)\n",
    "    print(f\"[{name}] pe:\", pe_cont.shape, pe_cont.device, pe_cont.dtype)\n",
    "    print(f\"[{name}] future_exo_cb is None?\", loader.collate_fn.future_exo_cb is None)\n",
    "    if fe.shape[-1] > 0:\n",
    "        print(f\"[{name}] fe sample:\", fe[0, :3, :])\n",
    "\n",
    "# seed_list = [11, 22, 33, 44, 55]\n",
    "seed_list = [55]\n",
    "for seed in seed_list:\n",
    "    set_seed(seed)\n",
    "\n",
    "    save_dir = os.path.join(DIR, \"fit\", \"walmart_patchtst_ab\")\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    save_root_A0 = os.path.join(save_dir, \"A0_y_only\", f\"seed_{seed}\")\n",
    "    save_root_A1 = os.path.join(save_dir, \"A1_time_exog\", f\"seed_{seed}\")\n",
    "    save_root_A2 = os.path.join(save_dir, \"A2_time_holiday\", f\"seed_{seed}\")\n",
    "\n",
    "    data_module_A0 = build_datamodule(\"A0\", seed)\n",
    "    data_module_A1 = build_datamodule(\"A1\", seed)\n",
    "    data_module_A2 = build_datamodule(\"A2\", seed)\n",
    "\n",
    "    train_loader_A0 = data_module_A0.get_train_loader(batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    val_loader_A0   = data_module_A0.get_val_loader()\n",
    "\n",
    "    train_loader_A1 = data_module_A1.get_train_loader(batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    val_loader_A1   = data_module_A1.get_val_loader()\n",
    "\n",
    "    train_loader_A2 = data_module_A2.get_train_loader(batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    val_loader_A2   = data_module_A2.get_val_loader()\n",
    "\n",
    "    inspect(train_loader_A0, \"A0\")\n",
    "    inspect(train_loader_A1, \"A1\")\n",
    "    inspect(train_loader_A2, \"A2\")\n",
    "\n",
    "    # ============================================\n",
    "    # 학습 실행 (LTB total_train 포맷 유지)\n",
    "    # - 여기서는 \"외생변수 A/B/C\"만 비교하므로 use_ssl_pretrain=False로 고정\n",
    "    # Walmart처럼 항상 판매량이 있는(continuous) 데이터에서 “스파이크”를 잡는 규칙이:\n",
    "    # 스파이크 마스크가 과도하게 넓게 잡히거나(사실상 대부분 True)\n",
    "    # spike-loss가 MSE/제곱오차 기반인데 reduction이 sum 또는 정규화 없이 누적되어\n",
    "    # sales 스케일(1e4~1e5)에서 제곱오차가 1e8급으로 바로 올라가\n",
    "    # → 결과적으로 delta가 1e8 수준으로 튄다.\n",
    "    # → 그래서 최종적으로 이 상황에서는 spike_epoch를 0으로 잡아준다.\n",
    "    # ============================================\n",
    "    # ,\n",
    "    print('run result_A0')\n",
    "    results_A0 = run_total_train_weekly(\n",
    "        train_loader_A0,\n",
    "        val_loader_A0,\n",
    "        device=device,\n",
    "        lookback=lookback,\n",
    "        horizon=horizon,\n",
    "        warmup_epochs=20,\n",
    "        spike_epochs=0,\n",
    "        save_dir=save_root_A0,\n",
    "        loss = DistributionLoss(distribution=\"Normal\", level=[80, 90]),\n",
    "        models_to_run=[\"patchtst\"],\n",
    "        use_ssl_mode = 'full',\n",
    "        point_loss_mode=\"dist\",\n",
    "    )\n",
    "\n",
    "    print('run result_A1')\n",
    "    results_A1 = run_total_train_weekly(\n",
    "        train_loader_A1,\n",
    "        val_loader_A1,\n",
    "        device=device,\n",
    "        lookback=lookback,\n",
    "        horizon=horizon,\n",
    "        warmup_epochs=20,\n",
    "        spike_epochs=0,\n",
    "        save_dir=save_root_A1,\n",
    "        use_exogenous_mode = True,\n",
    "        models_to_run=[\"patchtst\"],\n",
    "        loss = DistributionLoss(distribution=\"Normal\", level=[80, 90]),\n",
    "        use_ssl_mode = 'full',\n",
    "        point_loss_mode=\"dist\",\n",
    "    )\n",
    "\n",
    "    print('run result_A2')\n",
    "    results_A2 = run_total_train_weekly(\n",
    "        train_loader_A2,\n",
    "        val_loader_A2,\n",
    "        device=device,\n",
    "        lookback=lookback,\n",
    "        horizon=horizon,\n",
    "        warmup_epochs=20,\n",
    "        spike_epochs=0,\n",
    "        save_dir=save_root_A2,\n",
    "        use_exogenous_mode = True,\n",
    "        models_to_run=[\"patchtst\"],\n",
    "        loss = DistributionLoss(distribution=\"Normal\", level=[80, 90]),\n",
    "        use_ssl_mode = 'full',\n",
    "        point_loss_mode=\"dist\",\n",
    "    )\n",
    "\n",
    "    builders = {\n",
    "    \"patchtst_quantile\": build_patchTST_quantile,\n",
    "    \"patchtst\": build_patchTST_base,\n",
    "    }\n",
    "\n",
    "    print(load_model_dict(save_root_A0, builders, device = device))\n",
    "\n",
    "    model_A0 = load_model_dict(save_root_A0, builders, device = device)['patchtst']\n",
    "    model_A1 = load_model_dict(save_root_A1, builders, device = device)['patchtst']\n",
    "    model_A2 = load_model_dict(save_root_A2, builders, device = device)['patchtst']\n",
    "\n",
    "    y0, yhat0 = eval_on_loader(model_A0, val_loader_A0, device=device)\n",
    "    y1, yhat1 = eval_on_loader(model_A1, val_loader_A1, device=device, future_exo_cb = future_exo_cb_time)\n",
    "    y2, yhat2 = eval_on_loader(model_A2, val_loader_A2, device=device, future_exo_cb = future_exo_cb_time_plus_holiday)\n",
    "\n",
    "    metric_A0 = {\n",
    "    \"MAE\": float(mae(y0.reshape(-1), yhat0.reshape(-1))),\n",
    "    \"RMSE\": float(rmse(y0.reshape(-1), yhat0.reshape(-1))),\n",
    "    \"SMAPE\": float(smape(y0.reshape(-1), yhat0.reshape(-1))),\n",
    "    }\n",
    "    metric_A1 = {\n",
    "        \"MAE\": float(mae(y1.reshape(-1), yhat1.reshape(-1))),\n",
    "        \"RMSE\": float(rmse(y1.reshape(-1), yhat1.reshape(-1))),\n",
    "        \"SMAPE\": float(smape(y1.reshape(-1), yhat1.reshape(-1))),\n",
    "    }\n",
    "    metric_A2 = {\n",
    "        \"MAE\": float(mae(y2.reshape(-1), yhat2.reshape(-1))),\n",
    "        \"RMSE\": float(rmse(y2.reshape(-1), yhat2.reshape(-1))),\n",
    "        \"SMAPE\": float(smape(y2.reshape(-1), yhat2.reshape(-1))),\n",
    "    }\n",
    "\n",
    "    model_A0_q = load_model_dict(save_root_A0, builders, device = device)['patchtst_quantile']\n",
    "    model_A1_q = load_model_dict(save_root_A1, builders, device = device)['patchtst_quantile']\n",
    "    model_A2_q = load_model_dict(save_root_A2, builders, device = device)['patchtst_quantile']\n",
    "\n",
    "    y0_q, yhat0_q = eval_on_loader_quantile(model_A0_q, val_loader_A0, device=device)\n",
    "    y1_q, yhat1_q = eval_on_loader_quantile(model_A1_q, val_loader_A1, device=device, future_exo_cb = future_exo_cb_time)\n",
    "    y2_q, yhat2_q = eval_on_loader_quantile(model_A2_q, val_loader_A2, device=device, future_exo_cb = future_exo_cb_time_plus_holiday)\n",
    "\n",
    "    metric_A0_q = {\n",
    "    \"MAE\": float(mae(y0_q.reshape(-1), yhat0_q.reshape(-1))),\n",
    "    \"RMSE\": float(rmse(y0_q.reshape(-1), yhat0_q.reshape(-1))),\n",
    "    \"SMAPE\": float(smape(y0_q.reshape(-1), yhat0_q.reshape(-1))),\n",
    "    }\n",
    "    metric_A1_q = {\n",
    "        \"MAE\": float(mae(y1_q.reshape(-1), yhat1_q.reshape(-1))),\n",
    "        \"RMSE\": float(rmse(y1_q.reshape(-1), yhat1_q.reshape(-1))),\n",
    "        \"SMAPE\": float(smape(y1_q.reshape(-1), yhat1_q.reshape(-1))),\n",
    "    }\n",
    "    metric_A2_q = {\n",
    "        \"MAE\": float(mae(y2_q.reshape(-1), yhat2_q.reshape(-1))),\n",
    "        \"RMSE\": float(rmse(y2_q.reshape(-1), yhat2_q.reshape(-1))),\n",
    "        \"SMAPE\": float(smape(y2_q.reshape(-1), yhat2_q.reshape(-1))),\n",
    "    }\n",
    "\n",
    "    # -------------------------\n",
    "    # Point metrics row append\n",
    "    # -------------------------\n",
    "    rows.append({\n",
    "        \"seed\": seed,\n",
    "        \"variant\": \"A0\",\n",
    "        \"model_type\": \"point\",\n",
    "        \"MAE\": metric_A0[\"MAE\"],\n",
    "        \"RMSE\": metric_A0[\"RMSE\"],\n",
    "        \"SMAPE\": metric_A0[\"SMAPE\"],\n",
    "        \"save_root\": save_root_A0,\n",
    "    })\n",
    "    rows.append({\n",
    "        \"seed\": seed,\n",
    "        \"variant\": \"A1\",\n",
    "        \"model_type\": \"point\",\n",
    "        \"MAE\": metric_A1[\"MAE\"],\n",
    "        \"RMSE\": metric_A1[\"RMSE\"],\n",
    "        \"SMAPE\": metric_A1[\"SMAPE\"],\n",
    "        \"save_root\": save_root_A1,\n",
    "    })\n",
    "    rows.append({\n",
    "        \"seed\": seed,\n",
    "        \"variant\": \"A2\",\n",
    "        \"model_type\": \"point\",\n",
    "        \"MAE\": metric_A2[\"MAE\"],\n",
    "        \"RMSE\": metric_A2[\"RMSE\"],\n",
    "        \"SMAPE\": metric_A2[\"SMAPE\"],\n",
    "        \"save_root\": save_root_A2,\n",
    "    })\n",
    "\n",
    "    # -------------------------\n",
    "    # Quantile metrics row append\n",
    "    # (주의: q50 등 기준이 명확해야 함)\n",
    "    # -------------------------\n",
    "    rows.append({\n",
    "        \"seed\": seed,\n",
    "        \"variant\": \"A0\",\n",
    "        \"model_type\": \"quantile(q50)\",\n",
    "        \"MAE\": metric_A0_q[\"MAE\"],\n",
    "        \"RMSE\": metric_A0_q[\"RMSE\"],\n",
    "        \"SMAPE\": metric_A0_q[\"SMAPE\"],\n",
    "        \"save_root\": save_root_A0,\n",
    "    })\n",
    "    rows.append({\n",
    "        \"seed\": seed,\n",
    "        \"variant\": \"A1\",\n",
    "        \"model_type\": \"quantile(q50)\",\n",
    "        \"MAE\": metric_A1_q[\"MAE\"],\n",
    "        \"RMSE\": metric_A1_q[\"RMSE\"],\n",
    "        \"SMAPE\": metric_A1_q[\"SMAPE\"],\n",
    "        \"save_root\": save_root_A1,\n",
    "    })\n",
    "    rows.append({\n",
    "        \"seed\": seed,\n",
    "        \"variant\": \"A2\",\n",
    "        \"model_type\": \"quantile(q50)\",\n",
    "        \"MAE\": metric_A2_q[\"MAE\"],\n",
    "        \"RMSE\": metric_A2_q[\"RMSE\"],\n",
    "        \"SMAPE\": metric_A2_q[\"SMAPE\"],\n",
    "        \"save_root\": save_root_A2,\n",
    "    })\n",
    "\n",
    "# loop 종료 후 저장\n",
    "df_out = pd.DataFrame(rows)\n",
    "\n",
    "out_csv = os.path.join(save_dir, \"ab_results_by_seed.csv\")\n",
    "df_out.to_csv(out_csv, index=False)\n",
    "\n",
    "# variant별 mean/std 요약도 같이 저장 추천\n",
    "summary = (\n",
    "    df_out.groupby([\"variant\", \"model_type\"])[[\"MAE\", \"RMSE\", \"SMAPE\"]]\n",
    "    .agg([\"mean\", \"std\"])\n",
    "    .reset_index()\n",
    ")\n",
    "out_sum = os.path.join(save_dir, \"ab_results_summary.csv\")\n",
    "summary.to_csv(out_sum, index=False)\n",
    "\n",
    "print(\"saved:\", out_csv, out_sum)\n"
   ],
   "id": "48588eb2368b0b6b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[A0] x: torch.Size([256, 52, 1]) cpu torch.float32\n",
      "[A0] fe: torch.Size([256, 27, 0]) cpu torch.float32\n",
      "[A0] pe: torch.Size([256, 52, 4]) cpu torch.float32\n",
      "[A0] future_exo_cb is None? True\n",
      "[A1] x: torch.Size([256, 52, 1]) cpu torch.float32\n",
      "[A1] fe: torch.Size([256, 27, 2]) cpu torch.float32\n",
      "[A1] pe: torch.Size([256, 52, 4]) cpu torch.float32\n",
      "[A1] future_exo_cb is None? False\n",
      "[A1] fe sample: tensor([[-0.7468,  0.6651],\n",
      "        [-0.6624,  0.7491],\n",
      "        [-0.5671,  0.8237]])\n",
      "[A2] x: torch.Size([256, 52, 1]) cpu torch.float32\n",
      "[A2] fe: torch.Size([256, 27, 3]) cpu torch.float32\n",
      "[A2] pe: torch.Size([256, 52, 4]) cpu torch.float32\n",
      "[A2] future_exo_cb is None? False\n",
      "[A2] fe sample: tensor([[0.8859, 0.4639, 0.0000],\n",
      "        [0.9355, 0.3534, 0.0000],\n",
      "        [0.9708, 0.2398, 0.0000]])\n",
      "run result_A0\n",
      "[total_train] use_exogenous_mode: False has_fe: True, fe_dim: 0\n",
      "\n",
      "[total_train] === RUN: patchtst (weekly) ===\n",
      "point_train_cfg:  TrainingConfig(device='cuda', log_every=100, use_amp=True, lookback=52, horizon=27, epochs=1, lr=0.0003, weight_decay=0.001, t_max=40, patience=100, max_grad_norm=30.0, amp_device='cuda', loss_mode='dist', point_loss='huber', huber_delta=0.8, q_star=0.5, use_cost_q_star=False, Cu=1.0, Co=1.0, quantiles=(0.1, 0.5, 0.9), dist_name='normal', dist_scale_transform='softplus', dist_min_scale=1000, dist_use_weights=True, dist_family='normal', dist_eps=1e-08, dist_scale_is_positive=True, custom_loss=DistributionLoss(), dist_pred_format='auto', use_intermittent=True, alpha_zero=3.0, alpha_pos=1.0, gamma_run=0.3, cap=None, use_horizon_decay=False, tau_h=0.85, val_use_weights=False, use_exogenous_mode=False, spike_loss=SpikeLossConfig(enabled=True, strategy='mix', huber_delta=0.6, asym_up_weight=1.0, asym_down_weight=2.0, mad_k=1.5, w_spike=4.0, w_norm=1.0, alpha_huber=0.6, beta_asym=0.4, mix_with_baseline=False, gamma_baseline=0.0), lambda_hist_scale=0.1, lambda_hist_var=0.03, hist_window=12, anchor_last_k=8, anchor_weight=0.05)\n",
      "[SSL] PatchTST Pretrain (Weekly) -> C:\\Users\\USER\\PycharmProjects\\ts_forecaster_lib\\raw_data\\fit\\walmart_patchtst_ab\\A0_y_only\\seed_55\\pretrain\\patchtst_pretrain_best.pt\n",
      "[Pretrain] stage=0 epochs=10 lr=0.0003 wd=0.001 mask_ratio=0.3 loss=mse\n",
      "[Pretrain][stage=0 ep=1/10] train=1.110539 val=1.038985\n",
      "[Pretrain][stage=0 ep=2/10] train=0.949097 val=0.925883\n",
      "[Pretrain][stage=0 ep=3/10] train=0.918604 val=0.899317\n",
      "[Pretrain][stage=0 ep=4/10] train=0.861436 val=0.824380\n",
      "[Pretrain][stage=0 ep=5/10] train=0.815977 val=0.826792\n",
      "[Pretrain][stage=0 ep=6/10] train=0.797554 val=0.758452\n",
      "[Pretrain][stage=0 ep=7/10] train=0.747556 val=0.711683\n",
      "[Pretrain][stage=0 ep=8/10] train=0.725091 val=0.701716\n",
      "[Pretrain][stage=0 ep=9/10] train=0.721416 val=0.706956\n",
      "[Pretrain][stage=0 ep=10/10] train=0.695943 val=0.669263\n",
      "[Pretrain] done | best_val=0.669263\n",
      "[DBG-backbone-init] d_past_cont=0 cont_input_dim=0 target_input_dim=12 total_input_dim=12\n",
      "PatchTST Base (Weekly)\n",
      "[Finetune] loaded pretrain ckpt: C:\\Users\\USER\\PycharmProjects\\ts_forecaster_lib\\raw_data\\fit\\walmart_patchtst_ab\\A0_y_only\\seed_55\\pretrain\\patchtst_pretrain_best.pt\n",
      "[Finetune] ckpt meta keys: ['best_val']\n",
      "[Finetune] matched: 73\n",
      "[Finetune] missing: 4 / total: 77 ratio=0.052\n",
      "[Finetune] missing sample (first 30): ['head.loc_head.bias', 'head.loc_head.weight', 'head.scale_head.bias', 'head.scale_head.weight']\n",
      "[Finetune] load_strict=False\n",
      "[Finetune] load_state_dict -> missing_keys=4 unexpected_keys=0\n",
      "  - missing (first 30): ['head.loc_head.weight', 'head.loc_head.bias', 'head.scale_head.weight', 'head.scale_head.bias']\n",
      "[train_patchtst] loss_mode: dist\n",
      "[train_patchtst] exogenous_mode: True\n",
      "[train_patchtst] dist head rebuilt: d_future 0 -> 0\n",
      "\n",
      "[train_patchtst] ===== Stage 1/2 =====\n",
      "  - spike: OFF\n",
      "  - epochs: 20 | lr=0.0003 | horizon_decay=False\n",
      "[train_patchtst] Effective TrainingConfig:\n",
      "{\n",
      "  \"device\": \"cuda\",\n",
      "  \"log_every\": 100,\n",
      "  \"use_amp\": true,\n",
      "  \"lookback\": 52,\n",
      "  \"horizon\": 27,\n",
      "  \"epochs\": 20,\n",
      "  \"lr\": 0.0003,\n",
      "  \"weight_decay\": 0.001,\n",
      "  \"t_max\": 40,\n",
      "  \"patience\": 100,\n",
      "  \"max_grad_norm\": 30.0,\n",
      "  \"amp_device\": \"cuda\",\n",
      "  \"loss_mode\": \"dist\",\n",
      "  \"point_loss\": \"huber\",\n",
      "  \"huber_delta\": 0.8,\n",
      "  \"q_star\": 0.5,\n",
      "  \"use_cost_q_star\": false,\n",
      "  \"Cu\": 1.0,\n",
      "  \"Co\": 1.0,\n",
      "  \"quantiles\": [\n",
      "    0.1,\n",
      "    0.5,\n",
      "    0.9\n",
      "  ],\n",
      "  \"dist_name\": \"normal\",\n",
      "  \"dist_scale_transform\": \"softplus\",\n",
      "  \"dist_min_scale\": 1000,\n",
      "  \"dist_use_weights\": true,\n",
      "  \"dist_family\": \"normal\",\n",
      "  \"dist_eps\": 1e-08,\n",
      "  \"dist_scale_is_positive\": true,\n",
      "  \"custom_loss\": \"DistributionLoss()\",\n",
      "  \"dist_pred_format\": \"auto\",\n",
      "  \"use_intermittent\": true,\n",
      "  \"alpha_zero\": 3.0,\n",
      "  \"alpha_pos\": 1.0,\n",
      "  \"gamma_run\": 0.3,\n",
      "  \"cap\": null,\n",
      "  \"use_horizon_decay\": false,\n",
      "  \"tau_h\": 0.85,\n",
      "  \"val_use_weights\": false,\n",
      "  \"use_exogenous_mode\": false,\n",
      "  \"spike_loss\": {\n",
      "    \"enabled\": false,\n",
      "    \"strategy\": \"mix\",\n",
      "    \"huber_delta\": 0.6,\n",
      "    \"asym_up_weight\": 1.0,\n",
      "    \"asym_down_weight\": 2.0,\n",
      "    \"mad_k\": 1.5,\n",
      "    \"w_spike\": 4.0,\n",
      "    \"w_norm\": 1.0,\n",
      "    \"alpha_huber\": 0.6,\n",
      "    \"beta_asym\": 0.4,\n",
      "    \"mix_with_baseline\": false,\n",
      "    \"gamma_baseline\": 0.0\n",
      "  },\n",
      "  \"lambda_hist_scale\": 0.1,\n",
      "  \"lambda_hist_var\": 0.03,\n",
      "  \"hist_window\": 12,\n",
      "  \"anchor_last_k\": 8,\n",
      "  \"anchor_weight\": 0.05\n",
      "}\n",
      "[CommonTrainer] TrainingConfig (final)\n",
      "{\n",
      "  \"device\": \"cuda\",\n",
      "  \"log_every\": 100,\n",
      "  \"use_amp\": true,\n",
      "  \"lookback\": 52,\n",
      "  \"horizon\": 27,\n",
      "  \"epochs\": 20,\n",
      "  \"lr\": 0.0003,\n",
      "  \"weight_decay\": 0.001,\n",
      "  \"t_max\": 40,\n",
      "  \"patience\": 100,\n",
      "  \"max_grad_norm\": 30.0,\n",
      "  \"amp_device\": \"cuda\",\n",
      "  \"loss_mode\": \"dist\",\n",
      "  \"point_loss\": \"huber\",\n",
      "  \"huber_delta\": 0.8,\n",
      "  \"q_star\": 0.5,\n",
      "  \"use_cost_q_star\": false,\n",
      "  \"Cu\": 1.0,\n",
      "  \"Co\": 1.0,\n",
      "  \"quantiles\": [\n",
      "    0.1,\n",
      "    0.5,\n",
      "    0.9\n",
      "  ],\n",
      "  \"dist_name\": \"normal\",\n",
      "  \"dist_scale_transform\": \"softplus\",\n",
      "  \"dist_min_scale\": 1000,\n",
      "  \"dist_use_weights\": true,\n",
      "  \"dist_family\": \"normal\",\n",
      "  \"dist_eps\": 1e-08,\n",
      "  \"dist_scale_is_positive\": true,\n",
      "  \"custom_loss\": \"DistributionLoss()\",\n",
      "  \"dist_pred_format\": \"auto\",\n",
      "  \"use_intermittent\": true,\n",
      "  \"alpha_zero\": 3.0,\n",
      "  \"alpha_pos\": 1.0,\n",
      "  \"gamma_run\": 0.3,\n",
      "  \"cap\": null,\n",
      "  \"use_horizon_decay\": false,\n",
      "  \"tau_h\": 0.85,\n",
      "  \"val_use_weights\": false,\n",
      "  \"use_exogenous_mode\": false,\n",
      "  \"spike_loss\": {\n",
      "    \"enabled\": false,\n",
      "    \"strategy\": \"mix\",\n",
      "    \"huber_delta\": 0.6,\n",
      "    \"asym_up_weight\": 1.0,\n",
      "    \"asym_down_weight\": 2.0,\n",
      "    \"mad_k\": 1.5,\n",
      "    \"w_spike\": 4.0,\n",
      "    \"w_norm\": 1.0,\n",
      "    \"alpha_huber\": 0.6,\n",
      "    \"beta_asym\": 0.4,\n",
      "    \"mix_with_baseline\": false,\n",
      "    \"gamma_baseline\": 0.0\n",
      "  },\n",
      "  \"lambda_hist_scale\": 0.1,\n",
      "  \"lambda_hist_var\": 0.03,\n",
      "  \"hist_window\": 12,\n",
      "  \"anchor_last_k\": 8,\n",
      "  \"anchor_weight\": 0.05\n",
      "}\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Normal.__init__() takes from 3 to 4 positional arguments but 257 were given",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mTypeError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[7]\u001B[39m\u001B[32m, line 98\u001B[39m\n\u001B[32m     86\u001B[39m \u001B[38;5;66;03m# ============================================\u001B[39;00m\n\u001B[32m     87\u001B[39m \u001B[38;5;66;03m# 학습 실행 (LTB total_train 포맷 유지)\u001B[39;00m\n\u001B[32m     88\u001B[39m \u001B[38;5;66;03m# - 여기서는 \"외생변수 A/B/C\"만 비교하므로 use_ssl_pretrain=False로 고정\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m     95\u001B[39m \u001B[38;5;66;03m# ============================================\u001B[39;00m\n\u001B[32m     96\u001B[39m \u001B[38;5;66;03m# ,\u001B[39;00m\n\u001B[32m     97\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m'\u001B[39m\u001B[33mrun result_A0\u001B[39m\u001B[33m'\u001B[39m)\n\u001B[32m---> \u001B[39m\u001B[32m98\u001B[39m results_A0 = \u001B[43mrun_total_train_weekly\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m     99\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtrain_loader_A0\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    100\u001B[39m \u001B[43m    \u001B[49m\u001B[43mval_loader_A0\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    101\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    102\u001B[39m \u001B[43m    \u001B[49m\u001B[43mlookback\u001B[49m\u001B[43m=\u001B[49m\u001B[43mlookback\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    103\u001B[39m \u001B[43m    \u001B[49m\u001B[43mhorizon\u001B[49m\u001B[43m=\u001B[49m\u001B[43mhorizon\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    104\u001B[39m \u001B[43m    \u001B[49m\u001B[43mwarmup_epochs\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m20\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m    105\u001B[39m \u001B[43m    \u001B[49m\u001B[43mspike_epochs\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m0\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m    106\u001B[39m \u001B[43m    \u001B[49m\u001B[43msave_dir\u001B[49m\u001B[43m=\u001B[49m\u001B[43msave_root_A0\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    107\u001B[39m \u001B[43m    \u001B[49m\u001B[43mloss\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[43mDistributionLoss\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdistribution\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mNormal\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlevel\u001B[49m\u001B[43m=\u001B[49m\u001B[43m[\u001B[49m\u001B[32;43m80\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m90\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    108\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmodels_to_run\u001B[49m\u001B[43m=\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mpatchtst\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    109\u001B[39m \u001B[43m    \u001B[49m\u001B[43muse_ssl_mode\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mfull\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m    110\u001B[39m \u001B[43m    \u001B[49m\u001B[43mpoint_loss_mode\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mdist\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m    111\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    113\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m'\u001B[39m\u001B[33mrun result_A1\u001B[39m\u001B[33m'\u001B[39m)\n\u001B[32m    114\u001B[39m results_A1 = run_total_train_weekly(\n\u001B[32m    115\u001B[39m     train_loader_A1,\n\u001B[32m    116\u001B[39m     val_loader_A1,\n\u001B[32m   (...)\u001B[39m\u001B[32m    127\u001B[39m     point_loss_mode=\u001B[33m\"\u001B[39m\u001B[33mdist\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    128\u001B[39m )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\ts_forecaster_lib\\src\\modeling_module\\training\\model_trainers\\total_train.py:923\u001B[39m, in \u001B[36mrun_total_train_weekly\u001B[39m\u001B[34m(train_loader, val_loader, device, lookback, horizon, warmup_epochs, spike_epochs, base_lr, point_loss_mode, loss, save_dir, use_exogenous_mode, models_to_run, use_ssl_mode, ssl_pretrain_epochs, ssl_mask_ratio, ssl_loss_type, ssl_freeze_encoder_before_ft, ssl_pretrained_ckpt_path)\u001B[39m\n\u001B[32m    899\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mrun_total_train_weekly\u001B[39m(\n\u001B[32m    900\u001B[39m         train_loader,\n\u001B[32m    901\u001B[39m         val_loader,\n\u001B[32m   (...)\u001B[39m\u001B[32m    920\u001B[39m \n\u001B[32m    921\u001B[39m ):\n\u001B[32m    922\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"주간(Weekly) 데이터 통합 학습 실행기.\"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m923\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_run_total_train_generic\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    924\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    925\u001B[39m \u001B[43m        \u001B[49m\u001B[43mval_loader\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    926\u001B[39m \u001B[43m        \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    927\u001B[39m \u001B[43m        \u001B[49m\u001B[43mlookback\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    928\u001B[39m \u001B[43m        \u001B[49m\u001B[43mhorizon\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    929\u001B[39m \u001B[43m        \u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mweekly\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m    930\u001B[39m \u001B[43m        \u001B[49m\u001B[43msave_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    931\u001B[39m \u001B[43m        \u001B[49m\u001B[43muse_exogenous_mode\u001B[49m\u001B[43m=\u001B[49m\u001B[43muse_exogenous_mode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    932\u001B[39m \u001B[43m        \u001B[49m\u001B[43mwarmup_epochs\u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[43mwarmup_epochs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    933\u001B[39m \u001B[43m        \u001B[49m\u001B[43mspike_epochs\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[43mspike_epochs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    934\u001B[39m \u001B[43m        \u001B[49m\u001B[43mbase_lr\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[43mbase_lr\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    935\u001B[39m \u001B[43m        \u001B[49m\u001B[43mpoint_loss_mode\u001B[49m\u001B[43m=\u001B[49m\u001B[43mpoint_loss_mode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    936\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmodels_to_run\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmodels_to_run\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    937\u001B[39m \u001B[43m        \u001B[49m\u001B[43muse_ssl_mode\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[43muse_ssl_mode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    938\u001B[39m \u001B[43m        \u001B[49m\u001B[43mloss\u001B[49m\u001B[43m=\u001B[49m\u001B[43mloss\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    939\u001B[39m \u001B[43m        \u001B[49m\u001B[43mssl_pretrain_epochs\u001B[49m\u001B[43m=\u001B[49m\u001B[43mssl_pretrain_epochs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    940\u001B[39m \u001B[43m        \u001B[49m\u001B[43mssl_mask_ratio\u001B[49m\u001B[43m=\u001B[49m\u001B[43mssl_mask_ratio\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    941\u001B[39m \u001B[43m        \u001B[49m\u001B[43mssl_loss_type\u001B[49m\u001B[43m=\u001B[49m\u001B[43mssl_loss_type\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    942\u001B[39m \u001B[43m        \u001B[49m\u001B[43mssl_freeze_encoder_before_ft\u001B[49m\u001B[43m=\u001B[49m\u001B[43mssl_freeze_encoder_before_ft\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    943\u001B[39m \u001B[43m        \u001B[49m\u001B[43mssl_pretrained_ckpt_path\u001B[49m\u001B[43m=\u001B[49m\u001B[43mssl_pretrained_ckpt_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    944\u001B[39m \n\u001B[32m    945\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\ts_forecaster_lib\\src\\modeling_module\\training\\model_trainers\\total_train.py:892\u001B[39m, in \u001B[36m_run_total_train_generic\u001B[39m\u001B[34m(train_loader, val_loader, device, lookback, horizon, freq, save_dir, use_exogenous_mode, models_to_run, warmup_epochs, spike_epochs, base_lr, point_loss_mode, loss, use_ssl_mode, ssl_pretrain_epochs, ssl_mask_ratio, ssl_loss_type, ssl_freeze_encoder_before_ft, ssl_pretrained_ckpt_path)\u001B[39m\n\u001B[32m    881\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m m == \u001B[33m\"\u001B[39m\u001B[33mpatchtst\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m    882\u001B[39m         kwargs.update(\u001B[38;5;28mdict\u001B[39m(\n\u001B[32m    883\u001B[39m             point_loss_mode=point_loss_mode,\n\u001B[32m    884\u001B[39m             use_ssl_mode = use_ssl_mode,\n\u001B[32m   (...)\u001B[39m\u001B[32m    889\u001B[39m             ssl_pretrained_ckpt_path=ssl_pretrained_ckpt_path,\n\u001B[32m    890\u001B[39m         ))\n\u001B[32m--> \u001B[39m\u001B[32m892\u001B[39m     \u001B[43mMODEL_REGISTRY\u001B[49m\u001B[43m[\u001B[49m\u001B[43mm\u001B[49m\u001B[43m]\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    894\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m results\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\ts_forecaster_lib\\src\\modeling_module\\training\\model_trainers\\total_train.py:446\u001B[39m, in \u001B[36m_run_patchtst\u001B[39m\u001B[34m(results, freq, train_loader, val_loader, save_root, lookback, horizon, future_exo_cb, exo_dim, patch_len, stride, point_train_cfg, quantile_train_cfg, stages, device, use_exogenous_mode, use_ssl_mode, ssl_pretrain_epochs, ssl_mask_ratio, ssl_loss_type, ssl_freeze_encoder_before_ft, ssl_pretrained_ckpt_path, point_loss_mode)\u001B[39m\n\u001B[32m    443\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m'\u001B[39m\u001B[33mPatchTST Base (\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfreq.capitalize()\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m)\u001B[39m\u001B[33m'\u001B[39m)\n\u001B[32m    444\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m (use_ssl_mode == \u001B[33m'\u001B[39m\u001B[33mfull\u001B[39m\u001B[33m'\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m (pretrain_ckpt_path \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[32m    445\u001B[39m     \u001B[38;5;66;03m# 사전학습 가중치 로드 및 파인튜닝\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m446\u001B[39m     best_pt_base = \u001B[43mtrain_patchtst_finetune\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    447\u001B[39m \u001B[43m        \u001B[49m\u001B[43mpt_base\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_loader\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    448\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtrain_cfg\u001B[49m\u001B[43m=\u001B[49m\u001B[43mpoint_train_cfg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstages\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mlist\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mstages\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    449\u001B[39m \u001B[43m        \u001B[49m\u001B[43mfuture_exo_cb\u001B[49m\u001B[43m=\u001B[49m\u001B[43mfuture_exo_cb\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    450\u001B[39m \u001B[43m        \u001B[49m\u001B[43mexo_is_normalized\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    451\u001B[39m \u001B[43m        \u001B[49m\u001B[43mpretrain_ckpt_path\u001B[49m\u001B[43m=\u001B[49m\u001B[43mpretrain_ckpt_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    452\u001B[39m \u001B[43m        \u001B[49m\u001B[43mload_strict\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    453\u001B[39m \u001B[43m        \u001B[49m\u001B[43mfreeze_encoder_before_ft\u001B[49m\u001B[43m=\u001B[49m\u001B[43mssl_freeze_encoder_before_ft\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    454\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    455\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    456\u001B[39m     \u001B[38;5;66;03m# 처음부터 학습 (Scratch)\u001B[39;00m\n\u001B[32m    457\u001B[39m     best_pt_base = train_patchtst(\n\u001B[32m    458\u001B[39m         pt_base, train_loader, val_loader,\n\u001B[32m    459\u001B[39m         train_cfg=point_train_cfg, stages=\u001B[38;5;28mlist\u001B[39m(stages),\n\u001B[32m    460\u001B[39m         future_exo_cb=future_exo_cb,\n\u001B[32m    461\u001B[39m         use_exogenous_mode=use_exogenous_mode,\n\u001B[32m    462\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\ts_forecaster_lib\\src\\modeling_module\\training\\model_trainers\\patchtst_finetune.py:435\u001B[39m, in \u001B[36mtrain_patchtst_finetune\u001B[39m\u001B[34m(model, train_loader, val_loader, train_cfg, stages, pretrain_ckpt_path, load_strict, freeze_encoder_before_ft, unfreeze_after_stage0, future_exo_cb, exo_is_normalized)\u001B[39m\n\u001B[32m    432\u001B[39m     \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33m[Finetune] encoder blocks frozen (stage0).\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m    434\u001B[39m \u001B[38;5;66;03m# 3) 지도학습 파이프라인 실행\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m435\u001B[39m out = \u001B[43mtrain_patchtst\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    436\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    437\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    438\u001B[39m \u001B[43m    \u001B[49m\u001B[43mval_loader\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    439\u001B[39m \u001B[43m    \u001B[49m\u001B[43mstages\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstages\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    440\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtrain_cfg\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtrain_cfg\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    441\u001B[39m \u001B[43m    \u001B[49m\u001B[43mfuture_exo_cb\u001B[49m\u001B[43m=\u001B[49m\u001B[43mfuture_exo_cb\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    442\u001B[39m \u001B[43m    \u001B[49m\u001B[43mexo_is_normalized\u001B[49m\u001B[43m=\u001B[49m\u001B[43mexo_is_normalized\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    443\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    445\u001B[39m \u001B[38;5;66;03m# 4) 학습 후 동결 해제 (옵션)\u001B[39;00m\n\u001B[32m    446\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m freeze_encoder_before_ft \u001B[38;5;129;01mand\u001B[39;00m unfreeze_after_stage0:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\ts_forecaster_lib\\src\\modeling_module\\training\\model_trainers\\patchtst_train.py:238\u001B[39m, in \u001B[36mtrain_patchtst\u001B[39m\u001B[34m(model, train_loader, val_loader, stages, train_cfg, future_exo_cb, exo_is_normalized, use_exogenous_mode)\u001B[39m\n\u001B[32m    228\u001B[39m     \u001B[38;5;66;03m# 트레이너 초기화 및 학습 수행\u001B[39;00m\n\u001B[32m    229\u001B[39m     trainer = CommonTrainer(\n\u001B[32m    230\u001B[39m         cfg=cfg_i,\n\u001B[32m    231\u001B[39m         adapter=adapter,\n\u001B[32m   (...)\u001B[39m\u001B[32m    236\u001B[39m         use_exogenous_mode=use_exogenous_mode\n\u001B[32m    237\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m238\u001B[39m     model = \u001B[43mtrainer\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtl_i\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtta_steps\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m0\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m    239\u001B[39m     best = {\u001B[33m\"\u001B[39m\u001B[33mmodel\u001B[39m\u001B[33m\"\u001B[39m: model, \u001B[33m\"\u001B[39m\u001B[33mcfg\u001B[39m\u001B[33m\"\u001B[39m: cfg_i}\n\u001B[32m    241\u001B[39m \u001B[38;5;28mprint\u001B[39m(\n\u001B[32m    242\u001B[39m     \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m[EXO-train] inferred E=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mE\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m | future_exo_cb? \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfuture_exo_cb\u001B[38;5;250m \u001B[39m\u001B[38;5;129;01mis\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;129;01mnot\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m | exo_is_normalized=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mexo_is_normalized\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\ts_forecaster_lib\\src\\modeling_module\\training\\engine.py:421\u001B[39m, in \u001B[36mCommonTrainer.fit\u001B[39m\u001B[34m(self, model, train_loader, val_loader, tta_steps)\u001B[39m\n\u001B[32m    417\u001B[39m     \u001B[38;5;28mself\u001B[39m.adapter.tta_reset(model)\n\u001B[32m    419\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mself\u001B[39m.cfg.epochs):\n\u001B[32m    420\u001B[39m     \u001B[38;5;66;03m# 1. 학습 루프 실행\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m421\u001B[39m     train_loss = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_run_epoch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[32m    423\u001B[39m     \u001B[38;5;66;03m# 2. 검증 루프 진입\u001B[39;00m\n\u001B[32m    424\u001B[39m     model.eval()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\ts_forecaster_lib\\src\\modeling_module\\training\\engine.py:349\u001B[39m, in \u001B[36mCommonTrainer._run_epoch\u001B[39m\u001B[34m(self, model, loader, train)\u001B[39m\n\u001B[32m    346\u001B[39m \u001B[38;5;28mself\u001B[39m._nan_stat(\u001B[33m\"\u001B[39m\u001B[33mpred\u001B[39m\u001B[33m\"\u001B[39m, pred)\n\u001B[32m    348\u001B[39m \u001B[38;5;66;03m# 손실 함수 계산 (Validation 여부 반영)\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m349\u001B[39m loss = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mloss_comp\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcompute\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpred\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mis_val\u001B[49m\u001B[43m=\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    351\u001B[39m \u001B[38;5;66;03m# 디버깅: Spike Loss 상세 분석 (초기 배치 한정)\u001B[39;00m\n\u001B[32m    352\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._get_spike_enabled():\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\ts_forecaster_lib\\src\\modeling_module\\training\\model_losses\\losses.py:520\u001B[39m, in \u001B[36mLossComputer.compute\u001B[39m\u001B[34m(self, pred, y, is_val)\u001B[39m\n\u001B[32m    517\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m mode == \u001B[33m\"\u001B[39m\u001B[33mdist\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m    518\u001B[39m     \u001B[38;5;66;03m# Prefer custom distribution loss if provided (e.g., DistributionLoss).\u001B[39;00m\n\u001B[32m    519\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.cfg.custom_loss \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(\u001B[38;5;28mself\u001B[39m.cfg.custom_loss, \u001B[33m\"\u001B[39m\u001B[33mis_distribution_output\u001B[39m\u001B[33m\"\u001B[39m, \u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[32m--> \u001B[39m\u001B[32m520\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_compute_dist_custom\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpred\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mis_val\u001B[49m\u001B[43m=\u001B[49m\u001B[43mis_val\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    521\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compute_dist(pred, y, is_val=is_val)\n\u001B[32m    523\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m mode == \u001B[33m\"\u001B[39m\u001B[33mquantile\u001B[39m\u001B[33m\"\u001B[39m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\ts_forecaster_lib\\src\\modeling_module\\training\\model_losses\\losses.py:730\u001B[39m, in \u001B[36mLossComputer._compute_dist_custom\u001B[39m\u001B[34m(self, pred, y, is_val)\u001B[39m\n\u001B[32m    727\u001B[39m         mask = mask * \u001B[38;5;28mself\u001B[39m._effective_horizon_weights(y2)\n\u001B[32m    729\u001B[39m \u001B[38;5;66;03m# DistributionLoss treats `mask` as weights (via _compute_weights -> weighted_average).\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m730\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mloss_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43my2\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmask\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\ts_forecaster_lib\\src\\modeling_module\\training\\model_losses\\loss_module.py:2055\u001B[39m, in \u001B[36mDistributionLoss.__call__\u001B[39m\u001B[34m(self, y, distr_args, mask)\u001B[39m\n\u001B[32m   2031\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33mr\u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m   2032\u001B[39m \u001B[33;03mComputes the negative log-likelihood objective function.\u001B[39;00m\n\u001B[32m   2033\u001B[39m \u001B[33;03mTo estimate the following predictive distribution:\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m   2052\u001B[39m \u001B[33;03m    float: Weighted loss function against which backpropagation will be performed.\u001B[39;00m\n\u001B[32m   2053\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m   2054\u001B[39m \u001B[38;5;66;03m# Instantiate Scaled Decoupled Distribution\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m2055\u001B[39m distr = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mget_distribution\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdistr_args\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdistr_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mdistribution_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   2056\u001B[39m loss_values = -distr.log_prob(y)\n\u001B[32m   2057\u001B[39m loss_weights = \u001B[38;5;28mself\u001B[39m._compute_weights(y=y, mask=mask)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\ts_forecaster_lib\\src\\modeling_module\\training\\model_losses\\loss_module.py:1949\u001B[39m, in \u001B[36mDistributionLoss.get_distribution\u001B[39m\u001B[34m(self, distr_args, **distribution_kwargs)\u001B[39m\n\u001B[32m   1938\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mget_distribution\u001B[39m(\u001B[38;5;28mself\u001B[39m, distr_args, **distribution_kwargs) -> Distribution:\n\u001B[32m   1939\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m   1940\u001B[39m \u001B[33;03m    Construct the associated Pytorch Distribution, given the collection of\u001B[39;00m\n\u001B[32m   1941\u001B[39m \u001B[33;03m    constructor arguments and, optionally, location and scale tensors.\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m   1947\u001B[39m \u001B[33;03m        Distribution: AffineTransformed distribution.\u001B[39;00m\n\u001B[32m   1948\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1949\u001B[39m     distr = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_base_distribution\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43mdistr_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mdistribution_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1950\u001B[39m     \u001B[38;5;28mself\u001B[39m.distr_mean = distr.mean\n\u001B[32m   1952\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.distribution \u001B[38;5;129;01min\u001B[39;00m (\u001B[33m\"\u001B[39m\u001B[33mPoisson\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mNegativeBinomial\u001B[39m\u001B[33m\"\u001B[39m):\n",
      "\u001B[31mTypeError\u001B[39m: Normal.__init__() takes from 3 to 4 positional arguments but 257 were given"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ============================================\n",
    "# CELL 6) 체크포인트 로드 + 평가 (첨부 노트북 흐름 유지)\n",
    "# - quantile 모델이면 eval_on_loader_quantile 사용\n",
    "# - point 모델이면 eval_on_loader 사용\n",
    "# ============================================\n",
    "\n",
    "# builders는 \"당신이 실제 저장한 모델 키\"에 맞추세요.\n",
    "# (첨부 노트북에서는 patchtst_quantile을 사용)\n",
    "# builders = {\n",
    "#     \"patchtst_quantile\": build_patchTST_quantile,\n",
    "#     \"patchtst\": build_patchTST_base,\n",
    "# }\n",
    "\n",
    "# model_A0 = load_model_dict(save_root_A0, builders, device = device)['patchtst_quantile']\n",
    "# model_A1 = load_model_dict(save_root_A1, builders, device = device)['patchtst_quantile']\n",
    "# model_A2 = load_model_dict(save_root_A2, builders, device = device)['patchtst_quantile']\n",
    "\n",
    "model_A0 = load_model_dict(save_root_A0, builders, device = device)['patchtst']\n",
    "model_A1 = load_model_dict(save_root_A1, builders, device = device)['patchtst']\n",
    "model_A2 = load_model_dict(save_root_A2, builders, device = device)['patchtst']\n",
    "\n",
    "# 첨부 노트북에서 사용하던 eval 유틸을 그대로 쓴다고 가정합니다.\n",
    "# (이미 앞 셀에 정의돼 있거나, 별도 모듈에 있으면 import 하세요.)\n",
    "# from your_notebook_utils import eval_on_loader_quantile\n",
    "\n",
    "# y0, yhat0 = eval_on_loader_quantile(model_A0, val_loader_A0, device, prefer_q=0.5)\n",
    "# y1, yhat1 = eval_on_loader_quantile(model_A1, val_loader_A1, device, prefer_q=0.5, future_exo_cb = future_exo_cb_time)\n",
    "# y2, yhat2 = eval_on_loader_quantile(model_A2, val_loader_A2, device, prefer_q=0.5, future_exo_cb = future_exo_cb_time_plus_holiday)\n",
    "\n",
    "y0, yhat0 = eval_on_loader(model_A0, val_loader_A0, device=device)\n",
    "y1, yhat1 = eval_on_loader(model_A1, val_loader_A1, device=device, future_exo_cb = future_exo_cb_time)\n",
    "y2, yhat2 = eval_on_loader(model_A2, val_loader_A2, device=device, future_exo_cb = future_exo_cb_time_plus_holiday)\n",
    "\n",
    "metric_A0 = {\n",
    "    \"MAE\": float(mae(y0.reshape(-1), yhat0.reshape(-1))),\n",
    "    \"RMSE\": float(rmse(y0.reshape(-1), yhat0.reshape(-1))),\n",
    "    \"SMAPE\": float(smape(y0.reshape(-1), yhat0.reshape(-1))),\n",
    "}\n",
    "metric_A1 = {\n",
    "    \"MAE\": float(mae(y1.reshape(-1), yhat1.reshape(-1))),\n",
    "    \"RMSE\": float(rmse(y1.reshape(-1), yhat1.reshape(-1))),\n",
    "    \"SMAPE\": float(smape(y1.reshape(-1), yhat1.reshape(-1))),\n",
    "}\n",
    "metric_A2 = {\n",
    "    \"MAE\": float(mae(y2.reshape(-1), yhat2.reshape(-1))),\n",
    "    \"RMSE\": float(rmse(y2.reshape(-1), yhat2.reshape(-1))),\n",
    "    \"SMAPE\": float(smape(y2.reshape(-1), yhat2.reshape(-1))),\n",
    "}\n",
    "print(f'metric_A0: {metric_A0}')\n",
    "print(f'metric_A1: {metric_A1}')\n",
    "print(f'metric_A2: {metric_A2}')\n",
    "# metric_A0, metric_A1, metric_A2"
   ],
   "id": "23848d388f5c7199",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ============================================\n",
    "# CELL 7) (선택) 예측 시각화 (첨부 노트북 스타일)\n",
    "# ============================================\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_samples(y_true, preds: dict, max_n: int = 64):\n",
    "    n = min(max_n, y_true.shape[0])\n",
    "    fig, axes = plt.subplots(nrows=n, ncols=1, figsize=(12, 2.2*n), sharex=True)\n",
    "\n",
    "    if n == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for i in range(n):\n",
    "        ax = axes[i]\n",
    "        ax.plot(y_true[i], label=\"true\")\n",
    "        for k, v in preds.items():\n",
    "            ax.plot(v[i], label=k)\n",
    "        ax.set_title(f\"sample={i}\", fontsize=9)\n",
    "        ax.legend(loc=\"upper right\", fontsize=8)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_samples(\n",
    "    y_true=y0,\n",
    "    preds={\n",
    "        \"A0_y_only\": yhat0,\n",
    "        \"A1_time\": yhat1,\n",
    "        \"A2_time+holiday\": yhat2,\n",
    "    },\n",
    "    max_n=32,\n",
    ")"
   ],
   "id": "b6f4c36e6eb72ff4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from modeling_module.training.forecater import DMSForecaster\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from modeling_module.utils.date_util import DateUtil\n",
    "\n",
    "from neuralforecast import NeuralForecast\n",
    "from neuralforecast.models import PatchTST as NixtlaPatchTST\n",
    "from neuralforecast.losses.pytorch import DistributionLoss\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 0) Reproducibility\n",
    "# =========================\n",
    "def set_seed(seed: int = 22):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(55)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 1) Config\n",
    "# =========================\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "H = 27\n",
    "LOOKBACK = 52\n",
    "PATCH_LEN = 24\n",
    "STRIDE = 24\n",
    "FREQ = \"W-MON\"\n",
    "\n",
    "PARQUET_PATH = DIR + \"train_data/walmart_best_feature_train.parquet\"\n",
    "\n",
    "# our_model\n",
    "our_model = model_A0\n",
    "our_model.eval()\n",
    "forecaster = DMSForecaster(our_model)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 2) Load polars -> pandas (Nixtla format)\n",
    "# =========================\n",
    "df = pl.read_parquet(PARQUET_PATH)\n",
    "\n",
    "# DateUtil을 통일해서 사용 (DateUtil.yyyyww_to_date)\n",
    "nixtla_df = (\n",
    "    df.select([\"unique_id\", \"date\", \"y\"])\n",
    "      .with_columns([\n",
    "          pl.col(\"unique_id\").cast(pl.Utf8),\n",
    "          pl.col(\"date\").map_elements(DateUtil.yyyyww_to_date, return_dtype=pl.Date).alias(\"ds\"),\n",
    "          pl.col(\"y\").cast(pl.Float64),\n",
    "      ])\n",
    "      .select([\"unique_id\", \"ds\", \"y\"])\n",
    "      .sort([\"unique_id\", \"ds\"])\n",
    "      .to_pandas()\n",
    ")\n",
    "nixtla_df[\"ds\"] = pd.to_datetime(nixtla_df[\"ds\"])\n",
    "nixtla_df[\"unique_id\"] = nixtla_df[\"unique_id\"].astype(str)\n",
    "\n",
    "print(\"[debug] raw unique_id:\", nixtla_df[\"unique_id\"].nunique())\n",
    "print(\"[debug] raw ds range:\", nixtla_df[\"ds\"].min(), \"~\", nixtla_df[\"ds\"].max())\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 3) Weekly reindex (안전 버전)\n",
    "#    - 같은 ds에 중복 row가 있으면 sum으로 집계 후 reindex\n",
    "# =========================\n",
    "def reindex_weekly_panel_sum(df_: pd.DataFrame, freq: str = \"W-MON\") -> pd.DataFrame:\n",
    "    out = []\n",
    "    for uid, g in df_.groupby(\"unique_id\", sort=False):\n",
    "        g = g.sort_values(\"ds\")\n",
    "\n",
    "        # (중요) 중복 ds를 sum으로 집계 (데이터에 중복 주차가 있으면 reindex가 꼬임)\n",
    "        g = g.groupby(\"ds\", as_index=False)[\"y\"].sum()\n",
    "\n",
    "        idx = pd.date_range(start=g[\"ds\"].min(), end=g[\"ds\"].max(), freq=freq)\n",
    "        gg = g.set_index(\"ds\").reindex(idx)\n",
    "        gg.index.name = \"ds\"\n",
    "        gg = gg.reset_index()\n",
    "        gg[\"unique_id\"] = uid\n",
    "        gg[\"y\"] = gg[\"y\"].fillna(0.0)\n",
    "\n",
    "        out.append(gg[[\"unique_id\", \"ds\", \"y\"]])\n",
    "    return pd.concat(out, ignore_index=True)\n",
    "\n",
    "nixtla_df = reindex_weekly_panel_sum(nixtla_df, freq=FREQ)\n",
    "\n",
    "print(\"[debug] after reindex unique_id:\", nixtla_df[\"unique_id\"].nunique())\n",
    "print(\"[debug] after reindex ds freq check sample:\")\n",
    "tmp = nixtla_df[nixtla_df[\"unique_id\"] == nixtla_df[\"unique_id\"].iloc[0]].sort_values(\"ds\")\n",
    "print(tmp[\"ds\"].diff().dropna().value_counts().head(3))\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 4) Length filter (여기가 0 만들기 가장 흔한 포인트)\n",
    "# =========================\n",
    "# val_size=H를 쓰면 train에 최소 (LOOKBACK + H)가 필요하고\n",
    "# holdout test로 H를 따로 빼니 전체는 LOOKBACK + 2H가 \"권장\"이지만,\n",
    "# 데이터가 짧다면 우선 완화해서 돌리는 게 맞습니다.\n",
    "MIN_REQUIRED = LOOKBACK + H  # <-- 완화 (기존 LOOKBACK + 2H -> 너무 빡빡해서 0될 가능성 큼)\n",
    "\n",
    "lens = nixtla_df.groupby(\"unique_id\")[\"y\"].size().sort_values()\n",
    "valid_ids = lens[lens >= (MIN_REQUIRED + H)].index  # train에 LOOKBACK+H, test에 H => total >= LOOKBACK+2H\n",
    "# 만약 여기서도 0이면, 아래 줄을 더 완화:\n",
    "# valid_ids = lens[lens >= (LOOKBACK + H)].index\n",
    "\n",
    "panel = nixtla_df[nixtla_df[\"unique_id\"].isin(valid_ids)].copy()\n",
    "\n",
    "print(\"[debug] valid_ids:\", len(valid_ids))\n",
    "if len(valid_ids) == 0:\n",
    "    print(\"[debug] shortest series length top5:\\n\", lens.head(5))\n",
    "    print(\"[debug] longest series length top5:\\n\", lens.tail(5))\n",
    "    raise RuntimeError(\n",
    "        \"No valid_ids remain after length filtering. \"\n",
    "        \"Lower LOOKBACK/H or relax MIN_REQUIRED condition.\"\n",
    "    )\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 5) Holdout split (last H as test)\n",
    "# =========================\n",
    "def split_last_h(df_: pd.DataFrame, h: int) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    df_ = df_.sort_values([\"unique_id\", \"ds\"]).reset_index(drop=True)\n",
    "    test = df_.groupby(\"unique_id\", group_keys=False).tail(h)\n",
    "    train = df_.drop(test.index)\n",
    "    return train.reset_index(drop=True), test.reset_index(drop=True)\n",
    "\n",
    "train_df, test_df = split_last_h(panel, h=H)\n",
    "\n",
    "print(\"[debug] train unique_id:\", train_df[\"unique_id\"].nunique(), \"rows:\", len(train_df))\n",
    "print(\"[debug] test  unique_id:\", test_df[\"unique_id\"].nunique(), \"rows:\", len(test_df))\n",
    "\n",
    "if train_df[\"unique_id\"].nunique() == 0:\n",
    "    raise RuntimeError(\"train_df has zero groups. Check filtering/splitting logic.\")\n",
    "\n",
    "futr_df = test_df[[\"unique_id\", \"ds\"]].copy()\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 6) Nixtla PatchTST train + predict (holdout ds 사용)\n",
    "# =========================\n",
    "nixtla_model = NixtlaPatchTST(\n",
    "    h=H,\n",
    "    input_size=LOOKBACK,\n",
    "    patch_len=PATCH_LEN,\n",
    "    stride=STRIDE,\n",
    "    hidden_size=16,\n",
    "    n_heads=4,\n",
    "    start_padding_enabled=True,\n",
    "\n",
    "    scaler_type=\"standard\",\n",
    "    revin=True,\n",
    "\n",
    "    loss=DistributionLoss(distribution=\"StudentT\", level=[80, 90]),\n",
    "    learning_rate=1e-3,\n",
    "    max_steps=500,\n",
    "    val_check_steps=50,\n",
    "    early_stop_patience_steps=2,\n",
    "\n",
    "    accelerator=\"gpu\" if DEVICE == \"cuda\" else \"cpu\",\n",
    "    devices=1,\n",
    "    enable_progress_bar=False,\n",
    ")\n",
    "\n",
    "nf = NeuralForecast(models=[nixtla_model], freq=FREQ)\n",
    "\n",
    "# (중요) train이 짧다면 val_size=H가 다시 문제를 만들 수 있음 -> 안전하게 0으로도 가능\n",
    "val_size = H if (train_df.groupby(\"unique_id\")[\"y\"].size().min() >= (LOOKBACK + H)) else 0\n",
    "print(\"[debug] using val_size =\", val_size)\n",
    "\n",
    "nf.fit(df=train_df, val_size=val_size)\n",
    "\n",
    "fcst_nf = nf.predict(futr_df=futr_df).reset_index(drop=False)\n",
    "\n",
    "# 예측 컬럼명 추출 (DistributionLoss면 PatchTST-median이 있을 수도 있음)\n",
    "pred_cols = [c for c in fcst_nf.columns if c.startswith(\"PatchTST\")]\n",
    "NI_COL = \"PatchTST-median\" if \"PatchTST-median\" in pred_cols else \"PatchTST\"\n",
    "print(\"[debug] nixtla pred col:\", NI_COL)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 7) Our predict (DMSForecaster)\n",
    "# =========================\n",
    "our_rows = []\n",
    "for uid, g_tr in train_df.groupby(\"unique_id\", sort=False):\n",
    "    g_tr = g_tr.sort_values(\"ds\")\n",
    "    y_hist = g_tr[\"y\"].to_numpy(dtype=np.float32)\n",
    "\n",
    "    # 마지막 LOOKBACK window\n",
    "    x_win = y_hist[-LOOKBACK:]\n",
    "    x_t = torch.from_numpy(x_win).unsqueeze(0)  # (1, LOOKBACK)\n",
    "\n",
    "    out = forecaster.predict(x_t, horizon=H, device=DEVICE, mode=\"eval\", future_exo_cb = future_exo_cb_time)\n",
    "    y_hat = np.asarray(out[\"point\"], dtype=np.float32).reshape(-1)\n",
    "\n",
    "    g_te = test_df[test_df[\"unique_id\"] == uid].sort_values(\"ds\")\n",
    "    ds_te = g_te[\"ds\"].to_list()\n",
    "    y_te = g_te[\"y\"].to_numpy(dtype=np.float32)\n",
    "\n",
    "    if len(ds_te) != H:\n",
    "        raise RuntimeError(f\"uid={uid} test length != H. got {len(ds_te)}\")\n",
    "\n",
    "    for d, yt, yp in zip(ds_te, y_te, y_hat):\n",
    "        our_rows.append((uid, d, float(yt), float(yp)))\n",
    "\n",
    "our_df = pd.DataFrame(our_rows, columns=[\"unique_id\", \"ds\", \"y_true\", \"y_pred_ours\"])\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 8) Merge\n",
    "# =========================\n",
    "test_true = test_df.rename(columns={\"y\": \"y_true\"})\n",
    "nixtla_pred = fcst_nf[[\"unique_id\", \"ds\", NI_COL]].rename(columns={NI_COL: \"y_pred_nixtla\"})\n",
    "\n",
    "merged = (\n",
    "    test_true.merge(nixtla_pred, on=[\"unique_id\", \"ds\"], how=\"inner\")\n",
    "             .merge(our_df, on=[\"unique_id\", \"ds\", \"y_true\"], how=\"inner\")\n",
    ")\n",
    "\n",
    "merged_pl = pl.from_pandas(merged).sort([\"unique_id\", \"ds\"])\n",
    "print(merged_pl.head())\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 9) Metrics\n",
    "# =========================\n",
    "def mae(a, b): return float(np.mean(np.abs(a - b)))\n",
    "def rmse(a, b): return float(np.sqrt(np.mean((a - b) ** 2)))\n",
    "def smape(a, b):\n",
    "    denom = (np.abs(a) + np.abs(b)) + 1e-9\n",
    "    return float(np.mean(2.0 * np.abs(a - b) / denom))\n",
    "\n",
    "y = merged[\"y_true\"].to_numpy(np.float32)\n",
    "p_n = merged[\"y_pred_nixtla\"].to_numpy(np.float32)\n",
    "p_o = merged[\"y_pred_ours\"].to_numpy(np.float32)\n",
    "\n",
    "print(\"\\n[Overall Metrics]\")\n",
    "print(f\"Nixtla  MAE={mae(y,p_n):.4f} | RMSE={rmse(y,p_n):.4f} | sMAPE={smape(y,p_n):.4f}\")\n",
    "print(f\"Our     MAE={mae(y,p_o):.4f} | RMSE={rmse(y,p_o):.4f} | sMAPE={smape(y,p_o):.4f}\")\n",
    "\n",
    "per_id_mae = (\n",
    "    merged_pl.group_by(\"unique_id\")\n",
    "      .agg([\n",
    "          (pl.col(\"y_true\") - pl.col(\"y_pred_nixtla\")).abs().mean().alias(\"mae_nixtla\"),\n",
    "          (pl.col(\"y_true\") - pl.col(\"y_pred_ours\")).abs().mean().alias(\"mae_ours\"),\n",
    "      ])\n",
    "      .sort(\"mae_ours\")\n",
    ")\n",
    "print(\"\\n[Per-ID MAE]\")\n",
    "print(per_id_mae.head(10))\n",
    "\n",
    "per_id = (\n",
    "    merged_pl.group_by(\"unique_id\")\n",
    "    .agg([\n",
    "        (pl.col(\"y_true\") - pl.col(\"y_pred_nixtla\")).abs().mean().alias(\"mae_nixtla\"),\n",
    "        (pl.col(\"y_true\") - pl.col(\"y_pred_ours\")).abs().mean().alias(\"mae_ours\"),\n",
    "    ])\n",
    "    .with_columns((pl.col(\"mae_nixtla\") - pl.col(\"mae_ours\")).alias(\"mae_diff\"))  # +면 ours 승\n",
    ")\n",
    "\n",
    "print(\"ours wins:\", per_id.filter(pl.col(\"mae_diff\") > 0).height)\n",
    "print(\"nixtla wins:\", per_id.filter(pl.col(\"mae_diff\") < 0).height)\n",
    "print(\"ties:\", per_id.filter(pl.col(\"mae_diff\") == 0).height)\n",
    "\n",
    "print(\"\\n[Nixtla best 10]\")\n",
    "print(per_id.sort(\"mae_diff\").head(10))\n",
    "\n",
    "print(\"\\n[Ours best 10]\")\n",
    "print(per_id.sort(\"mae_diff\", descending=True).head(10))\n",
    "\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 10) Plot sample id\n",
    "# =========================\n",
    "def plot_all_uids_with_error(\n",
    "    merged_pl: pl.DataFrame,\n",
    "    uids,\n",
    "    *,\n",
    "    ncols: int = 1,\n",
    "    figsize_per_uid=(12, 6),\n",
    "    sharex: bool = False,\n",
    "):\n",
    "    \"\"\"\n",
    "    각 uid마다 (Forecast, Absolute Error) 2행 subplot을 만들고,\n",
    "    전체 uid를 하나의 fig에 그려서 복사/저장하기 쉽게 구성.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    merged_pl : pl.DataFrame\n",
    "        columns: unique_id, ds, y_true, y_pred_nixtla, y_pred_ours\n",
    "    uids : Iterable[str|int]\n",
    "        그릴 unique_id 목록\n",
    "    ncols : int\n",
    "        uid 패널을 가로로 몇 개 놓을지 (1이면 세로로만 쌓임)\n",
    "    figsize_per_uid : tuple\n",
    "        uid 1개(2행짜리)당 기본 figsize\n",
    "    sharex : bool\n",
    "        전체 축 sharex 여부 (uid별 기간이 다르면 False 권장)\n",
    "    \"\"\"\n",
    "    uids = [str(u) for u in uids]\n",
    "    n = len(uids)\n",
    "    if n == 0:\n",
    "        raise ValueError(\"uids is empty.\")\n",
    "\n",
    "    ncols = max(1, int(ncols))\n",
    "    nrows_uid = int(np.ceil(n / ncols))\n",
    "\n",
    "    # uid 1개당 2행이므로 전체 행 수는 2 * nrows_uid\n",
    "    total_rows = 2 * nrows_uid\n",
    "    fig_w = figsize_per_uid[0] * ncols\n",
    "    fig_h = figsize_per_uid[1] * nrows_uid\n",
    "\n",
    "    fig, axes = plt.subplots(\n",
    "        total_rows, ncols,\n",
    "        figsize=(fig_w, fig_h),\n",
    "        sharex=sharex,\n",
    "        squeeze=False\n",
    "    )\n",
    "\n",
    "    # axes indexing helper: uid index i -> (block_row, col)\n",
    "    for i, uid in enumerate(uids):\n",
    "        block_r = (i // ncols)  # uid block row\n",
    "        c = (i % ncols)\n",
    "\n",
    "        ax_fore = axes[2 * block_r][c]\n",
    "        ax_err  = axes[2 * block_r + 1][c]\n",
    "\n",
    "        p = (\n",
    "            merged_pl\n",
    "            .filter(pl.col(\"unique_id\") == uid)\n",
    "            .sort(\"ds\")\n",
    "            .to_pandas()\n",
    "        )\n",
    "        if len(p) == 0:\n",
    "            ax_fore.set_title(f\"Forecast (uid={uid}) - NO DATA\")\n",
    "            ax_fore.axis(\"off\")\n",
    "            ax_err.axis(\"off\")\n",
    "            continue\n",
    "\n",
    "        err_n = np.abs(p[\"y_true\"] - p[\"y_pred_nixtla\"])\n",
    "        err_o = np.abs(p[\"y_true\"] - p[\"y_pred_ours\"])\n",
    "\n",
    "        # --- Forecast\n",
    "        ax_fore.plot(p[\"ds\"], p[\"y_true\"], label=\"true\")\n",
    "        ax_fore.plot(p[\"ds\"], p[\"y_pred_nixtla\"], label=\"nixtla\")\n",
    "        ax_fore.plot(p[\"ds\"], p[\"y_pred_ours\"], label=\"ours\")\n",
    "        ax_fore.set_title(f\"Forecast (uid={uid})\")\n",
    "        ax_fore.legend(loc=\"best\")\n",
    "\n",
    "        # --- Error\n",
    "        ax_err.plot(p[\"ds\"], err_n, label=\"abs_err_nixtla\")\n",
    "        ax_err.plot(p[\"ds\"], err_o, label=\"abs_err_ours\")\n",
    "        ax_err.set_title(\"Absolute Error\")\n",
    "        ax_err.legend(loc=\"best\")\n",
    "\n",
    "    # 남는 subplot(빈칸) 끄기\n",
    "    for j in range(n, nrows_uid * ncols):\n",
    "        block_r = (j // ncols)\n",
    "        c = (j % ncols)\n",
    "        axes[2 * block_r][c].axis(\"off\")\n",
    "        axes[2 * block_r + 1][c].axis(\"off\")\n",
    "\n",
    "    fig.tight_layout()\n",
    "    return fig, axes\n",
    "\n",
    "# 사용 예시 1) 세로로 전부 쌓기 (복사하기 가장 직관적)\n",
    "fig, axes = plot_all_uids_with_error(merged_pl, valid_ids, ncols=1, sharex=False)"
   ],
   "id": "518ad6633e730409",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
