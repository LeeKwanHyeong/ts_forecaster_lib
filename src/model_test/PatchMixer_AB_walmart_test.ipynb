{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T01:04:07.542686Z",
     "start_time": "2026-01-28T01:04:06.338484Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# LTB modules (expected to exist in your repo)\n",
    "\n",
    "# optional\n",
    "\n",
    "'''\n",
    "pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128\n",
    "https://developer.nvidia.com/cuda-12-8-0-download-archive\n",
    "'''\n",
    "\n",
    "MAC_DIR = \"/Users/igwanhyeong/PycharmProjects/ts_forecaster_lib/raw_data/\"\n",
    "WINDOW_DIR = \"C:/Users/USER/PycharmProjects/ts_forecaster_lib/raw_data/\"\n",
    "\n",
    "DIR = WINDOW_DIR if sys.platform == \"win32\" else MAC_DIR\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(\"DIR:\", DIR)\n",
    "print(\"device:\", device)\n",
    "if device == \"cuda\":\n",
    "    print(\"cuda:\", torch.version.cuda, \"gpu_count:\", torch.cuda.device_count())"
   ],
   "id": "7ea7c6455ecd9068",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DIR: C:/Users/USER/PycharmProjects/ts_forecaster_lib/raw_data/\n",
      "device: cuda\n",
      "cuda: 12.8 gpu_count: 1\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T01:04:07.820261Z",
     "start_time": "2026-01-28T01:04:07.804352Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = pl.read_parquet(DIR + 'train_data/walmart_best_feature_train.parquet')\n",
    "df"
   ],
   "id": "9e0647c6520c5d56",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "shape: (6_435, 38)\n",
       "┌───────────┬──────────┬────────┬────────────┬───┬────────────┬────────────┬───────────┬───────────┐\n",
       "│ unique_id ┆ date_idx ┆ date   ┆ y          ┆ … ┆ exo_p_mark ┆ exo_p_mark ┆ exo_p_mar ┆ exo_c_woy │\n",
       "│ ---       ┆ ---      ┆ ---    ┆ ---        ┆   ┆ down3      ┆ down4      ┆ kdown5    ┆ _bucket   │\n",
       "│ str       ┆ i64      ┆ i32    ┆ f32        ┆   ┆ ---        ┆ ---        ┆ ---       ┆ ---       │\n",
       "│           ┆          ┆        ┆            ┆   ┆ f32        ┆ f32        ┆ f32       ┆ i32       │\n",
       "╞═══════════╪══════════╪════════╪════════════╪═══╪════════════╪════════════╪═══════════╪═══════════╡\n",
       "│ 1         ┆ 14641    ┆ 201005 ┆ 1.6437e6   ┆ … ┆ 0.0        ┆ 0.0        ┆ 0.0       ┆ 1         │\n",
       "│ 1         ┆ 14648    ┆ 201006 ┆ 1641957.5  ┆ … ┆ 0.0        ┆ 0.0        ┆ 0.0       ┆ 1         │\n",
       "│ 1         ┆ 14655    ┆ 201007 ┆ 1.6120e6   ┆ … ┆ 0.0        ┆ 0.0        ┆ 0.0       ┆ 1         │\n",
       "│ 1         ┆ 14662    ┆ 201008 ┆ 1.4097e6   ┆ … ┆ 0.0        ┆ 0.0        ┆ 0.0       ┆ 2         │\n",
       "│ 1         ┆ 14669    ┆ 201009 ┆ 1.5548e6   ┆ … ┆ 0.0        ┆ 0.0        ┆ 0.0       ┆ 2         │\n",
       "│ …         ┆ …        ┆ …      ┆ …          ┆ … ┆ …          ┆ …          ┆ …         ┆ …         │\n",
       "│ 9         ┆ 15607    ┆ 201239 ┆ 516361.062 ┆ … ┆ 0.55       ┆ 190.380005 ┆ 1819.1500 ┆ 9         │\n",
       "│           ┆          ┆        ┆ 5          ┆   ┆            ┆            ┆ 24        ┆           │\n",
       "│ 9         ┆ 15614    ┆ 201240 ┆ 606755.312 ┆ … ┆ 3.01       ┆ 1107.79003 ┆ 1560.5500 ┆ 10        │\n",
       "│           ┆          ┆        ┆ 5          ┆   ┆            ┆ 9          ┆ 49        ┆           │\n",
       "│ 9         ┆ 15621    ┆ 201241 ┆ 558464.812 ┆ … ┆ 6.01       ┆ 0.0        ┆ 2839.8400 ┆ 10        │\n",
       "│           ┆          ┆        ┆ 5          ┆   ┆            ┆            ┆ 88        ┆           │\n",
       "│ 9         ┆ 15628    ┆ 201242 ┆ 542009.437 ┆ … ┆ 8.0        ┆ 28.940001  ┆ 3098.8701 ┆ 10        │\n",
       "│           ┆          ┆        ┆ 5          ┆   ┆            ┆            ┆ 17        ┆           │\n",
       "│ 9         ┆ 15635    ┆ 201243 ┆ 549731.5   ┆ … ┆ 8.0        ┆ 0.0        ┆ 1666.3800 ┆ 10        │\n",
       "│           ┆          ┆        ┆            ┆   ┆            ┆            ┆ 05        ┆           │\n",
       "└───────────┴──────────┴────────┴────────────┴───┴────────────┴────────────┴───────────┴───────────┘"
      ],
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (6_435, 38)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>unique_id</th><th>date_idx</th><th>date</th><th>y</th><th>exo_is_holiday</th><th>exo_temperature</th><th>exo_fuel_price</th><th>exo_cpi</th><th>exo_unemployment</th><th>exo_markdown_sum</th><th>exo_markdown1</th><th>exo_markdown2</th><th>exo_markdown3</th><th>exo_markdown4</th><th>exo_markdown5</th><th>exo_markdown1_isnull</th><th>exo_markdown2_isnull</th><th>exo_markdown3_isnull</th><th>exo_markdown4_isnull</th><th>exo_markdown5_isnull</th><th>exo_p_y_lag_1w</th><th>exo_p_y_lag_2w</th><th>exo_p_y_lag_52w</th><th>exo_p_y_rollmean_4w</th><th>exo_p_y_rollmean_12w</th><th>exo_p_y_rollstd_4w</th><th>exo_p_weeks_since_holiday</th><th>exo_p_temperature</th><th>exo_p_fuel_price</th><th>exo_p_cpi</th><th>exo_p_unemployment</th><th>exo_p_markdown_sum</th><th>exo_p_markdown1</th><th>exo_p_markdown2</th><th>exo_p_markdown3</th><th>exo_p_markdown4</th><th>exo_p_markdown5</th><th>exo_c_woy_bucket</th></tr><tr><td>str</td><td>i64</td><td>i32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>i32</td></tr></thead><tbody><tr><td>&quot;1&quot;</td><td>14641</td><td>201005</td><td>1.6437e6</td><td>0.0</td><td>42.310001</td><td>2.572</td><td>211.096359</td><td>8.106</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>1.0</td><td>1.0</td><td>1.0</td><td>1.0</td><td>1.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>999.0</td><td>42.310001</td><td>2.572</td><td>211.096359</td><td>8.106</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>1</td></tr><tr><td>&quot;1&quot;</td><td>14648</td><td>201006</td><td>1641957.5</td><td>1.0</td><td>38.509998</td><td>2.548</td><td>211.242172</td><td>8.106</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>1.0</td><td>1.0</td><td>1.0</td><td>1.0</td><td>1.0</td><td>1.6437e6</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>38.509998</td><td>2.548</td><td>211.242172</td><td>8.106</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>1</td></tr><tr><td>&quot;1&quot;</td><td>14655</td><td>201007</td><td>1.6120e6</td><td>0.0</td><td>39.93</td><td>2.514</td><td>211.289139</td><td>8.106</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>1.0</td><td>1.0</td><td>1.0</td><td>1.0</td><td>1.0</td><td>1641957.5</td><td>1.6437e6</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>1.0</td><td>39.93</td><td>2.514</td><td>211.289139</td><td>8.106</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>1</td></tr><tr><td>&quot;1&quot;</td><td>14662</td><td>201008</td><td>1.4097e6</td><td>0.0</td><td>46.630001</td><td>2.561</td><td>211.319641</td><td>8.106</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>1.0</td><td>1.0</td><td>1.0</td><td>1.0</td><td>1.0</td><td>1.6120e6</td><td>1641957.5</td><td>0.0</td><td>1.576836e6</td><td>0.0</td><td>112353.398438</td><td>2.0</td><td>46.630001</td><td>2.561</td><td>211.319641</td><td>8.106</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>2</td></tr><tr><td>&quot;1&quot;</td><td>14669</td><td>201009</td><td>1.5548e6</td><td>0.0</td><td>46.5</td><td>2.625</td><td>211.350143</td><td>8.106</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>1.0</td><td>1.0</td><td>1.0</td><td>1.0</td><td>1.0</td><td>1.4097e6</td><td>1.6120e6</td><td>0.0</td><td>1.554615e6</td><td>0.0</td><td>103135.0</td><td>3.0</td><td>46.5</td><td>2.625</td><td>211.350143</td><td>8.106</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>2</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>&quot;9&quot;</td><td>15607</td><td>201239</td><td>516361.0625</td><td>0.0</td><td>76.800003</td><td>3.666</td><td>226.763077</td><td>5.277</td><td>3711.670166</td><td>1699.680054</td><td>1.91</td><td>0.55</td><td>190.380005</td><td>1819.150024</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>533756.875</td><td>523427.34375</td><td>508567.03125</td><td>534839.375</td><td>536946.5625</td><td>21849.310547</td><td>3.0</td><td>76.800003</td><td>3.666</td><td>226.763077</td><td>5.277</td><td>3711.670166</td><td>1699.680054</td><td>1.91</td><td>0.55</td><td>190.380005</td><td>1819.150024</td><td>9</td></tr><tr><td>&quot;9&quot;</td><td>15614</td><td>201240</td><td>606755.3125</td><td>0.0</td><td>66.610001</td><td>3.617</td><td>226.966232</td><td>4.954</td><td>5328.919922</td><td>2657.570068</td><td>0.0</td><td>3.01</td><td>1107.790039</td><td>1560.550049</td><td>0.0</td><td>1.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>516361.0625</td><td>533756.875</td><td>553837.0</td><td>545075.125</td><td>542798.0625</td><td>41735.964844</td><td>4.0</td><td>66.610001</td><td>3.617</td><td>226.966232</td><td>4.954</td><td>5328.919922</td><td>2657.570068</td><td>0.0</td><td>3.01</td><td>1107.790039</td><td>1560.550049</td><td>10</td></tr><tr><td>&quot;9&quot;</td><td>15621</td><td>201241</td><td>558464.8125</td><td>0.0</td><td>60.09</td><td>3.601</td><td>227.169388</td><td>4.954</td><td>3366.26001</td><td>520.409973</td><td>0.0</td><td>6.01</td><td>0.0</td><td>2839.840088</td><td>0.0</td><td>1.0</td><td>0.0</td><td>1.0</td><td>0.0</td><td>606755.3125</td><td>516361.0625</td><td>529515.6875</td><td>553834.5</td><td>546504.1875</td><td>39282.828125</td><td>5.0</td><td>60.09</td><td>3.601</td><td>227.169388</td><td>4.954</td><td>3366.26001</td><td>520.409973</td><td>0.0</td><td>6.01</td><td>0.0</td><td>2839.840088</td><td>10</td></tr><tr><td>&quot;9&quot;</td><td>15628</td><td>201242</td><td>542009.4375</td><td>0.0</td><td>68.010002</td><td>3.594</td><td>227.214294</td><td>4.954</td><td>3681.530029</td><td>545.719971</td><td>0.0</td><td>8.0</td><td>28.940001</td><td>3098.870117</td><td>0.0</td><td>1.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>558464.8125</td><td>606755.3125</td><td>557075.1875</td><td>555897.625</td><td>550342.375</td><td>38074.996094</td><td>6.0</td><td>68.010002</td><td>3.594</td><td>227.214294</td><td>4.954</td><td>3681.530029</td><td>545.719971</td><td>0.0</td><td>8.0</td><td>28.940001</td><td>3098.870117</td><td>10</td></tr><tr><td>&quot;9&quot;</td><td>15635</td><td>201243</td><td>549731.5</td><td>0.0</td><td>69.519997</td><td>3.506</td><td>227.232803</td><td>4.954</td><td>2189.609863</td><td>512.22998</td><td>3.0</td><td>8.0</td><td>0.0</td><td>1666.380005</td><td>0.0</td><td>0.0</td><td>0.0</td><td>1.0</td><td>0.0</td><td>542009.4375</td><td>558464.8125</td><td>548527.5</td><td>564240.25</td><td>551662.6875</td><td>29129.589844</td><td>7.0</td><td>69.519997</td><td>3.506</td><td>227.232803</td><td>4.954</td><td>2189.609863</td><td>512.22998</td><td>3.0</td><td>8.0</td><td>0.0</td><td>1666.380005</td><td>10</td></tr></tbody></table></div>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T01:04:08.288585Z",
     "start_time": "2026-01-28T01:04:08.285312Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import random\n",
    "\n",
    "past_exo_cont_cols = (\n",
    "    # \"exo_p_y_lag_1w\",\n",
    "    \"exo_p_y_lag_2w\",\n",
    "    # \"exo_p_y_lag_52w\",\n",
    "    # \"exo_p_y_rollmean_4w\",\"exo_p_y_rollmean_12w\",\"exo_p_y_rollstd_4w\",\n",
    "    # \"exo_p_weeks_since_holiday`\",\n",
    "    # \"exo_p_temperature\",\n",
    "    # \"exo_p_fuel_price\",\n",
    "    # \"exo_p_cpi\",\n",
    "    # \"exo_p_unemployment\",\n",
    "    # \"exo_p_markdown_sum\",\n",
    "    # \"exo_p_markdown1\",\n",
    "    # \"exo_p_markdown2\",\n",
    "    # \"exo_p_markdown3\",\n",
    "    # \"exo_p_markdown4\",\n",
    "    # \"exo_p_markdown5\",\n",
    "    # \"exo_markdown1_isnull\",\n",
    "    # \"exo_markdown2_isnull\",\n",
    "    # \"exo_markdown3_isnull\",\n",
    "    # \"exo_markdown4_isnull\",\n",
    "    # \"exo_markdown5_isnull\",\n",
    ")\n",
    "past_exo_cat_cols = (\n",
    "    # \"exo_c_woy_bucket\",\n",
    ")\n",
    "\n",
    "lookback = 52\n",
    "horizon = 27\n",
    "batch_size = 256\n",
    "\n",
    "freq = \"weekly\"          # walmart dt is weekly\n",
    "split_mode = \"multi\"     # id-disjoint split (leakage-safe)\n",
    "shuffle = True\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "print(\"device:\", device)\n"
   ],
   "id": "eb88c7fa04338f17",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T01:04:08.828161Z",
     "start_time": "2026-01-28T01:04:08.816128Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "from modeling_module.utils.exogenous_utils import compose_exo_calendar_cb\n",
    "\n",
    "future_exo_cb_time = compose_exo_calendar_cb(date_type=freq)\n",
    "\n",
    "# ============================================================\n",
    "# Holiday lookup (vectorized) + FutureExo callback (time + holiday)\n",
    "# ============================================================\n",
    "holiday_map_dayidx = {\n",
    "    int(row[0]): float(row[1])\n",
    "    for row in (\n",
    "        df.select([\"date_idx\", \"exo_is_holiday\"])\n",
    "          .group_by(\"date_idx\")\n",
    "          .agg(pl.max(\"exo_is_holiday\").alias(\"exo_is_holiday\"))\n",
    "          .sort(\"date_idx\")\n",
    "          .iter_rows()\n",
    "    )\n",
    "}\n",
    "\n",
    "def build_holiday_array(holiday_map_dayidx: dict[int, float], *, pad: int = 0) -> np.ndarray:\n",
    "    if not holiday_map_dayidx:\n",
    "        return np.zeros((1,), dtype=np.float32)\n",
    "    max_k = max(int(k) for k in holiday_map_dayidx.keys())\n",
    "    arr = np.zeros((max_k + 1 + int(pad),), dtype=np.float32)\n",
    "    for k, v in holiday_map_dayidx.items():\n",
    "        kk = int(k)\n",
    "        if kk >= 0:\n",
    "            arr[kk] = float(v)\n",
    "    return arr\n",
    "\n",
    "class FutureExoTimePlusHoliday:\n",
    "    def __init__(self, holiday_by_dayidx: np.ndarray, *, step_days: int = 7):\n",
    "        self.holiday = holiday_by_dayidx.astype(np.float32, copy=False)\n",
    "        self.step_days = int(step_days)\n",
    "\n",
    "    def __call__(self, start_idx, H: int, device: str = \"cpu\"):\n",
    "        # 1) calendar exo (batch-safe)\n",
    "        cal = future_exo_cb_time(start_idx, H, device=device)  # scalar: (H,E) | batch: (B,H,E)\n",
    "\n",
    "        # 2) holiday exo (vectorized in numpy)\n",
    "        is_scalar = isinstance(start_idx, (int, np.integer))\n",
    "        if is_scalar:\n",
    "            s = np.asarray([int(start_idx)], dtype=np.int64)\n",
    "        else:\n",
    "            s = np.asarray(start_idx, dtype=np.int64).reshape(-1)\n",
    "\n",
    "        B = s.shape[0]\n",
    "        H = int(H)\n",
    "\n",
    "        offsets = (self.step_days * np.arange(H, dtype=np.int64))[None, :]  # (1,H)\n",
    "        idx = s[:, None] + offsets                                          # (B,H)\n",
    "\n",
    "        hol = np.zeros((B, H), dtype=np.float32)\n",
    "        valid = (idx >= 0) & (idx < self.holiday.shape[0])\n",
    "        hol[valid] = self.holiday[idx[valid]]\n",
    "        hol_t = torch.from_numpy(hol).unsqueeze(-1)                         # (B,H,1), CPU\n",
    "\n",
    "        target_device = cal.device  # cal이 이미 cuda일 수 있음\n",
    "        cal = cal.to(target_device, dtype=torch.float32)\n",
    "        hol_t = hol_t.to(target_device, dtype=torch.float32)\n",
    "\n",
    "        # 3) concat\n",
    "        if is_scalar:\n",
    "            out = torch.cat([cal.to(torch.float32).unsqueeze(0), hol_t], dim=-1)[0]  # (H,E+1)\n",
    "        else:\n",
    "            out = torch.cat([cal.to(torch.float32), hol_t], dim=-1)                  # (B,H,E+1)\n",
    "\n",
    "        return out\n",
    "\n",
    "holiday_by_dayidx = build_holiday_array(holiday_map_dayidx, pad=7 * (horizon + 2))\n",
    "future_exo_cb_time_plus_holiday = FutureExoTimePlusHoliday(holiday_by_dayidx, step_days=7)\n"
   ],
   "id": "f6d34cdf570a92ed",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T01:04:13.875598Z",
     "start_time": "2026-01-28T01:04:09.351317Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from modeling_module.training.model_losses.loss_module import DistributionLoss, MQLoss\n",
    "from modeling_module.utils.metrics import mae, rmse, smape\n",
    "from modeling_module.utils.eval_utils import eval_on_loader, eval_on_loader_quantile\n",
    "from modeling_module.utils.checkpoint import load_model_dict\n",
    "from modeling_module.models import build_patchTST_quantile, build_patchTST_base, build_patch_mixer_quantile, \\\n",
    "    build_patch_mixer_base\n",
    "from modeling_module.training.model_trainers.total_train import run_total_train_weekly\n",
    "from modeling_module.data_loader import MultiPartExoDataModule\n",
    "import pandas as pd\n",
    "rows = []  # seed loop 밖에서 선언\n",
    "\n",
    "def set_seed(seed: int = 11):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.backends.mps.is_available():\n",
    "        torch.mps.manual_seed(seed)\n",
    "    elif torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def build_datamodule(variant: str, seed: int) -> MultiPartExoDataModule:\n",
    "    if variant == \"A0\":\n",
    "        future_exo_cb = None\n",
    "    elif variant == \"A1\":\n",
    "        future_exo_cb = future_exo_cb_time\n",
    "    elif variant == \"A2\":\n",
    "        future_exo_cb = future_exo_cb_time_plus_holiday\n",
    "    else:\n",
    "        raise ValueError(variant)\n",
    "\n",
    "    return MultiPartExoDataModule(\n",
    "        df=df,\n",
    "        id_col=\"unique_id\",\n",
    "        date_col=\"date\",\n",
    "        y_col=\"y\",\n",
    "        lookback=lookback,\n",
    "        horizon=horizon,\n",
    "        batch_size=batch_size,\n",
    "        past_exo_cont_cols=past_exo_cont_cols,\n",
    "        past_exo_cat_cols=past_exo_cat_cols,\n",
    "        future_exo_cb=future_exo_cb,\n",
    "        freq=freq,\n",
    "        shuffle=shuffle,\n",
    "        split_mode=split_mode,\n",
    "        seed=seed,\n",
    "    )\n",
    "\n",
    "def inspect(loader, name):\n",
    "    b = next(iter(loader))\n",
    "    x, y, uid, fe, pe_cont, pe_cat = b\n",
    "    print(f\"[{name}] x:\", x.shape, x.device, x.dtype)\n",
    "    print(f\"[{name}] fe:\", fe.shape, fe.device, fe.dtype)\n",
    "    print(f\"[{name}] pe:\", pe_cont.shape, pe_cont.device, pe_cont.dtype)\n",
    "    print(f\"[{name}] future_exo_cb is None?\", loader.collate_fn.future_exo_cb is None)\n",
    "    if fe.shape[-1] > 0:\n",
    "        print(f\"[{name}] fe sample:\", fe[0, :3, :])\n",
    "\n",
    "# seed_list = [11, 22, 33, 44, 55]\n",
    "seed_list = [55]\n",
    "for seed in seed_list:\n",
    "    set_seed(seed)\n",
    "\n",
    "    save_dir = os.path.join(DIR, \"fit\", \"walmart_patchmixer_ab\")\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    save_root_A0 = os.path.join(save_dir, \"A0_y_only\", f\"seed_{seed}\")\n",
    "    save_root_A1 = os.path.join(save_dir, \"A1_time_exog\", f\"seed_{seed}\")\n",
    "    save_root_A2 = os.path.join(save_dir, \"A2_time_holiday\", f\"seed_{seed}\")\n",
    "\n",
    "    data_module_A0 = build_datamodule(\"A0\", seed)\n",
    "    data_module_A1 = build_datamodule(\"A1\", seed)\n",
    "    data_module_A2 = build_datamodule(\"A2\", seed)\n",
    "\n",
    "    train_loader_A0 = data_module_A0.get_train_loader(batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    val_loader_A0   = data_module_A0.get_val_loader()\n",
    "\n",
    "    train_loader_A1 = data_module_A1.get_train_loader(batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    val_loader_A1   = data_module_A1.get_val_loader()\n",
    "\n",
    "    train_loader_A2 = data_module_A2.get_train_loader(batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    val_loader_A2   = data_module_A2.get_val_loader()\n",
    "\n",
    "    inspect(train_loader_A0, \"A0\")\n",
    "    inspect(train_loader_A1, \"A1\")\n",
    "    inspect(train_loader_A2, \"A2\")\n",
    "\n",
    "    # ============================================\n",
    "    # 학습 실행 (LTB total_train 포맷 유지)\n",
    "    # - 여기서는 \"외생변수 A/B/C\"만 비교하므로 use_ssl_pretrain=False로 고정\n",
    "    # Walmart처럼 항상 판매량이 있는(continuous) 데이터에서 “스파이크”를 잡는 규칙이:\n",
    "    # 스파이크 마스크가 과도하게 넓게 잡히거나(사실상 대부분 True)\n",
    "    # spike-loss가 MSE/제곱오차 기반인데 reduction이 sum 또는 정규화 없이 누적되어\n",
    "    # sales 스케일(1e4~1e5)에서 제곱오차가 1e8급으로 바로 올라가\n",
    "    # → 결과적으로 delta가 1e8 수준으로 튄다.\n",
    "    # → 그래서 최종적으로 이 상황에서는 spike_epoch를 0으로 잡아준다.\n",
    "    # ============================================\n",
    "    print('run result_A0')\n",
    "    results_A0 = run_total_train_weekly(\n",
    "        train_loader_A0,\n",
    "        val_loader_A0,\n",
    "        device=device,\n",
    "        lookback=lookback,\n",
    "        horizon=horizon,\n",
    "        warmup_epochs=30,\n",
    "        spike_epochs=10,\n",
    "        save_dir=save_root_A0,\n",
    "        loss = DistributionLoss(distribution=\"StudentT\", level=[80, 90]),\n",
    "        # loss = HuberLoss(delta = 5.0),\n",
    "        loss_quantile=MQLoss(quantiles=[0.1,0.5,0.99]),  # Quantile 학습용\n",
    "        use_ssl_mode='sl_only',\n",
    "        use_exogenous_mode = False,\n",
    "        models_to_run=[\"patchmixer\"],\n",
    "    )\n",
    "\n",
    "    print('run result_A1')\n",
    "    results_A1 = run_total_train_weekly(\n",
    "        train_loader_A1,\n",
    "        val_loader_A1,\n",
    "        device=device,\n",
    "        lookback=lookback,\n",
    "        horizon=horizon,\n",
    "        warmup_epochs=30,\n",
    "        spike_epochs=10,\n",
    "        save_dir=save_root_A1,\n",
    "        loss = DistributionLoss(distribution=\"StudentT\", level=[80, 90]),\n",
    "        # loss = HuberLoss(delta = 5.0),\n",
    "        loss_quantile=MQLoss(quantiles=[0.1,0.5,0.99]),  # Quantile 학습용\n",
    "        use_ssl_mode='sl_only',\n",
    "        use_exogenous_mode = True,\n",
    "        models_to_run=[\"patchmixer\"],\n",
    "    )\n",
    "\n",
    "    print('run result_A2')\n",
    "    results_A2 = run_total_train_weekly(\n",
    "        train_loader_A2,\n",
    "        val_loader_A2,\n",
    "        device=device,\n",
    "        lookback=lookback,\n",
    "        horizon=horizon,\n",
    "        warmup_epochs=30,\n",
    "        spike_epochs=10,\n",
    "        save_dir=save_root_A2,\n",
    "        loss = DistributionLoss(distribution=\"StudentT\", level=[80, 90]),\n",
    "        # loss = HuberLoss(delta = 5.0),\n",
    "        loss_quantile=MQLoss(quantiles=[0.1,0.5,0.99]),  # Quantile 학습용\n",
    "        use_ssl_mode='sl_only',\n",
    "        use_exogenous_mode = True,\n",
    "        models_to_run=[\"patchmixer\"],\n",
    "    )\n",
    "\n",
    "    builders = {\n",
    "    \"patchtst_quantile\": build_patchTST_quantile,\n",
    "    \"patchtst\": build_patchTST_base,\n",
    "    }\n",
    "    builders = {\n",
    "        'PatchMixerQuantile': build_patch_mixer_quantile,\n",
    "        'PatchMixerBase': build_patch_mixer_base,\n",
    "    }\n",
    "\n",
    "    print(load_model_dict(save_root_A0, builders, device = device))\n",
    "\n",
    "    model_A0 = load_model_dict(save_root_A0, builders, device = device)['patchmixer_base']\n",
    "    model_A1 = load_model_dict(save_root_A1, builders, device = device)['patchmixer_base']\n",
    "    model_A2 = load_model_dict(save_root_A2, builders, device = device)['patchmixer_base']\n",
    "\n",
    "    y0, yhat0 = eval_on_loader(model_A0, val_loader_A0, device=device)\n",
    "    y1, yhat1 = eval_on_loader(model_A1, val_loader_A1, device=device, future_exo_cb = future_exo_cb_time)\n",
    "    y2, yhat2 = eval_on_loader(model_A2, val_loader_A2, device=device, future_exo_cb = future_exo_cb_time_plus_holiday)\n",
    "\n",
    "    metric_A0 = {\n",
    "    \"MAE\": float(mae(y0.reshape(-1), yhat0.reshape(-1))),\n",
    "    \"RMSE\": float(rmse(y0.reshape(-1), yhat0.reshape(-1))),\n",
    "    \"SMAPE\": float(smape(y0.reshape(-1), yhat0.reshape(-1))),\n",
    "    }\n",
    "    metric_A1 = {\n",
    "        \"MAE\": float(mae(y1.reshape(-1), yhat1.reshape(-1))),\n",
    "        \"RMSE\": float(rmse(y1.reshape(-1), yhat1.reshape(-1))),\n",
    "        \"SMAPE\": float(smape(y1.reshape(-1), yhat1.reshape(-1))),\n",
    "    }\n",
    "    metric_A2 = {\n",
    "        \"MAE\": float(mae(y2.reshape(-1), yhat2.reshape(-1))),\n",
    "        \"RMSE\": float(rmse(y2.reshape(-1), yhat2.reshape(-1))),\n",
    "        \"SMAPE\": float(smape(y2.reshape(-1), yhat2.reshape(-1))),\n",
    "    }\n",
    "\n",
    "    model_A0_q = load_model_dict(save_root_A0, builders, device = device)['patchmixer_quantile']\n",
    "    model_A1_q = load_model_dict(save_root_A1, builders, device = device)['patchmixer_quantile']\n",
    "    model_A2_q = load_model_dict(save_root_A2, builders, device = device)['patchmixer_quantile']\n",
    "\n",
    "    y0_q, yhat0_q = eval_on_loader_quantile(model_A0_q, val_loader_A0, device=device)\n",
    "    y1_q, yhat1_q = eval_on_loader_quantile(model_A1_q, val_loader_A1, device=device, future_exo_cb = future_exo_cb_time)\n",
    "    y2_q, yhat2_q = eval_on_loader_quantile(model_A2_q, val_loader_A2, device=device, future_exo_cb = future_exo_cb_time_plus_holiday)\n",
    "\n",
    "    metric_A0_q = {\n",
    "    \"MAE\": float(mae(y0_q.reshape(-1), yhat0_q.reshape(-1))),\n",
    "    \"RMSE\": float(rmse(y0_q.reshape(-1), yhat0_q.reshape(-1))),\n",
    "    \"SMAPE\": float(smape(y0_q.reshape(-1), yhat0_q.reshape(-1))),\n",
    "    }\n",
    "    metric_A1_q = {\n",
    "        \"MAE\": float(mae(y1_q.reshape(-1), yhat1_q.reshape(-1))),\n",
    "        \"RMSE\": float(rmse(y1_q.reshape(-1), yhat1_q.reshape(-1))),\n",
    "        \"SMAPE\": float(smape(y1_q.reshape(-1), yhat1_q.reshape(-1))),\n",
    "    }\n",
    "    metric_A2_q = {\n",
    "        \"MAE\": float(mae(y2_q.reshape(-1), yhat2_q.reshape(-1))),\n",
    "        \"RMSE\": float(rmse(y2_q.reshape(-1), yhat2_q.reshape(-1))),\n",
    "        \"SMAPE\": float(smape(y2_q.reshape(-1), yhat2_q.reshape(-1))),\n",
    "    }\n",
    "\n",
    "    # -------------------------\n",
    "    # Point metrics row append\n",
    "    # -------------------------\n",
    "    rows.append({\n",
    "        \"seed\": seed,\n",
    "        \"variant\": \"A0\",\n",
    "        \"model_type\": \"point\",\n",
    "        \"MAE\": metric_A0[\"MAE\"],\n",
    "        \"RMSE\": metric_A0[\"RMSE\"],\n",
    "        \"SMAPE\": metric_A0[\"SMAPE\"],\n",
    "        \"save_root\": save_root_A0,\n",
    "    })\n",
    "    rows.append({\n",
    "        \"seed\": seed,\n",
    "        \"variant\": \"A1\",\n",
    "        \"model_type\": \"point\",\n",
    "        \"MAE\": metric_A1[\"MAE\"],\n",
    "        \"RMSE\": metric_A1[\"RMSE\"],\n",
    "        \"SMAPE\": metric_A1[\"SMAPE\"],\n",
    "        \"save_root\": save_root_A1,\n",
    "    })\n",
    "    rows.append({\n",
    "        \"seed\": seed,\n",
    "        \"variant\": \"A2\",\n",
    "        \"model_type\": \"point\",\n",
    "        \"MAE\": metric_A2[\"MAE\"],\n",
    "        \"RMSE\": metric_A2[\"RMSE\"],\n",
    "        \"SMAPE\": metric_A2[\"SMAPE\"],\n",
    "        \"save_root\": save_root_A2,\n",
    "    })\n",
    "\n",
    "    # -------------------------\n",
    "    # Quantile metrics row append\n",
    "    # (주의: q50 등 기준이 명확해야 함)\n",
    "    # -------------------------\n",
    "    rows.append({\n",
    "        \"seed\": seed,\n",
    "        \"variant\": \"A0\",\n",
    "        \"model_type\": \"quantile(q50)\",\n",
    "        \"MAE\": metric_A0_q[\"MAE\"],\n",
    "        \"RMSE\": metric_A0_q[\"RMSE\"],\n",
    "        \"SMAPE\": metric_A0_q[\"SMAPE\"],\n",
    "        \"save_root\": save_root_A0,\n",
    "    })\n",
    "    rows.append({\n",
    "        \"seed\": seed,\n",
    "        \"variant\": \"A1\",\n",
    "        \"model_type\": \"quantile(q50)\",\n",
    "        \"MAE\": metric_A1_q[\"MAE\"],\n",
    "        \"RMSE\": metric_A1_q[\"RMSE\"],\n",
    "        \"SMAPE\": metric_A1_q[\"SMAPE\"],\n",
    "        \"save_root\": save_root_A1,\n",
    "    })\n",
    "    rows.append({\n",
    "        \"seed\": seed,\n",
    "        \"variant\": \"A2\",\n",
    "        \"model_type\": \"quantile(q50)\",\n",
    "        \"MAE\": metric_A2_q[\"MAE\"],\n",
    "        \"RMSE\": metric_A2_q[\"RMSE\"],\n",
    "        \"SMAPE\": metric_A2_q[\"SMAPE\"],\n",
    "        \"save_root\": save_root_A2,\n",
    "    })\n",
    "\n",
    "# loop 종료 후 저장\n",
    "df_out = pd.DataFrame(rows)\n",
    "\n",
    "out_csv = os.path.join(save_dir, \"ab_results_by_seed.csv\")\n",
    "df_out.to_csv(out_csv, index=False)\n",
    "\n",
    "# variant별 mean/std 요약도 같이 저장 추천\n",
    "summary = (\n",
    "    df_out.groupby([\"variant\", \"model_type\"])[[\"MAE\", \"RMSE\", \"SMAPE\"]]\n",
    "    .agg([\"mean\", \"std\"])\n",
    "    .reset_index()\n",
    ")\n",
    "out_sum = os.path.join(save_dir, \"ab_results_summary.csv\")\n",
    "summary.to_csv(out_sum, index=False)\n",
    "\n",
    "print(\"saved:\", out_csv, out_sum)\n"
   ],
   "id": "7449dd2dee6aed3c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[A0] x: torch.Size([256, 52, 1]) cpu torch.float32\n",
      "[A0] fe: torch.Size([256, 27, 0]) cpu torch.float32\n",
      "[A0] pe: torch.Size([256, 52, 1]) cpu torch.float32\n",
      "[A0] future_exo_cb is None? True\n",
      "[A1] x: torch.Size([256, 52, 1]) cpu torch.float32\n",
      "[A1] fe: torch.Size([256, 27, 2]) cpu torch.float32\n",
      "[A1] pe: torch.Size([256, 52, 1]) cpu torch.float32\n",
      "[A1] future_exo_cb is None? False\n",
      "[A1] fe sample: tensor([[-0.7468,  0.6651],\n",
      "        [-0.6624,  0.7491],\n",
      "        [-0.5671,  0.8237]])\n",
      "[A2] x: torch.Size([256, 52, 1]) cpu torch.float32\n",
      "[A2] fe: torch.Size([256, 27, 3]) cpu torch.float32\n",
      "[A2] pe: torch.Size([256, 52, 1]) cpu torch.float32\n",
      "[A2] future_exo_cb is None? False\n",
      "[A2] fe sample: tensor([[0.8859, 0.4639, 0.0000],\n",
      "        [0.9355, 0.3534, 0.0000],\n",
      "        [0.9708, 0.2398, 0.0000]])\n",
      "run result_A0\n",
      "[total_train] use_exogenous_mode: False has_fe: True, fe_dim: 0\n",
      "\n",
      "[total_train] === RUN: patchmixer (weekly) ===\n",
      "loss: DistributionLoss() param_names: ['-df', '-loc', '-scale'] out_mult: 3\n",
      "PatchMixer Base (Weekly)\n",
      "[EXO-setup] (loader) future_exo_cb=None → skip exo_head setup. model.exo_dim=0, has_head=False\n",
      "\n",
      "[train_patchmixer] ===== Stage 1/2 =====\n",
      "  - spike: OFF\n",
      "  - epochs: 30 | lr=0.0001 | horizon_decay=False\n",
      "[train_patchmixer] Effective TrainingConfig:\n",
      "{\n",
      "  \"device\": \"cuda\",\n",
      "  \"log_every\": 100,\n",
      "  \"use_amp\": true,\n",
      "  \"lookback\": 52,\n",
      "  \"horizon\": 27,\n",
      "  \"epochs\": 30,\n",
      "  \"lr\": 0.0001,\n",
      "  \"weight_decay\": 0.001,\n",
      "  \"t_max\": 40,\n",
      "  \"patience\": 100,\n",
      "  \"max_grad_norm\": 30.0,\n",
      "  \"amp_device\": \"cuda\",\n",
      "  \"loss\": \"DistributionLoss()\",\n",
      "  \"loss_mode\": \"auto\",\n",
      "  \"point_loss\": \"mse\",\n",
      "  \"huber_delta\": 0.8,\n",
      "  \"q_star\": 0.5,\n",
      "  \"use_cost_q_star\": false,\n",
      "  \"Cu\": 2.0,\n",
      "  \"Co\": 1.0,\n",
      "  \"quantiles\": [\n",
      "    0.1,\n",
      "    0.5,\n",
      "    0.9\n",
      "  ],\n",
      "  \"dist_name\": \"normal\",\n",
      "  \"dist_scale_transform\": \"softplus\",\n",
      "  \"dist_min_scale\": 1000,\n",
      "  \"dist_use_weights\": true,\n",
      "  \"dist_family\": \"normal\",\n",
      "  \"dist_eps\": 1e-08,\n",
      "  \"dist_scale_is_positive\": true,\n",
      "  \"use_intermittent\": true,\n",
      "  \"alpha_zero\": 0.3,\n",
      "  \"alpha_pos\": 1.0,\n",
      "  \"gamma_run\": 0.5,\n",
      "  \"cap\": null,\n",
      "  \"use_horizon_decay\": false,\n",
      "  \"tau_h\": 1.0,\n",
      "  \"val_use_weights\": true,\n",
      "  \"use_exogenous_mode\": false,\n",
      "  \"spike_loss\": {\n",
      "    \"enabled\": false,\n",
      "    \"strategy\": \"mix\",\n",
      "    \"huber_delta\": 0.8,\n",
      "    \"asym_up_weight\": 2.0,\n",
      "    \"asym_down_weight\": 1.0,\n",
      "    \"mad_k\": 3.5,\n",
      "    \"w_spike\": 6.0,\n",
      "    \"w_norm\": 1.0,\n",
      "    \"alpha_huber\": 1.0,\n",
      "    \"beta_asym\": 1.0,\n",
      "    \"mix_with_baseline\": false,\n",
      "    \"gamma_baseline\": 0.1\n",
      "  },\n",
      "  \"lambda_hist_scale\": 0.1,\n",
      "  \"lambda_hist_var\": 0.03,\n",
      "  \"hist_window\": 12,\n",
      "  \"anchor_last_k\": 8,\n",
      "  \"anchor_weight\": 0.05\n",
      "}\n",
      "[CommonTrainer] TrainingConfig (final)\n",
      "{\n",
      "  \"device\": \"cuda\",\n",
      "  \"log_every\": 100,\n",
      "  \"use_amp\": true,\n",
      "  \"lookback\": 52,\n",
      "  \"horizon\": 27,\n",
      "  \"epochs\": 30,\n",
      "  \"lr\": 0.0001,\n",
      "  \"weight_decay\": 0.001,\n",
      "  \"t_max\": 40,\n",
      "  \"patience\": 100,\n",
      "  \"max_grad_norm\": 30.0,\n",
      "  \"amp_device\": \"cuda\",\n",
      "  \"loss\": \"DistributionLoss()\",\n",
      "  \"loss_mode\": \"auto\",\n",
      "  \"point_loss\": \"mse\",\n",
      "  \"huber_delta\": 0.8,\n",
      "  \"q_star\": 0.5,\n",
      "  \"use_cost_q_star\": false,\n",
      "  \"Cu\": 2.0,\n",
      "  \"Co\": 1.0,\n",
      "  \"quantiles\": [\n",
      "    0.1,\n",
      "    0.5,\n",
      "    0.9\n",
      "  ],\n",
      "  \"dist_name\": \"normal\",\n",
      "  \"dist_scale_transform\": \"softplus\",\n",
      "  \"dist_min_scale\": 1000,\n",
      "  \"dist_use_weights\": true,\n",
      "  \"dist_family\": \"normal\",\n",
      "  \"dist_eps\": 1e-08,\n",
      "  \"dist_scale_is_positive\": true,\n",
      "  \"use_intermittent\": true,\n",
      "  \"alpha_zero\": 0.3,\n",
      "  \"alpha_pos\": 1.0,\n",
      "  \"gamma_run\": 0.5,\n",
      "  \"cap\": null,\n",
      "  \"use_horizon_decay\": false,\n",
      "  \"tau_h\": 1.0,\n",
      "  \"val_use_weights\": true,\n",
      "  \"use_exogenous_mode\": false,\n",
      "  \"spike_loss\": {\n",
      "    \"enabled\": false,\n",
      "    \"strategy\": \"mix\",\n",
      "    \"huber_delta\": 0.8,\n",
      "    \"asym_up_weight\": 2.0,\n",
      "    \"asym_down_weight\": 1.0,\n",
      "    \"mad_k\": 3.5,\n",
      "    \"w_spike\": 6.0,\n",
      "    \"w_norm\": 1.0,\n",
      "    \"alpha_huber\": 1.0,\n",
      "    \"beta_asym\": 1.0,\n",
      "    \"mix_with_baseline\": false,\n",
      "    \"gamma_baseline\": 0.1\n",
      "  },\n",
      "  \"lambda_hist_scale\": 0.1,\n",
      "  \"lambda_hist_var\": 0.03,\n",
      "  \"hist_window\": 12,\n",
      "  \"anchor_last_k\": 8,\n",
      "  \"anchor_weight\": 0.05\n",
      "}\n",
      "Epoch 1/30 | LR 0.000100 | Train 5587503.333333 | Val 6350974.833333\n",
      "Epoch 2/30 | LR 0.000099 | Train 5258734.611111 | Val 5997576.166667\n",
      "Epoch 3/30 | LR 0.000099 | Train 5004291.111111 | Val 5742257.333333\n",
      "Epoch 4/30 | LR 0.000098 | Train 4812392.666667 | Val 5515555.666667\n",
      "Epoch 5/30 | LR 0.000096 | Train 4646691.944444 | Val 5302305.833333\n",
      "Epoch 6/30 | LR 0.000095 | Train 4481079.722222 | Val 5100960.000000\n",
      "Epoch 7/30 | LR 0.000093 | Train 4306545.638889 | Val 4895228.916667\n",
      "Epoch 8/30 | LR 0.000090 | Train 4144880.416667 | Val 4707379.166667\n",
      "Epoch 9/30 | LR 0.000088 | Train 3977835.694444 | Val 4516263.250000\n",
      "Epoch 10/30 | LR 0.000085 | Train 3833001.500000 | Val 4338182.500000\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected parameter df (Tensor of shape (256, 27, 1)) of distribution Chi2() to satisfy the constraint GreaterThan(lower_bound=0.0), but found invalid values:\ntensor([[[ 643072.],\n         [1003520.],\n         [1011712.],\n         ...,\n         [1003520.],\n         [ 999424.],\n         [ 839680.]],\n\n        [[ 163840.],\n         [ 430080.],\n         [ 454656.],\n         ...,\n         [ 421888.],\n         [ 434176.],\n         [ 307200.]],\n\n        [[ 143360.],\n         [ 192512.],\n         [ 205824.],\n         ...,\n         [ 192512.],\n         [ 197632.],\n         [ 169984.]],\n\n        ...,\n\n        [[ 643072.],\n         [1044480.],\n         [ 995328.],\n         ...,\n         [1019904.],\n         [1032192.],\n         [ 802816.]],\n\n        [[ 223232.],\n         [ 246784.],\n         [ 250880.],\n         ...,\n         [ 248832.],\n         [ 249856.],\n         [ 237568.]],\n\n        [[ 847872.],\n         [1171456.],\n         [1146880.],\n         ...,\n         [1163264.],\n         [1163264.],\n         [1036288.]]], device='cuda:0', dtype=torch.bfloat16,\n       grad_fn=<MulBackward0>)",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mValueError\u001B[39m                                Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[5]\u001B[39m\u001B[32m, line 98\u001B[39m\n\u001B[32m     87\u001B[39m \u001B[38;5;66;03m# ============================================\u001B[39;00m\n\u001B[32m     88\u001B[39m \u001B[38;5;66;03m# 학습 실행 (LTB total_train 포맷 유지)\u001B[39;00m\n\u001B[32m     89\u001B[39m \u001B[38;5;66;03m# - 여기서는 \"외생변수 A/B/C\"만 비교하므로 use_ssl_pretrain=False로 고정\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m     95\u001B[39m \u001B[38;5;66;03m# → 그래서 최종적으로 이 상황에서는 spike_epoch를 0으로 잡아준다.\u001B[39;00m\n\u001B[32m     96\u001B[39m \u001B[38;5;66;03m# ============================================\u001B[39;00m\n\u001B[32m     97\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m'\u001B[39m\u001B[33mrun result_A0\u001B[39m\u001B[33m'\u001B[39m)\n\u001B[32m---> \u001B[39m\u001B[32m98\u001B[39m results_A0 = \u001B[43mrun_total_train_weekly\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m     99\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtrain_loader_A0\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    100\u001B[39m \u001B[43m    \u001B[49m\u001B[43mval_loader_A0\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    101\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    102\u001B[39m \u001B[43m    \u001B[49m\u001B[43mlookback\u001B[49m\u001B[43m=\u001B[49m\u001B[43mlookback\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    103\u001B[39m \u001B[43m    \u001B[49m\u001B[43mhorizon\u001B[49m\u001B[43m=\u001B[49m\u001B[43mhorizon\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    104\u001B[39m \u001B[43m    \u001B[49m\u001B[43mwarmup_epochs\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m30\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m    105\u001B[39m \u001B[43m    \u001B[49m\u001B[43mspike_epochs\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m10\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m    106\u001B[39m \u001B[43m    \u001B[49m\u001B[43msave_dir\u001B[49m\u001B[43m=\u001B[49m\u001B[43msave_root_A0\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    107\u001B[39m \u001B[43m    \u001B[49m\u001B[43mloss\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[43mDistributionLoss\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdistribution\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mStudentT\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlevel\u001B[49m\u001B[43m=\u001B[49m\u001B[43m[\u001B[49m\u001B[32;43m80\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m90\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    108\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m# loss = HuberLoss(delta = 5.0),\u001B[39;49;00m\n\u001B[32m    109\u001B[39m \u001B[43m    \u001B[49m\u001B[43mloss_quantile\u001B[49m\u001B[43m=\u001B[49m\u001B[43mMQLoss\u001B[49m\u001B[43m(\u001B[49m\u001B[43mquantiles\u001B[49m\u001B[43m=\u001B[49m\u001B[43m[\u001B[49m\u001B[32;43m0.1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[32;43m0.5\u001B[39;49m\u001B[43m,\u001B[49m\u001B[32;43m0.99\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Quantile 학습용\u001B[39;49;00m\n\u001B[32m    110\u001B[39m \u001B[43m    \u001B[49m\u001B[43muse_ssl_mode\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43msl_only\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m    111\u001B[39m \u001B[43m    \u001B[49m\u001B[43muse_exogenous_mode\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    112\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmodels_to_run\u001B[49m\u001B[43m=\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mpatchmixer\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    113\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    115\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m'\u001B[39m\u001B[33mrun result_A1\u001B[39m\u001B[33m'\u001B[39m)\n\u001B[32m    116\u001B[39m results_A1 = run_total_train_weekly(\n\u001B[32m    117\u001B[39m     train_loader_A1,\n\u001B[32m    118\u001B[39m     val_loader_A1,\n\u001B[32m   (...)\u001B[39m\u001B[32m    130\u001B[39m     models_to_run=[\u001B[33m\"\u001B[39m\u001B[33mpatchmixer\u001B[39m\u001B[33m\"\u001B[39m],\n\u001B[32m    131\u001B[39m )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\ts_forecaster_lib\\src\\modeling_module\\training\\model_trainers\\total_train.py:1140\u001B[39m, in \u001B[36mrun_total_train_weekly\u001B[39m\u001B[34m(train_loader, val_loader, device, lookback, horizon, warmup_epochs, spike_epochs, base_lr, save_dir, use_exogenous_mode, models_to_run, loss_point, loss_quantile, loss, use_ssl_mode, ssl_pretrain_epochs, ssl_mask_ratio, ssl_loss_type, ssl_freeze_encoder_before_ft, ssl_pretrained_ckpt_path)\u001B[39m\n\u001B[32m   1115\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mrun_total_train_weekly\u001B[39m(\n\u001B[32m   1116\u001B[39m     train_loader,\n\u001B[32m   1117\u001B[39m     val_loader,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1138\u001B[39m     ssl_pretrained_ckpt_path: Optional[\u001B[38;5;28mstr\u001B[39m] = \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[32m   1139\u001B[39m ):\n\u001B[32m-> \u001B[39m\u001B[32m1140\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_run_total_train_generic\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1141\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1142\u001B[39m \u001B[43m        \u001B[49m\u001B[43mval_loader\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1143\u001B[39m \u001B[43m        \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1144\u001B[39m \u001B[43m        \u001B[49m\u001B[43mlookback\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1145\u001B[39m \u001B[43m        \u001B[49m\u001B[43mhorizon\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1146\u001B[39m \u001B[43m        \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mweekly\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m   1147\u001B[39m \u001B[43m        \u001B[49m\u001B[43msave_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1148\u001B[39m \u001B[43m        \u001B[49m\u001B[43muse_exogenous_mode\u001B[49m\u001B[43m=\u001B[49m\u001B[43muse_exogenous_mode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1149\u001B[39m \u001B[43m        \u001B[49m\u001B[43mwarmup_epochs\u001B[49m\u001B[43m=\u001B[49m\u001B[43mwarmup_epochs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1150\u001B[39m \u001B[43m        \u001B[49m\u001B[43mspike_epochs\u001B[49m\u001B[43m=\u001B[49m\u001B[43mspike_epochs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1151\u001B[39m \u001B[43m        \u001B[49m\u001B[43mbase_lr\u001B[49m\u001B[43m=\u001B[49m\u001B[43mbase_lr\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1152\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmodels_to_run\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmodels_to_run\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1153\u001B[39m \u001B[43m        \u001B[49m\u001B[43mloss_point\u001B[49m\u001B[43m=\u001B[49m\u001B[43mloss_point\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1154\u001B[39m \u001B[43m        \u001B[49m\u001B[43mloss_quantile\u001B[49m\u001B[43m=\u001B[49m\u001B[43mloss_quantile\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1155\u001B[39m \u001B[43m        \u001B[49m\u001B[43mloss\u001B[49m\u001B[43m=\u001B[49m\u001B[43mloss\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1156\u001B[39m \u001B[43m        \u001B[49m\u001B[43muse_ssl_mode\u001B[49m\u001B[43m=\u001B[49m\u001B[43muse_ssl_mode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1157\u001B[39m \u001B[43m        \u001B[49m\u001B[43mssl_pretrain_epochs\u001B[49m\u001B[43m=\u001B[49m\u001B[43mssl_pretrain_epochs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1158\u001B[39m \u001B[43m        \u001B[49m\u001B[43mssl_mask_ratio\u001B[49m\u001B[43m=\u001B[49m\u001B[43mssl_mask_ratio\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1159\u001B[39m \u001B[43m        \u001B[49m\u001B[43mssl_loss_type\u001B[49m\u001B[43m=\u001B[49m\u001B[43mssl_loss_type\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1160\u001B[39m \u001B[43m        \u001B[49m\u001B[43mssl_freeze_encoder_before_ft\u001B[49m\u001B[43m=\u001B[49m\u001B[43mssl_freeze_encoder_before_ft\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1161\u001B[39m \u001B[43m        \u001B[49m\u001B[43mssl_pretrained_ckpt_path\u001B[49m\u001B[43m=\u001B[49m\u001B[43mssl_pretrained_ckpt_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1162\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\ts_forecaster_lib\\src\\modeling_module\\training\\model_trainers\\total_train.py:1106\u001B[39m, in \u001B[36m_run_total_train_generic\u001B[39m\u001B[34m(train_loader, val_loader, device, lookback, horizon, freq, save_dir, use_exogenous_mode, models_to_run, warmup_epochs, spike_epochs, base_lr, loss_point, loss_quantile, loss, use_ssl_mode, ssl_pretrain_epochs, ssl_mask_ratio, ssl_loss_type, ssl_freeze_encoder_before_ft, ssl_pretrained_ckpt_path)\u001B[39m\n\u001B[32m   1103\u001B[39m         kwargs.pop(\u001B[33m\"\u001B[39m\u001B[33mquantile_train_cfg\u001B[39m\u001B[33m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[32m   1104\u001B[39m         kwargs.pop(\u001B[33m\"\u001B[39m\u001B[33mloss_quantile\u001B[39m\u001B[33m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[32m-> \u001B[39m\u001B[32m1106\u001B[39m     \u001B[43mMODEL_REGISTRY\u001B[49m\u001B[43m[\u001B[49m\u001B[43mm\u001B[49m\u001B[43m]\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1108\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m results\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\ts_forecaster_lib\\src\\modeling_module\\training\\model_trainers\\total_train.py:911\u001B[39m, in \u001B[36m_run_patchmixer\u001B[39m\u001B[34m(results, freq, train_loader, val_loader, save_root, lookback, horizon, future_exo_cb, exo_dim, patch_len, stride, loss_point, loss_quantile, use_exogenous_mode, point_train_cfg, quantile_train_cfg, stages, device)\u001B[39m\n\u001B[32m    907\u001B[39m     name_base_model = \u001B[33m\"\u001B[39m\u001B[33mPatchMixer Base\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    910\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mPatchMixer Base (\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfreq.capitalize()\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m)\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m--> \u001B[39m\u001B[32m911\u001B[39m best_pm_base = \u001B[43mtrain_patchmixer\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    912\u001B[39m \u001B[43m    \u001B[49m\u001B[43mpm_base_model\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    913\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    914\u001B[39m \u001B[43m    \u001B[49m\u001B[43mval_loader\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    915\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtrain_cfg\u001B[49m\u001B[43m=\u001B[49m\u001B[43mpoint_train_cfg\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    916\u001B[39m \u001B[43m    \u001B[49m\u001B[43mstages\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mlist\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mstages\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    917\u001B[39m \u001B[43m    \u001B[49m\u001B[43mfuture_exo_cb\u001B[49m\u001B[43m=\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfuture_exo_cb\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43muse_exogenous_mode\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    918\u001B[39m \u001B[43m    \u001B[49m\u001B[43mexo_is_normalized\u001B[49m\u001B[43m=\u001B[49m\u001B[43mpm_base_cfg\u001B[49m\u001B[43m.\u001B[49m\u001B[43mexo_is_normalized_default\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    919\u001B[39m \u001B[43m    \u001B[49m\u001B[43muse_exogenous_mode\u001B[49m\u001B[43m=\u001B[49m\u001B[43muse_exogenous_mode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    920\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    921\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m save_root:\n\u001B[32m    922\u001B[39m     ckpt_path = _make_ckpt_path(save_root, freq, name_base_model.replace(\u001B[33m\"\u001B[39m\u001B[33m \u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33m\"\u001B[39m), lookback, horizon)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\ts_forecaster_lib\\src\\modeling_module\\training\\model_trainers\\patchmixer_train.py:219\u001B[39m, in \u001B[36mtrain_patchmixer\u001B[39m\u001B[34m(model, train_loader, val_loader, stages, train_cfg, future_exo_cb, exo_is_normalized, use_exogenous_mode)\u001B[39m\n\u001B[32m    207\u001B[39m     \u001B[38;5;66;03m# 트레이너 초기화 및 학습 수행\u001B[39;00m\n\u001B[32m    208\u001B[39m     trainer = CommonTrainer(\n\u001B[32m    209\u001B[39m         cfg=cfg_i,\n\u001B[32m    210\u001B[39m         adapter=adapter,\n\u001B[32m   (...)\u001B[39m\u001B[32m    217\u001B[39m \n\u001B[32m    218\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m219\u001B[39m     model = \u001B[43mtrainer\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtl_i\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtta_steps\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m0\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m    220\u001B[39m     best = {\u001B[33m\"\u001B[39m\u001B[33mmodel\u001B[39m\u001B[33m\"\u001B[39m: model, \u001B[33m\"\u001B[39m\u001B[33mcfg\u001B[39m\u001B[33m\"\u001B[39m: cfg_i}\n\u001B[32m    222\u001B[39m \u001B[38;5;66;03m# 학습 완료 상태 로그\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\ts_forecaster_lib\\src\\modeling_module\\training\\engine.py:421\u001B[39m, in \u001B[36mCommonTrainer.fit\u001B[39m\u001B[34m(self, model, train_loader, val_loader, tta_steps)\u001B[39m\n\u001B[32m    417\u001B[39m     \u001B[38;5;28mself\u001B[39m.adapter.tta_reset(model)\n\u001B[32m    419\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mself\u001B[39m.cfg.epochs):\n\u001B[32m    420\u001B[39m     \u001B[38;5;66;03m# 1. 학습 루프 실행\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m421\u001B[39m     train_loss = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_run_epoch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[32m    423\u001B[39m     \u001B[38;5;66;03m# 2. 검증 루프 진입\u001B[39;00m\n\u001B[32m    424\u001B[39m     model.eval()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\ts_forecaster_lib\\src\\modeling_module\\training\\engine.py:349\u001B[39m, in \u001B[36mCommonTrainer._run_epoch\u001B[39m\u001B[34m(self, model, loader, train)\u001B[39m\n\u001B[32m    346\u001B[39m \u001B[38;5;28mself\u001B[39m._nan_stat(\u001B[33m\"\u001B[39m\u001B[33mpred\u001B[39m\u001B[33m\"\u001B[39m, pred)\n\u001B[32m    348\u001B[39m \u001B[38;5;66;03m# 손실 함수 계산 (Validation 여부 반영)\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m349\u001B[39m loss = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mloss_comp\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcompute\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpred\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mis_val\u001B[49m\u001B[43m=\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    351\u001B[39m \u001B[38;5;66;03m# 디버깅: Spike Loss 상세 분석 (초기 배치 한정)\u001B[39;00m\n\u001B[32m    352\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._get_spike_enabled():\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\ts_forecaster_lib\\src\\modeling_module\\training\\model_losses\\losses.py:224\u001B[39m, in \u001B[36mLossComputer.compute\u001B[39m\u001B[34m(self, y_hat, y, is_val, y_insample)\u001B[39m\n\u001B[32m    221\u001B[39m             \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mUnsupported outputsize_multiplier=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mm\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m for DistributionLoss\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m    223\u001B[39m     \u001B[38;5;66;03m# 3) DistributionLoss는 __call__(y, distr_args, mask) 시그니처\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m224\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mloss_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43my\u001B[49m\u001B[43m=\u001B[49m\u001B[43my3\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdistr_args\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdistr_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmask\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    226\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01min\u001B[39;00m (\u001B[33m\"\u001B[39m\u001B[33mMQLoss\u001B[39m\u001B[33m\"\u001B[39m,):\n\u001B[32m    227\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m torch.is_tensor(y_hat):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\ts_forecaster_lib\\src\\modeling_module\\training\\model_losses\\loss_module.py:2593\u001B[39m, in \u001B[36mDistributionLoss.__call__\u001B[39m\u001B[34m(self, y, distr_args, mask)\u001B[39m\n\u001B[32m   2569\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33mr\u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m   2570\u001B[39m \u001B[33;03mComputes the negative log-likelihood objective function.\u001B[39;00m\n\u001B[32m   2571\u001B[39m \u001B[33;03mTo estimate the following predictive distribution:\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m   2590\u001B[39m \u001B[33;03m    float: Weighted loss function against which backpropagation will be performed.\u001B[39;00m\n\u001B[32m   2591\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m   2592\u001B[39m \u001B[38;5;66;03m# Instantiate Scaled Decoupled Distribution\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m2593\u001B[39m distr = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mget_distribution\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdistr_args\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdistr_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mdistribution_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   2594\u001B[39m loss_values = -distr.log_prob(y)\n\u001B[32m   2595\u001B[39m loss_weights = \u001B[38;5;28mself\u001B[39m._compute_weights(y=y, mask=mask)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\ts_forecaster_lib\\src\\modeling_module\\training\\model_losses\\loss_module.py:2487\u001B[39m, in \u001B[36mDistributionLoss.get_distribution\u001B[39m\u001B[34m(self, distr_args, **distribution_kwargs)\u001B[39m\n\u001B[32m   2476\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mget_distribution\u001B[39m(\u001B[38;5;28mself\u001B[39m, distr_args, **distribution_kwargs) -> Distribution:\n\u001B[32m   2477\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m   2478\u001B[39m \u001B[33;03m    Construct the associated Pytorch Distribution, given the collection of\u001B[39;00m\n\u001B[32m   2479\u001B[39m \u001B[33;03m    constructor arguments and, optionally, location and scale tensors.\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m   2485\u001B[39m \u001B[33;03m        Distribution: AffineTransformed distribution.\u001B[39;00m\n\u001B[32m   2486\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m2487\u001B[39m     distr = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_base_distribution\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43mdistr_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mdistribution_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   2488\u001B[39m     \u001B[38;5;28mself\u001B[39m.distr_mean = distr.mean\n\u001B[32m   2490\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.distribution \u001B[38;5;129;01min\u001B[39;00m (\u001B[33m\"\u001B[39m\u001B[33mPoisson\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mNegativeBinomial\u001B[39m\u001B[33m\"\u001B[39m):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniconda3\\envs\\tsf-win312\\Lib\\site-packages\\torch\\distributions\\studentT.py:72\u001B[39m, in \u001B[36mStudentT.__init__\u001B[39m\u001B[34m(self, df, loc, scale, validate_args)\u001B[39m\n\u001B[32m     64\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m__init__\u001B[39m(\n\u001B[32m     65\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m     66\u001B[39m     df: Tensor | \u001B[38;5;28mfloat\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m     69\u001B[39m     validate_args: \u001B[38;5;28mbool\u001B[39m | \u001B[38;5;28;01mNone\u001B[39;00m = \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[32m     70\u001B[39m ) -> \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m     71\u001B[39m     \u001B[38;5;28mself\u001B[39m.df, \u001B[38;5;28mself\u001B[39m.loc, \u001B[38;5;28mself\u001B[39m.scale = broadcast_all(df, loc, scale)\n\u001B[32m---> \u001B[39m\u001B[32m72\u001B[39m     \u001B[38;5;28mself\u001B[39m._chi2 = \u001B[43mChi2\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mdf\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     73\u001B[39m     batch_shape = \u001B[38;5;28mself\u001B[39m.df.size()\n\u001B[32m     74\u001B[39m     \u001B[38;5;28msuper\u001B[39m().\u001B[34m__init__\u001B[39m(batch_shape, validate_args=validate_args)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniconda3\\envs\\tsf-win312\\Lib\\site-packages\\torch\\distributions\\chi2.py:34\u001B[39m, in \u001B[36mChi2.__init__\u001B[39m\u001B[34m(self, df, validate_args)\u001B[39m\n\u001B[32m     29\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m__init__\u001B[39m(\n\u001B[32m     30\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m     31\u001B[39m     df: Tensor | \u001B[38;5;28mfloat\u001B[39m,\n\u001B[32m     32\u001B[39m     validate_args: \u001B[38;5;28mbool\u001B[39m | \u001B[38;5;28;01mNone\u001B[39;00m = \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[32m     33\u001B[39m ) -> \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m---> \u001B[39m\u001B[32m34\u001B[39m     \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[34;43m__init__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[32;43m0.5\u001B[39;49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m \u001B[49m\u001B[43mdf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m0.5\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalidate_args\u001B[49m\u001B[43m=\u001B[49m\u001B[43mvalidate_args\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniconda3\\envs\\tsf-win312\\Lib\\site-packages\\torch\\distributions\\gamma.py:68\u001B[39m, in \u001B[36mGamma.__init__\u001B[39m\u001B[34m(self, concentration, rate, validate_args)\u001B[39m\n\u001B[32m     66\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m     67\u001B[39m     batch_shape = \u001B[38;5;28mself\u001B[39m.concentration.size()\n\u001B[32m---> \u001B[39m\u001B[32m68\u001B[39m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[34;43m__init__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mbatch_shape\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalidate_args\u001B[49m\u001B[43m=\u001B[49m\u001B[43mvalidate_args\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniconda3\\envs\\tsf-win312\\Lib\\site-packages\\torch\\distributions\\distribution.py:77\u001B[39m, in \u001B[36mDistribution.__init__\u001B[39m\u001B[34m(self, batch_shape, event_shape, validate_args)\u001B[39m\n\u001B[32m     75\u001B[39m         valid = constraint.check(value)\n\u001B[32m     76\u001B[39m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m torch._is_all_true(valid):\n\u001B[32m---> \u001B[39m\u001B[32m77\u001B[39m             \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[32m     78\u001B[39m                 \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mExpected parameter \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mparam\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m     79\u001B[39m                 \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m(\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(value).\u001B[34m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m of shape \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtuple\u001B[39m(value.shape)\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m) \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m     80\u001B[39m                 \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mof distribution \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mrepr\u001B[39m(\u001B[38;5;28mself\u001B[39m)\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m     81\u001B[39m                 \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mto satisfy the constraint \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mrepr\u001B[39m(constraint)\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m, \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m     82\u001B[39m                 \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mbut found invalid values:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mvalue\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m\n\u001B[32m     83\u001B[39m             )\n\u001B[32m     84\u001B[39m \u001B[38;5;28msuper\u001B[39m().\u001B[34m__init__\u001B[39m()\n",
      "\u001B[31mValueError\u001B[39m: Expected parameter df (Tensor of shape (256, 27, 1)) of distribution Chi2() to satisfy the constraint GreaterThan(lower_bound=0.0), but found invalid values:\ntensor([[[ 643072.],\n         [1003520.],\n         [1011712.],\n         ...,\n         [1003520.],\n         [ 999424.],\n         [ 839680.]],\n\n        [[ 163840.],\n         [ 430080.],\n         [ 454656.],\n         ...,\n         [ 421888.],\n         [ 434176.],\n         [ 307200.]],\n\n        [[ 143360.],\n         [ 192512.],\n         [ 205824.],\n         ...,\n         [ 192512.],\n         [ 197632.],\n         [ 169984.]],\n\n        ...,\n\n        [[ 643072.],\n         [1044480.],\n         [ 995328.],\n         ...,\n         [1019904.],\n         [1032192.],\n         [ 802816.]],\n\n        [[ 223232.],\n         [ 246784.],\n         [ 250880.],\n         ...,\n         [ 248832.],\n         [ 249856.],\n         [ 237568.]],\n\n        [[ 847872.],\n         [1171456.],\n         [1146880.],\n         ...,\n         [1163264.],\n         [1163264.],\n         [1036288.]]], device='cuda:0', dtype=torch.bfloat16,\n       grad_fn=<MulBackward0>)"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ============================================\n",
    "# CELL 6) 체크포인트 로드 + 평가 (첨부 노트북 흐름 유지)\n",
    "# - quantile 모델이면 eval_on_loader_quantile 사용\n",
    "# - point 모델이면 eval_on_loader 사용\n",
    "# ============================================\n",
    "\n",
    "# builders는 \"당신이 실제 저장한 모델 키\"에 맞추세요.\n",
    "# (첨부 노트북에서는 patchtst_quantile을 사용)\n",
    "# builders = {\n",
    "#     \"patchtst_quantile\": build_patchTST_quantile,\n",
    "#     \"patchtst\": build_patchTST_base,\n",
    "# }\n",
    "\n",
    "# model_A0 = load_model_dict(save_root_A0, builders, device=device)[\"patchmixer_base\"]\n",
    "# model_A1 = load_model_dict(save_root_A1, builders, device=device)[\"patchmixer_base\"]\n",
    "# model_A2 = load_model_dict(save_root_A2, builders, device=device)[\"patchmixer_base\"]\n",
    "model_A0 = load_model_dict(save_root_A0, builders, device=device)[\"patchmixer_quantile\"]\n",
    "model_A1 = load_model_dict(save_root_A1, builders, device=device)[\"patchmixer_quantile\"]\n",
    "model_A2 = load_model_dict(save_root_A2, builders, device=device)[\"patchmixer_quantile\"]\n",
    "\n",
    "\n",
    "\n",
    "# 첨부 노트북에서 사용하던 eval 유틸을 그대로 쓴다고 가정합니다.\n",
    "# (이미 앞 셀에 정의돼 있거나, 별도 모듈에 있으면 import 하세요.)\n",
    "# from your_notebook_utils import eval_on_loader_quantile\n",
    "\n",
    "y0, yhat0 = eval_on_loader_quantile(model_A0, val_loader_A0, device, prefer_q=0.5)\n",
    "y1, yhat1 = eval_on_loader_quantile(model_A1, val_loader_A1, device, prefer_q=0.5, future_exo_cb = future_exo_cb_time)\n",
    "y2, yhat2 = eval_on_loader_quantile(model_A2, val_loader_A2, device, prefer_q=0.5, future_exo_cb = future_exo_cb_time_plus_holiday)\n",
    "\n",
    "# y0, yhat0 = eval_on_loader(model_A0, val_loader_A0, device=device)\n",
    "# y1, yhat1 = eval_on_loader(model_A1, val_loader_A1, device=device, future_exo_cb = future_exo_cb_time)\n",
    "# y2, yhat2 = eval_on_loader(model_A2, val_loader_A2, device=device, future_exo_cb = future_exo_cb_time_plus_holiday)\n",
    "\n",
    "metric_A0 = {\n",
    "    \"MAE\": float(mae(y0.reshape(-1), yhat0.reshape(-1))),\n",
    "    \"RMSE\": float(rmse(y0.reshape(-1), yhat0.reshape(-1))),\n",
    "    \"SMAPE\": float(smape(y0.reshape(-1), yhat0.reshape(-1))),\n",
    "}\n",
    "metric_A1 = {\n",
    "    \"MAE\": float(mae(y1.reshape(-1), yhat1.reshape(-1))),\n",
    "    \"RMSE\": float(rmse(y1.reshape(-1), yhat1.reshape(-1))),\n",
    "    \"SMAPE\": float(smape(y1.reshape(-1), yhat1.reshape(-1))),\n",
    "}\n",
    "metric_A2 = {\n",
    "    \"MAE\": float(mae(y2.reshape(-1), yhat2.reshape(-1))),\n",
    "    \"RMSE\": float(rmse(y2.reshape(-1), yhat2.reshape(-1))),\n",
    "    \"SMAPE\": float(smape(y2.reshape(-1), yhat2.reshape(-1))),\n",
    "}\n",
    "\n",
    "metric_A0, metric_A1, metric_A2"
   ],
   "id": "c67c56c98f1f8cef",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ============================================\n",
    "# CELL 7) (선택) 예측 시각화 (첨부 노트북 스타일)\n",
    "# ============================================\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_samples(y_true, preds: dict, max_n: int = 64):\n",
    "    n = min(max_n, y_true.shape[0])\n",
    "    fig, axes = plt.subplots(nrows=n, ncols=1, figsize=(12, 2.2*n), sharex=True)\n",
    "\n",
    "    if n == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for i in range(n):\n",
    "        ax = axes[i]\n",
    "        ax.plot(y_true[i], label=\"true\")\n",
    "        for k, v in preds.items():\n",
    "            ax.plot(v[i], label=k)\n",
    "        ax.set_title(f\"sample={i}\", fontsize=9)\n",
    "        ax.legend(loc=\"upper right\", fontsize=8)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_samples(\n",
    "    y_true=y0,\n",
    "    preds={\n",
    "        \"A0_y_only\": yhat0,\n",
    "        \"A1_time\": yhat1,\n",
    "        \"A2_time+holiday\": yhat2,\n",
    "    },\n",
    "    max_n=32,\n",
    ")"
   ],
   "id": "e2352ed6010a9b1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "db19faf8ecb6d7e5",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
