{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "trusted": false,
    "ExecuteTime": {
     "end_time": "2026-01-15T06:55:11.792484Z",
     "start_time": "2026-01-15T06:55:09.810496Z"
    }
   },
   "source": [
    "import os\n",
    "import time\n",
    "from datetime import date\n",
    "\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "\n",
    "\n",
    "from modeling_module.data_loader.MultiPartDataModule import MultiPartDataModule\n",
    "from modeling_module.data_loader.MultiPartExoDataModule import MultiPartExoDataModule\n",
    "\n",
    "'''\n",
    "pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128\n",
    "https://developer.nvidia.com/cuda-12-8-0-download-archive\n",
    "'''\n",
    "\n",
    "MAC_DIR = '/Users/igwanhyeong/PycharmProjects/ts_forecaster_lib/raw_data/'\n",
    "WINDOW_DIR = 'C:/Users/USER/PycharmProjects/ts_forecaster_lib/raw_data/'\n",
    "\n",
    "if sys.platform == 'win32':\n",
    "    DIR = WINDOW_DIR\n",
    "    print(torch.cuda.is_available())\n",
    "    print(torch.cuda.device_count())\n",
    "    print(torch.version.cuda)\n",
    "    print(torch.__version__)\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print(torch.__version__)\n",
    "else:\n",
    "    DIR = MAC_DIR\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n",
      "12.8\n",
      "2.11.0.dev20260112+cu128\n",
      "NVIDIA GeForce RTX 5080\n",
      "2.11.0.dev20260112+cu128\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Base Train Data",
   "id": "be48a650c5cc736b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# -----------------------------\n",
    "# YYYYWW -> day_idx (Monday of ISO week)\n",
    "# day_idx: days since 1970-01-01 (int)\n",
    "# -----------------------------\n",
    "\n",
    "walmart_df = pl.read_parquet(DIR + 'train_data/walmart_train.parquet')\n",
    "walmart_df.head()\n",
    "EPOCH = date(1970, 1, 1)\n",
    "\n",
    "def _yyyyww_to_monday_dayidx(yyyyww: int) -> int:\n",
    "    y = int(yyyyww) // 100\n",
    "    w = int(yyyyww) % 100\n",
    "    monday = date.fromisocalendar(y, w, 1)\n",
    "    return (monday - EPOCH).days\n",
    "\n",
    "from datetime import date, timedelta\n",
    "\n",
    "\n",
    "def _monday_dayidx_to_yyyyww(dayidx: int) -> int:\n",
    "    \"\"\"\n",
    "    dayidx: days since 1970-01-01 (EPOCH)\n",
    "    가정: dayidx는 '월요일'에 해당하는 날짜(ISO week Monday)\n",
    "    반환: YYYYWW (int)\n",
    "    \"\"\"\n",
    "    d = EPOCH + timedelta(days=int(dayidx))\n",
    "    iso_y, iso_w, iso_d = d.isocalendar()\n",
    "\n",
    "    # 안전장치: monday로 만든 값이 아니면 여기서 바로 티나게 함(원하면 제거 가능)\n",
    "    # iso_d: Monday=1 ... Sunday=7\n",
    "    if iso_d != 1:\n",
    "        # 월요일이 아니어도 해당 날짜가 속한 ISO week로 변환은 가능하지만,\n",
    "        # 지금 파이프라인 의도(week anchor)가 깨졌을 수 있어 경고 성격으로 둠\n",
    "        # raise ValueError(f\"dayidx={dayidx} is not Monday (iso_d={iso_d})\")\n",
    "        pass\n",
    "\n",
    "    return int(iso_y) * 100 + int(iso_w)\n",
    "\n",
    "df = (\n",
    "    walmart_df\n",
    "    .with_columns([\n",
    "        pl.col(\"unique_id\").cast(pl.Utf8),\n",
    "        pl.col(\"dt\").cast(pl.Int32).alias(\"yyyyww\"),\n",
    "        pl.col(\"y\").cast(pl.Float32),\n",
    "        pl.col(\"is_holiday\").cast(pl.Float32).alias(\"exo_is_holiday\"),\n",
    "    ])\n",
    "    .with_columns([\n",
    "        pl.col(\"yyyyww\").map_elements(_yyyyww_to_monday_dayidx, return_dtype=pl.Int32).alias(\"date_idx\")\n",
    "    ])\n",
    "    .with_columns([pl.col(\"yyyyww\").alias(\"date\")])\n",
    "    .select([\"unique_id\", \"date\", \"date_idx\", \"y\", \"exo_is_holiday\"])\n",
    ")\n",
    "\n",
    "# Optional: complete missing weeks per store (step=7 days), fill y=0, holiday=0\n",
    "def complete_weekly(g: pl.DataFrame) -> pl.DataFrame:\n",
    "    g = g.sort(\"date_idx\")\n",
    "    mn = int(g[\"date_idx\"].min())\n",
    "    mx = int(g[\"date_idx\"].max())\n",
    "    full = pl.DataFrame({\"date_idx\": pl.int_range(mn, mx + 1, step=7, eager=True).cast(pl.Int32)})\n",
    "    out = full.join(g, on=\"date_idx\", how=\"left\").with_columns([\n",
    "        pl.col(\"unique_id\").fill_null(g[\"unique_id\"][0]),\n",
    "        pl.col(\"date\").fill_null(pl.col(\"date_idx\").map_elements(_monday_dayidx_to_yyyyww, return_dtype=pl.Int32)),\n",
    "        pl.col(\"y\").fill_null(0.0),\n",
    "        pl.col(\"exo_is_holiday\").fill_null(0.0),\n",
    "    ])\n",
    "    return out\n",
    "\n",
    "df = pl.concat([complete_weekly(g) for g in df.partition_by(\"unique_id\")], how=\"vertical\").sort([\"unique_id\",\"date_idx\"])\n",
    "\n",
    "# -----------------------------\n",
    "# Past Exo Feature Engineering\n",
    "# -----------------------------\n",
    "df = (\n",
    "    df\n",
    "    .sort([\"unique_id\", \"date_idx\"])  # 시간 순 정렬 필수\n",
    "    .with_columns([\n",
    "        # 1. Rolling Features (최근 추세 및 변동성)\n",
    "        # 4주(한 달) 이동 평균\n",
    "        pl.col(\"y\")\n",
    "          .rolling_mean(window_size=4)\n",
    "          .over(\"unique_id\")\n",
    "          .fill_null(0) # 앞부분 결측 채움\n",
    "          .alias(\"exo_rolling_mean_4w\"),\n",
    "\n",
    "        # 12주(분기) 이동 평균\n",
    "        pl.col(\"y\")\n",
    "          .rolling_mean(window_size=12)\n",
    "          .over(\"unique_id\")\n",
    "          .fill_null(0)\n",
    "          .alias(\"exo_rolling_mean_12w\"),\n",
    "\n",
    "        # 4주 변동성 (표준편차)\n",
    "        pl.col(\"y\")\n",
    "          .rolling_std(window_size=4)\n",
    "          .over(\"unique_id\")\n",
    "          .fill_null(0)\n",
    "          .alias(\"exo_rolling_std_4w\"),\n",
    "\n",
    "        # 2. Lag Features (작년 동기 대비)\n",
    "        # 52주 전 데이터 (YoY) - 계절성 핵심\n",
    "        pl.col(\"y\")\n",
    "          .shift(52)\n",
    "          .over(\"unique_id\")\n",
    "          .fill_null(pl.col(\"y\")) # 결측이면 현재 값이나 0으로 대체 (상황따라 선택)\n",
    "          .alias(\"exo_lag_52w\"),\n",
    "    ])\n",
    ")\n",
    "\n",
    "# 3. Days Since Last Holiday (휴일로부터 경과일)\n",
    "# 이 로직은 조금 복잡해서 \"휴일이었던 날의 인덱스\"를 활용해 forward fill 하는 방식으로 구현\n",
    "df = df.with_columns([\n",
    "    pl.when(pl.col(\"exo_is_holiday\") == 1)\n",
    "      .then(pl.col(\"date_idx\"))\n",
    "      .otherwise(None)\n",
    "      .alias(\"last_holiday_idx\")\n",
    "])\n",
    "\n",
    "df = df.with_columns([\n",
    "    pl.col(\"last_holiday_idx\")\n",
    "      .forward_fill()\n",
    "      .over(\"unique_id\")\n",
    "])\n",
    "\n",
    "df = df.with_columns([\n",
    "    (pl.col(\"date_idx\") - pl.col(\"last_holiday_idx\"))\n",
    "      .fill_null(999) # 휴일이 한 번도 없었던 초반부는 큰 값으로\n",
    "      .cast(pl.Float32)\n",
    "      .alias(\"exo_days_since_holiday\")\n",
    "]).drop(\"last_holiday_idx\") # 임시 컬럼 삭제\n",
    "\n",
    "# 결과 확인\n",
    "print(df.select([\"unique_id\", \"date\", \"y\", \"exo_rolling_mean_4w\", \"exo_lag_52w\", \"exo_days_since_holiday\"]).tail(10))"
   ],
   "id": "848964e338bbf96a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Raw Train + Feature Data",
   "id": "8e70beccc2263f02"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "from datetime import date, timedelta\n",
    "\n",
    "# -----------------------------\n",
    "# 0) Load & basic clean\n",
    "# -----------------------------\n",
    "raw = (\n",
    "    pl.read_csv(DIR + 'csv/walmart/train.csv', infer_schema_length=100_000)\n",
    "    .join(\n",
    "        pl.read_csv(DIR + 'csv/walmart/features.csv', infer_schema_length=100_000),\n",
    "        on=['Store', 'Date'],\n",
    "        how='inner'\n",
    "    )\n",
    "    .select([\n",
    "        'Store', 'Date', 'Weekly_Sales', 'IsHoliday',\n",
    "        'Temperature', 'Fuel_Price',\n",
    "        'MarkDown1','MarkDown2','MarkDown3','MarkDown4','MarkDown5',\n",
    "        'CPI', 'Unemployment'\n",
    "    ])\n",
    ")\n",
    "\n",
    "# 문자열로 들어온 numeric 컬럼 정리 (\"\" -> null -> float)\n",
    "def to_float(col):\n",
    "    return (\n",
    "        pl.col(col)\n",
    "        .cast(pl.Utf8, strict=False)\n",
    "        .str.strip_chars()\n",
    "        .replace(\"\", None)\n",
    "        .cast(pl.Float32, strict=False)\n",
    "    )\n",
    "\n",
    "df = (\n",
    "    raw\n",
    "    .with_columns([\n",
    "        pl.col(\"Store\").cast(pl.Int32),\n",
    "        pl.col(\"Date\").str.strptime(pl.Date, \"%Y-%m-%d\", strict=False).alias(\"date_dt\"),\n",
    "        pl.col(\"Weekly_Sales\").cast(pl.Float32).alias(\"y\"),\n",
    "        pl.col(\"IsHoliday\").cast(pl.Int8).alias(\"is_holiday\"),\n",
    "        pl.col(\"Temperature\").cast(pl.Float32),\n",
    "        pl.col(\"Fuel_Price\").cast(pl.Float32),\n",
    "        to_float(\"MarkDown1\").alias(\"MarkDown1\"),\n",
    "        to_float(\"MarkDown2\").alias(\"MarkDown2\"),\n",
    "        to_float(\"MarkDown3\").alias(\"MarkDown3\"),\n",
    "        to_float(\"MarkDown4\").alias(\"MarkDown4\"),\n",
    "        to_float(\"MarkDown5\").alias(\"MarkDown5\"),\n",
    "        to_float(\"CPI\").alias(\"CPI\"),\n",
    "        to_float(\"Unemployment\").alias(\"Unemployment\"),\n",
    "    ])\n",
    ")\n",
    "\n",
    "# MarkDown은 결측이 잦습니다. \"0으로 채움 + 결측 indicator\" 조합이 실무적으로 안정적입니다.\n",
    "md_cols = [\"MarkDown1\",\"MarkDown2\",\"MarkDown3\",\"MarkDown4\",\"MarkDown5\"]\n",
    "\n",
    "df = df.with_columns(\n",
    "    [\n",
    "        pl.col(c).is_null().cast(pl.Int8).alias(f\"exo_{c}_isnull\") for c in md_cols\n",
    "    ] + [\n",
    "        pl.col(c).fill_null(0.0).alias(c) for c in md_cols\n",
    "    ] + [\n",
    "        # 총 markdown 강도(합)\n",
    "        sum([pl.col(c) for c in md_cols]).alias(\"MarkDown_sum\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 1) weekly index 만들기 (ISO week Monday anchor)\n",
    "# -----------------------------\n",
    "EPOCH = date(1970, 1, 1)\n",
    "\n",
    "def _date_to_monday_dayidx(d: date) -> int:\n",
    "    # d -> 해당 ISO week Monday -> dayidx\n",
    "    iso_y, iso_w, _ = d.isocalendar()\n",
    "    monday = date.fromisocalendar(iso_y, iso_w, 1)\n",
    "    return (monday - EPOCH).days\n",
    "\n",
    "def _dayidx_to_yyyyww(dayidx: int) -> int:\n",
    "    d = EPOCH + timedelta(days=int(dayidx))\n",
    "    iso_y, iso_w, _ = d.isocalendar()\n",
    "    return int(iso_y) * 100 + int(iso_w)\n",
    "\n",
    "df = (\n",
    "    df\n",
    "    .with_columns([\n",
    "        pl.col(\"Store\").cast(pl.Utf8).alias(\"unique_id\"),\n",
    "        pl.col(\"date_dt\").map_elements(_date_to_monday_dayidx, return_dtype=pl.Int32).alias(\"date_idx\"),\n",
    "    ])\n",
    "    .with_columns([\n",
    "        pl.col(\"date_idx\").map_elements(_dayidx_to_yyyyww, return_dtype=pl.Int32).alias(\"date\"),\n",
    "    ])\n",
    "    .select([\n",
    "        \"unique_id\", \"date\", \"date_idx\",\n",
    "        \"y\", \"is_holiday\",\n",
    "        \"Temperature\",\"Fuel_Price\",\"CPI\",\"Unemployment\",\n",
    "        *md_cols, \"MarkDown_sum\",\n",
    "        *[f\"exo_{c}_isnull\" for c in md_cols],\n",
    "    ])\n",
    "    .sort([\"unique_id\",\"date_idx\"])\n",
    ")\n",
    "\n",
    "# (선택) 주간 누락 보정: y=0, feature는 0 또는 forward-fill 정책 선택\n",
    "def complete_weekly(g: pl.DataFrame) -> pl.DataFrame:\n",
    "    g = g.sort(\"date_idx\")\n",
    "    mn = int(g[\"date_idx\"].min())\n",
    "    mx = int(g[\"date_idx\"].max())\n",
    "    full = pl.DataFrame({\"date_idx\": pl.int_range(mn, mx + 1, step=7, eager=True).cast(pl.Int32)})\n",
    "\n",
    "    out = (\n",
    "        full.join(g, on=\"date_idx\", how=\"left\")\n",
    "        .with_columns([\n",
    "            pl.col(\"unique_id\").fill_null(g[\"unique_id\"][0]),\n",
    "            pl.col(\"date\").fill_null(pl.col(\"date_idx\").map_elements(_dayidx_to_yyyyww, return_dtype=pl.Int32)),\n",
    "            pl.col(\"y\").fill_null(0.0),\n",
    "            pl.col(\"is_holiday\").fill_null(0).cast(pl.Int8),\n",
    "\n",
    "            # feature 정책: 실무에서는 보통 forward-fill 혹은 0 채움\n",
    "            pl.col(\"Temperature\").fill_null(strategy=\"forward\").fill_null(0.0),\n",
    "            pl.col(\"Fuel_Price\").fill_null(strategy=\"forward\").fill_null(0.0),\n",
    "            pl.col(\"CPI\").fill_null(strategy=\"forward\").fill_null(0.0),\n",
    "            pl.col(\"Unemployment\").fill_null(strategy=\"forward\").fill_null(0.0),\n",
    "\n",
    "            *[pl.col(c).fill_null(0.0) for c in md_cols],\n",
    "            pl.col(\"MarkDown_sum\").fill_null(0.0),\n",
    "            *[pl.col(f\"exo_{c}_isnull\").fill_null(1).cast(pl.Int8) for c in md_cols],\n",
    "        ])\n",
    "    )\n",
    "    return out\n",
    "\n",
    "df = (\n",
    "    pl.concat([complete_weekly(g) for g in df.partition_by(\"unique_id\")], how=\"vertical\")\n",
    "    .sort([\"unique_id\",\"date_idx\"])\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Future exo (calendar + holiday + (option) markdown plan)\n",
    "# -----------------------------\n",
    "# Calendar: week-of-year sin/cos, month sin/cos, linear trend\n",
    "# date_idx는 \"day\" 단위이므로 week_idx = date_idx/7\n",
    "df = df.with_columns([\n",
    "    (pl.col(\"date_idx\") / 7).cast(pl.Int32).alias(\"week_idx\"),\n",
    "])\n",
    "\n",
    "# week-of-year를 date(YYYYWW)에서 뽑기 귀찮으면, date_dt가 없으므로\n",
    "# 간단히 52주 주기 sin/cos를 week_idx로 만들 수 있습니다.\n",
    "# (완벽한 ISO week alignment는 아니지만 실무에서는 충분히 강력한 seasonality feature입니다.)\n",
    "two_pi = 2.0 * np.pi\n",
    "\n",
    "df = df.with_columns([\n",
    "    (pl.col(\"week_idx\") * (two_pi / 52.0)).sin().alias(\"exo_f_woy_sin\"),\n",
    "    (pl.col(\"week_idx\") * (two_pi / 52.0)).cos().alias(\"exo_f_woy_cos\"),\n",
    "    (pl.col(\"week_idx\") * (two_pi / 365.25)).sin().alias(\"exo_f_long_sin\"),  # 장주기 보조\n",
    "    (pl.col(\"week_idx\") * (two_pi / 365.25)).cos().alias(\"exo_f_long_cos\"),\n",
    "    pl.col(\"is_holiday\").cast(pl.Float32).alias(\"exo_f_is_holiday\"),\n",
    "    pl.col(\"week_idx\").cast(pl.Float32).alias(\"exo_f_trend\"),\n",
    "])\n",
    "\n",
    "# (선택) MarkDown을 \"미래에도 계획으로 제공된다\"는 가정이면 future로 승격 가능\n",
    "# 그렇지 않으면 아래 컬럼들은 future_exo가 아니라 past_exo로만 쓰는 것을 권합니다.\n",
    "df = df.with_columns([\n",
    "    pl.col(\"MarkDown_sum\").cast(pl.Float32).alias(\"exo_f_markdown_sum\"),\n",
    "    *[pl.col(c).cast(pl.Float32).alias(f\"exo_f_{c.lower()}\") for c in md_cols],\n",
    "])\n",
    "\n",
    "# -----------------------------\n",
    "# 3) Past exo cont (history-only derived)\n",
    "# -----------------------------\n",
    "# 타깃 기반 파생 (rolling / lag)\n",
    "df = df.with_columns([\n",
    "    pl.col(\"y\").rolling_mean(4).over(\"unique_id\").fill_null(0.0).alias(\"exo_p_y_rollmean_4w\"),\n",
    "    pl.col(\"y\").rolling_mean(12).over(\"unique_id\").fill_null(0.0).alias(\"exo_p_y_rollmean_12w\"),\n",
    "    pl.col(\"y\").rolling_std(4).over(\"unique_id\").fill_null(0.0).alias(\"exo_p_y_rollstd_4w\"),\n",
    "\n",
    "    pl.col(\"y\").shift(1).over(\"unique_id\").fill_null(0.0).alias(\"exo_p_y_lag_1w\"),\n",
    "    pl.col(\"y\").shift(2).over(\"unique_id\").fill_null(0.0).alias(\"exo_p_y_lag_2w\"),\n",
    "    pl.col(\"y\").shift(52).over(\"unique_id\").fill_null(0.0).alias(\"exo_p_y_lag_52w\"),\n",
    "])\n",
    "\n",
    "# 휴일 이후 경과 주(weeks since holiday)\n",
    "df = df.with_columns([\n",
    "    pl.when(pl.col(\"is_holiday\") == 1)\n",
    "      .then(pl.col(\"date_idx\"))\n",
    "      .otherwise(None)\n",
    "      .alias(\"_last_holiday_idx\"),\n",
    "]).with_columns([\n",
    "    pl.col(\"_last_holiday_idx\").forward_fill().over(\"unique_id\"),\n",
    "]).with_columns([\n",
    "    ((pl.col(\"date_idx\") - pl.col(\"_last_holiday_idx\")) / 7.0)\n",
    "      .fill_null(999.0)\n",
    "      .cast(pl.Float32)\n",
    "      .alias(\"exo_p_weeks_since_holiday\")\n",
    "]).drop(\"_last_holiday_idx\")\n",
    "\n",
    "# 과거 관측 feature들도 past_exo로 넣기 (미래값이 없다고 가정)\n",
    "df = df.with_columns([\n",
    "    pl.col(\"Temperature\").cast(pl.Float32).alias(\"exo_p_temperature\"),\n",
    "    pl.col(\"Fuel_Price\").cast(pl.Float32).alias(\"exo_p_fuel_price\"),\n",
    "    pl.col(\"CPI\").cast(pl.Float32).alias(\"exo_p_cpi\"),\n",
    "    pl.col(\"Unemployment\").cast(pl.Float32).alias(\"exo_p_unemployment\"),\n",
    "    pl.col(\"MarkDown_sum\").cast(pl.Float32).alias(\"exo_p_markdown_sum\"),\n",
    "    *[pl.col(c).cast(pl.Float32).alias(f\"exo_p_{c.lower()}\") for c in md_cols],\n",
    "    *[pl.col(f\"exo_{c}_isnull\").cast(pl.Float32).alias(f\"exo_p_{c.lower()}_isnull\") for c in md_cols],\n",
    "])\n",
    "\n",
    "# -----------------------------\n",
    "# 4) Past exo cat\n",
    "# -----------------------------\n",
    "# Store 자체가 범주형으로 가장 강력\n",
    "# 추가로 week-of-year bucket 같은 범주도 가능하지만, 기본은 Store만 추천\n",
    "df = df.with_columns([\n",
    "    pl.col(\"unique_id\").alias(\"exo_c_store\"),\n",
    "    # 선택: 52주 bucket (0~51)\n",
    "    (pl.col(\"week_idx\") % 52).cast(pl.Int16).alias(\"exo_c_woy_bucket\"),\n",
    "])\n",
    "\n",
    "df.write_parquet(DIR + 'train_data/walmart_feature_train_raw.parquet')"
   ],
   "id": "437e9b5b1e2f553e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Best Train + Feature Data",
   "id": "c4b74fb229aa4b37"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-15T06:55:15.979666Z",
     "start_time": "2026-01-15T06:55:15.859605Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import polars as pl\n",
    "from datetime import date, timedelta\n",
    "\n",
    "# ============================================================\n",
    "# 0) Utils: date <-> yyyyww/dayidx (ISO week Monday anchor)\n",
    "# ============================================================\n",
    "EPOCH = date(1970, 1, 1)\n",
    "\n",
    "def _dt_to_yyyyww(d: date) -> int:\n",
    "    iso_y, iso_w, _ = d.isocalendar()\n",
    "    return int(iso_y) * 100 + int(iso_w)\n",
    "\n",
    "def _dt_to_monday_dayidx(d: date) -> int:\n",
    "    \"\"\"dt가 속한 ISO week의 Monday를 anchor로 dayidx 생성\"\"\"\n",
    "    iso_y, iso_w, _ = d.isocalendar()\n",
    "    monday = date.fromisocalendar(int(iso_y), int(iso_w), 1)\n",
    "    return (monday - EPOCH).days\n",
    "\n",
    "def _monday_dayidx_to_yyyyww(dayidx: int) -> int:\n",
    "    d = EPOCH + timedelta(days=int(dayidx))\n",
    "    iso_y, iso_w, _ = d.isocalendar()\n",
    "    return int(iso_y) * 100 + int(iso_w)\n",
    "\n",
    "# ============================================================\n",
    "# 1) Load + Deduplicate (Store, Date)\n",
    "#    - train: Weekly_Sales sum, IsHoliday OR(max)\n",
    "#    - features: numeric mean, MarkDown mean (or sum 선택 가능)\n",
    "# ============================================================\n",
    "DIR = DIR  # 이미 선언돼 있다고 가정\n",
    "md_cols = [\"MarkDown1\",\"MarkDown2\",\"MarkDown3\",\"MarkDown4\",\"MarkDown5\"]\n",
    "\n",
    "train = pl.read_csv(DIR + \"csv/walmart/train.csv\", infer_schema_length=100_000)\n",
    "feat  = pl.read_csv(DIR + \"csv/walmart/features.csv\", infer_schema_length=100_000)\n",
    "\n",
    "train_s = (\n",
    "    train\n",
    "    .group_by([\"Store\",\"Date\"])\n",
    "    .agg([\n",
    "        pl.col(\"Weekly_Sales\").sum().alias(\"Weekly_Sales\"),\n",
    "        pl.col(\"IsHoliday\").max().alias(\"IsHoliday\"),  # OR\n",
    "    ])\n",
    ")\n",
    "\n",
    "feat_s = (\n",
    "    feat\n",
    "    .group_by([\"Store\",\"Date\"])\n",
    "    .agg([\n",
    "        pl.col(\"Temperature\").mean().alias(\"Temperature\"),\n",
    "        pl.col(\"Fuel_Price\").mean().alias(\"Fuel_Price\"),\n",
    "\n",
    "        # CPI, Unemployment: string->float->mean\n",
    "        pl.col(\"CPI\").cast(pl.Utf8).str.strip_chars().replace(\"\", None)\n",
    "          .cast(pl.Float32, strict=False).mean().alias(\"CPI\"),\n",
    "        pl.col(\"Unemployment\").cast(pl.Utf8).str.strip_chars().replace(\"\", None)\n",
    "          .cast(pl.Float32, strict=False).mean().alias(\"Unemployment\"),\n",
    "\n",
    "        # MarkDown: string->float->mean (원하면 sum으로 변경 가능)\n",
    "        *[\n",
    "            pl.col(c).cast(pl.Utf8).str.strip_chars().replace(\"\", None)\n",
    "              .cast(pl.Float32, strict=False).mean().alias(c)\n",
    "            for c in md_cols\n",
    "        ]\n",
    "    ])\n",
    ")\n",
    "\n",
    "df0 = (\n",
    "    train_s\n",
    "    .join(feat_s, on=[\"Store\",\"Date\"], how=\"inner\")\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# 2) Basic schema + dt/date/date_idx + observed exo\n",
    "# ============================================================\n",
    "df_base = (\n",
    "    df0\n",
    "    .with_columns([\n",
    "        pl.col(\"Store\").cast(pl.Int32),\n",
    "        pl.col(\"Store\").cast(pl.Utf8).alias(\"unique_id\"),\n",
    "\n",
    "        pl.col(\"Weekly_Sales\").cast(pl.Float32).alias(\"y\"),\n",
    "        pl.col(\"IsHoliday\").cast(pl.Float32).alias(\"exo_is_holiday\"),\n",
    "\n",
    "        pl.col(\"Temperature\").cast(pl.Float32).alias(\"exo_temperature\"),\n",
    "        pl.col(\"Fuel_Price\").cast(pl.Float32).alias(\"exo_fuel_price\"),\n",
    "\n",
    "        pl.col(\"CPI\").cast(pl.Float32).alias(\"exo_cpi\"),\n",
    "        pl.col(\"Unemployment\").cast(pl.Float32).alias(\"exo_unemployment\"),\n",
    "\n",
    "        pl.col(\"Date\").str.strptime(pl.Date, \"%Y-%m-%d\", strict=False).alias(\"dt\"),\n",
    "    ])\n",
    "    # MarkDown float + isnull flags\n",
    "    .with_columns([\n",
    "        *[\n",
    "            pl.col(c).cast(pl.Float32).alias(f\"exo_{c.lower()}\")\n",
    "            for c in md_cols\n",
    "        ],\n",
    "        *[\n",
    "            pl.col(c).is_null().cast(pl.Int8).cast(pl.Float32).alias(f\"exo_{c.lower()}_isnull\")\n",
    "            for c in md_cols\n",
    "        ],\n",
    "    ])\n",
    "    .with_columns([\n",
    "        (\n",
    "            pl.coalesce([pl.col(\"exo_markdown1\"), pl.lit(0.0)]) +\n",
    "            pl.coalesce([pl.col(\"exo_markdown2\"), pl.lit(0.0)]) +\n",
    "            pl.coalesce([pl.col(\"exo_markdown3\"), pl.lit(0.0)]) +\n",
    "            pl.coalesce([pl.col(\"exo_markdown4\"), pl.lit(0.0)]) +\n",
    "            pl.coalesce([pl.col(\"exo_markdown5\"), pl.lit(0.0)])\n",
    "        ).alias(\"exo_markdown_sum\"),\n",
    "\n",
    "        # 안전한 ISO week anchor\n",
    "        pl.col(\"dt\").map_elements(_dt_to_yyyyww, return_dtype=pl.Int32).alias(\"date\"),\n",
    "        pl.col(\"dt\").map_elements(_dt_to_monday_dayidx, return_dtype=pl.Int32).alias(\"date_idx\"),\n",
    "    ])\n",
    "    .select([\n",
    "        \"unique_id\",\"date\",\"date_idx\",\"y\",\n",
    "        \"exo_is_holiday\",\n",
    "        \"exo_temperature\",\"exo_fuel_price\",\"exo_cpi\",\"exo_unemployment\",\n",
    "        \"exo_markdown_sum\",\n",
    "        \"exo_markdown1\",\"exo_markdown2\",\"exo_markdown3\",\"exo_markdown4\",\"exo_markdown5\",\n",
    "        \"exo_markdown1_isnull\",\"exo_markdown2_isnull\",\"exo_markdown3_isnull\",\"exo_markdown4_isnull\",\"exo_markdown5_isnull\",\n",
    "    ])\n",
    "    .sort([\"unique_id\",\"date_idx\"])\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# 3) Weekly complete (벡터화): store별 [min..max] step=7 grid 생성 후 left join\n",
    "#    - y: missing=0\n",
    "#    - holiday: missing=0\n",
    "#    - temp/fuel/cpi/unemp: ff/bf\n",
    "#    - markdown: missing=0, isnull=1\n",
    "# ============================================================\n",
    "# store별 min/max\n",
    "rng = (\n",
    "    df_base\n",
    "    .group_by(\"unique_id\")\n",
    "    .agg([\n",
    "        pl.col(\"date_idx\").min().alias(\"mn\"),\n",
    "        pl.col(\"date_idx\").max().alias(\"mx\"),\n",
    "    ])\n",
    "    .with_columns([\n",
    "        pl.int_ranges(\"mn\", pl.col(\"mx\") + 1, step=7).alias(\"date_idx\")\n",
    "    ])\n",
    "    .explode(\"date_idx\")\n",
    "    .select([\"unique_id\",\"date_idx\"])\n",
    ")\n",
    "\n",
    "df_complete = (\n",
    "    rng\n",
    "    .join(df_base, on=[\"unique_id\",\"date_idx\"], how=\"left\")\n",
    "    .with_columns([\n",
    "        # missing date (yyyyww) 재생성\n",
    "        pl.col(\"date\").fill_null(\n",
    "            pl.col(\"date_idx\").map_elements(_monday_dayidx_to_yyyyww, return_dtype=pl.Int32)\n",
    "        ),\n",
    "\n",
    "        # y, holiday\n",
    "        pl.col(\"y\").fill_null(0.0),\n",
    "        pl.col(\"exo_is_holiday\").fill_null(0.0),\n",
    "    ])\n",
    ")\n",
    "\n",
    "# ff/bf for continuous observed covariates\n",
    "for c in [\"exo_temperature\",\"exo_fuel_price\",\"exo_cpi\",\"exo_unemployment\"]:\n",
    "    df_complete = df_complete.with_columns(\n",
    "        pl.col(c).forward_fill().over(\"unique_id\")\n",
    "    ).with_columns(\n",
    "        pl.col(c).backward_fill().over(\"unique_id\")\n",
    "    ).with_columns(\n",
    "        pl.col(c).fill_null(0.0)\n",
    "    )\n",
    "\n",
    "# markdown fill\n",
    "md_fill = [\"exo_markdown1\",\"exo_markdown2\",\"exo_markdown3\",\"exo_markdown4\",\"exo_markdown5\",\"exo_markdown_sum\"]\n",
    "md_isnull = [\"exo_markdown1_isnull\",\"exo_markdown2_isnull\",\"exo_markdown3_isnull\",\"exo_markdown4_isnull\",\"exo_markdown5_isnull\"]\n",
    "\n",
    "for c in md_fill:\n",
    "    df_complete = df_complete.with_columns(pl.col(c).fill_null(0.0))\n",
    "for c in md_isnull:\n",
    "    df_complete = df_complete.with_columns(pl.col(c).fill_null(1.0))\n",
    "\n",
    "df_complete = df_complete.sort([\"unique_id\",\"date_idx\"])\n",
    "\n",
    "# ============================================================\n",
    "# 4) Past exo (complete 이후 재계산이 핵심)\n",
    "# ============================================================\n",
    "df_feat = (\n",
    "    df_complete\n",
    "    .with_columns([\n",
    "        # y lags\n",
    "        pl.col(\"y\").shift(1).over(\"unique_id\").fill_null(0.0).alias(\"exo_p_y_lag_1w\"),\n",
    "        pl.col(\"y\").shift(2).over(\"unique_id\").fill_null(0.0).alias(\"exo_p_y_lag_2w\"),\n",
    "        pl.col(\"y\").shift(52).over(\"unique_id\").fill_null(0.0).alias(\"exo_p_y_lag_52w\"),\n",
    "\n",
    "        # rolling\n",
    "        pl.col(\"y\").rolling_mean(4).over(\"unique_id\").fill_null(0.0).alias(\"exo_p_y_rollmean_4w\"),\n",
    "        pl.col(\"y\").rolling_mean(12).over(\"unique_id\").fill_null(0.0).alias(\"exo_p_y_rollmean_12w\"),\n",
    "        pl.col(\"y\").rolling_std(4).over(\"unique_id\").fill_null(0.0).alias(\"exo_p_y_rollstd_4w\"),\n",
    "    ])\n",
    ")\n",
    "\n",
    "# weeks_since_holiday (complete 이후)\n",
    "df_feat = (\n",
    "    df_feat\n",
    "    .with_columns([\n",
    "        pl.when(pl.col(\"exo_is_holiday\") == 1.0).then(pl.col(\"date_idx\")).otherwise(None).alias(\"_last_hol_idx\")\n",
    "    ])\n",
    "    .with_columns([\n",
    "        pl.col(\"_last_hol_idx\").forward_fill().over(\"unique_id\")\n",
    "    ])\n",
    "    .with_columns([\n",
    "        ((pl.col(\"date_idx\") - pl.col(\"_last_hol_idx\")) / 7.0)\n",
    "        .fill_null(999.0).cast(pl.Float32).alias(\"exo_p_weeks_since_holiday\")\n",
    "    ])\n",
    "    .drop(\"_last_hol_idx\")\n",
    ")\n",
    "\n",
    "# observed covariates -> past naming 통일\n",
    "df_feat = df_feat.with_columns([\n",
    "    pl.col(\"exo_temperature\").alias(\"exo_p_temperature\"),\n",
    "    pl.col(\"exo_fuel_price\").alias(\"exo_p_fuel_price\"),\n",
    "    pl.col(\"exo_cpi\").alias(\"exo_p_cpi\"),\n",
    "    pl.col(\"exo_unemployment\").alias(\"exo_p_unemployment\"),\n",
    "\n",
    "    pl.col(\"exo_markdown_sum\").alias(\"exo_p_markdown_sum\"),\n",
    "    pl.col(\"exo_markdown1\").alias(\"exo_p_markdown1\"),\n",
    "    pl.col(\"exo_markdown2\").alias(\"exo_p_markdown2\"),\n",
    "    pl.col(\"exo_markdown3\").alias(\"exo_p_markdown3\"),\n",
    "    pl.col(\"exo_markdown4\").alias(\"exo_p_markdown4\"),\n",
    "    pl.col(\"exo_markdown5\").alias(\"exo_p_markdown5\"),\n",
    "])\n",
    "\n",
    "# categorical: woy bucket\n",
    "df_feat = (\n",
    "    df_feat\n",
    "    .with_columns((pl.col(\"date\") % 100).cast(pl.Int32).alias(\"_woy\"))\n",
    "    .with_columns((pl.col(\"_woy\") // 4).cast(pl.Int32).alias(\"exo_c_woy_bucket\"))  # 0~13\n",
    "    .drop(\"_woy\")\n",
    ")\n",
    "\n",
    "df = df_feat\n",
    "print(df.schema)\n",
    "print(df.shape)\n",
    "\n",
    "# ============================================================\n",
    "# 5) Sanity check: step=7 보장 여부\n",
    "# ============================================================\n",
    "chk = (\n",
    "    df\n",
    "    .with_columns((pl.col(\"date_idx\") - pl.col(\"date_idx\").shift(1).over(\"unique_id\")).alias(\"diff\"))\n",
    "    .filter(pl.col(\"diff\").is_not_null() & (pl.col(\"diff\") != 7))\n",
    ")\n",
    "print(\"n_breaks:\", chk.height)\n",
    "if chk.height > 0:\n",
    "    print(chk.select([\"diff\"]).value_counts().sort(\"diff\"))\n"
   ],
   "id": "68cf55a583fb0134",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema({'unique_id': String, 'date_idx': Int64, 'date': Int32, 'y': Float32, 'exo_is_holiday': Float32, 'exo_temperature': Float32, 'exo_fuel_price': Float32, 'exo_cpi': Float32, 'exo_unemployment': Float32, 'exo_markdown_sum': Float32, 'exo_markdown1': Float32, 'exo_markdown2': Float32, 'exo_markdown3': Float32, 'exo_markdown4': Float32, 'exo_markdown5': Float32, 'exo_markdown1_isnull': Float32, 'exo_markdown2_isnull': Float32, 'exo_markdown3_isnull': Float32, 'exo_markdown4_isnull': Float32, 'exo_markdown5_isnull': Float32, 'exo_p_y_lag_1w': Float32, 'exo_p_y_lag_2w': Float32, 'exo_p_y_lag_52w': Float32, 'exo_p_y_rollmean_4w': Float32, 'exo_p_y_rollmean_12w': Float32, 'exo_p_y_rollstd_4w': Float32, 'exo_p_weeks_since_holiday': Float32, 'exo_p_temperature': Float32, 'exo_p_fuel_price': Float32, 'exo_p_cpi': Float32, 'exo_p_unemployment': Float32, 'exo_p_markdown_sum': Float32, 'exo_p_markdown1': Float32, 'exo_p_markdown2': Float32, 'exo_p_markdown3': Float32, 'exo_p_markdown4': Float32, 'exo_p_markdown5': Float32, 'exo_c_woy_bucket': Int32})\n",
      "(6435, 38)\n",
      "n_breaks: 0\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-15T06:56:27.965686Z",
     "start_time": "2026-01-15T06:56:27.961688Z"
    }
   },
   "cell_type": "code",
   "source": "df.select('date').max()",
   "id": "8a1d5c42a9fb2288",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "shape: (1, 1)\n",
       "┌────────┐\n",
       "│ date   │\n",
       "│ ---    │\n",
       "│ i32    │\n",
       "╞════════╡\n",
       "│ 201243 │\n",
       "└────────┘"
      ],
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (1, 1)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>date</th></tr><tr><td>i32</td></tr></thead><tbody><tr><td>201243</td></tr></tbody></table></div>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "535843c969d45a85",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "5012956d6cd20c1d",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
