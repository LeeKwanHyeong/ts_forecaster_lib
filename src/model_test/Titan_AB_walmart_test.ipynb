{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# LTB modules (expected to exist in your repo)\n",
    "\n",
    "# optional\n",
    "\n",
    "'''\n",
    "pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128\n",
    "https://developer.nvidia.com/cuda-12-8-0-download-archive\n",
    "'''\n",
    "\n",
    "MAC_DIR = \"/Users/igwanhyeong/PycharmProjects/ts_forecaster_lib/raw_data/\"\n",
    "WINDOW_DIR = \"C:/Users/USER/PycharmProjects/ts_forecaster_lib/raw_data/\"\n",
    "\n",
    "DIR = WINDOW_DIR if sys.platform == \"win32\" else MAC_DIR\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(\"DIR:\", DIR)\n",
    "print(\"device:\", device)\n",
    "if device == \"cuda\":\n",
    "    print(\"cuda:\", torch.version.cuda, \"gpu_count:\", torch.cuda.device_count())"
   ],
   "id": "4b602662f0bf95fc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df = pl.read_parquet(DIR + 'train_data/walmart_best_feature_train.parquet')\n",
    "df"
   ],
   "id": "c4d63e78d5135c11"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T03:03:18.219907Z",
     "start_time": "2026-01-21T03:03:18.216737Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import random\n",
    "\n",
    "past_exo_cont_cols = (\n",
    "    # \"exo_p_y_lag_1w\",\n",
    "    \"exo_p_y_lag_2w\",\n",
    "    # \"exo_p_y_lag_52w\",\n",
    "    # \"exo_p_y_rollmean_4w\",\"exo_p_y_rollmean_12w\",\"exo_p_y_rollstd_4w\",\n",
    "    # \"exo_p_weeks_since_holiday`\",\n",
    "    # \"exo_p_temperature\",\n",
    "    # \"exo_p_fuel_price\",\n",
    "    # \"exo_p_cpi\",\n",
    "    # \"exo_p_unemployment\",\n",
    "    # \"exo_p_markdown_sum\",\n",
    "    # \"exo_p_markdown1\",\n",
    "    # \"exo_p_markdown2\",\n",
    "    # \"exo_p_markdown3\",\n",
    "    # \"exo_p_markdown4\",\n",
    "    # \"exo_p_markdown5\",\n",
    "    # \"exo_markdown1_isnull\",\n",
    "    # \"exo_markdown2_isnull\",\n",
    "    # \"exo_markdown3_isnull\",\n",
    "    # \"exo_markdown4_isnull\",\n",
    "    # \"exo_markdown5_isnull\",\n",
    ")\n",
    "past_exo_cat_cols = (\n",
    "    # \"exo_c_woy_bucket\",\n",
    ")\n",
    "\n",
    "lookback = 52\n",
    "horizon = 27\n",
    "batch_size = 256\n",
    "\n",
    "freq = \"weekly\"          # walmart dt is weekly\n",
    "split_mode = \"multi\"     # id-disjoint split (leakage-safe)\n",
    "shuffle = True\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "print(\"device:\", device)\n"
   ],
   "id": "b3a442e8de35c63e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "from modeling_module.utils.exogenous_utils import compose_exo_calendar_cb\n",
    "\n",
    "future_exo_cb_time = compose_exo_calendar_cb(date_type=freq)\n",
    "\n",
    "# ============================================================\n",
    "# Holiday lookup (vectorized) + FutureExo callback (time + holiday)\n",
    "# ============================================================\n",
    "holiday_map_dayidx = {\n",
    "    int(row[0]): float(row[1])\n",
    "    for row in (\n",
    "        df.select([\"date_idx\", \"exo_is_holiday\"])\n",
    "          .group_by(\"date_idx\")\n",
    "          .agg(pl.max(\"exo_is_holiday\").alias(\"exo_is_holiday\"))\n",
    "          .sort(\"date_idx\")\n",
    "          .iter_rows()\n",
    "    )\n",
    "}\n",
    "\n",
    "def build_holiday_array(holiday_map_dayidx: dict[int, float], *, pad: int = 0) -> np.ndarray:\n",
    "    if not holiday_map_dayidx:\n",
    "        return np.zeros((1,), dtype=np.float32)\n",
    "    max_k = max(int(k) for k in holiday_map_dayidx.keys())\n",
    "    arr = np.zeros((max_k + 1 + int(pad),), dtype=np.float32)\n",
    "    for k, v in holiday_map_dayidx.items():\n",
    "        kk = int(k)\n",
    "        if kk >= 0:\n",
    "            arr[kk] = float(v)\n",
    "    return arr\n",
    "\n",
    "class FutureExoTimePlusHoliday:\n",
    "    def __init__(self, holiday_by_dayidx: np.ndarray, *, step_days: int = 7):\n",
    "        self.holiday = holiday_by_dayidx.astype(np.float32, copy=False)\n",
    "        self.step_days = int(step_days)\n",
    "\n",
    "    def __call__(self, start_idx, H: int, device: str = \"cpu\"):\n",
    "        # 1) calendar exo (batch-safe)\n",
    "        cal = future_exo_cb_time(start_idx, H, device=device)  # scalar: (H,E) | batch: (B,H,E)\n",
    "\n",
    "        # 2) holiday exo (vectorized in numpy)\n",
    "        is_scalar = isinstance(start_idx, (int, np.integer))\n",
    "        if is_scalar:\n",
    "            s = np.asarray([int(start_idx)], dtype=np.int64)\n",
    "        else:\n",
    "            s = np.asarray(start_idx, dtype=np.int64).reshape(-1)\n",
    "\n",
    "        B = s.shape[0]\n",
    "        H = int(H)\n",
    "\n",
    "        offsets = (self.step_days * np.arange(H, dtype=np.int64))[None, :]  # (1,H)\n",
    "        idx = s[:, None] + offsets                                          # (B,H)\n",
    "\n",
    "        hol = np.zeros((B, H), dtype=np.float32)\n",
    "        valid = (idx >= 0) & (idx < self.holiday.shape[0])\n",
    "        hol[valid] = self.holiday[idx[valid]]\n",
    "        hol_t = torch.from_numpy(hol).unsqueeze(-1)                         # (B,H,1), CPU\n",
    "\n",
    "        target_device = cal.device  # cal이 이미 cuda일 수 있음\n",
    "        cal = cal.to(target_device, dtype=torch.float32)\n",
    "        hol_t = hol_t.to(target_device, dtype=torch.float32)\n",
    "\n",
    "        # 3) concat\n",
    "        if is_scalar:\n",
    "            out = torch.cat([cal.to(torch.float32).unsqueeze(0), hol_t], dim=-1)[0]  # (H,E+1)\n",
    "        else:\n",
    "            out = torch.cat([cal.to(torch.float32), hol_t], dim=-1)                  # (B,H,E+1)\n",
    "\n",
    "        return out\n",
    "\n",
    "holiday_by_dayidx = build_holiday_array(holiday_map_dayidx, pad=7 * (horizon + 2))\n",
    "future_exo_cb_time_plus_holiday = FutureExoTimePlusHoliday(holiday_by_dayidx, step_days=7)\n"
   ],
   "id": "45542332e11ee084"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "\n",
    "from modeling_module.utils.metrics import mae, rmse, smape\n",
    "from modeling_module.utils.eval_utils import eval_on_loader\n",
    "from modeling_module.utils.checkpoint import load_model_dict\n",
    "from modeling_module.models import build_titan_lmm, build_titan_base, build_titan_seq2seq\n",
    "from modeling_module.training.model_trainers.total_train import run_total_train_weekly\n",
    "from modeling_module.data_loader import MultiPartExoDataModule\n",
    "import pandas as pd\n",
    "rows = []  # seed loop 밖에서 선언\n",
    "\n",
    "def set_seed(seed: int = 11):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.backends.mps.is_available():\n",
    "        torch.mps.manual_seed(seed)\n",
    "    elif torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def build_datamodule(variant: str, seed: int) -> MultiPartExoDataModule:\n",
    "    if variant == \"A0\":\n",
    "        future_exo_cb = None\n",
    "    elif variant == \"A1\":\n",
    "        future_exo_cb = future_exo_cb_time\n",
    "    elif variant == \"A2\":\n",
    "        future_exo_cb = future_exo_cb_time_plus_holiday\n",
    "    else:\n",
    "        raise ValueError(variant)\n",
    "\n",
    "    return MultiPartExoDataModule(\n",
    "        df=df,\n",
    "        id_col=\"unique_id\",\n",
    "        date_col=\"date\",\n",
    "        y_col=\"y\",\n",
    "        lookback=lookback,\n",
    "        horizon=horizon,\n",
    "        batch_size=batch_size,\n",
    "        past_exo_cont_cols=past_exo_cont_cols,\n",
    "        past_exo_cat_cols=past_exo_cat_cols,\n",
    "        future_exo_cb=future_exo_cb,\n",
    "        freq=freq,\n",
    "        shuffle=shuffle,\n",
    "        split_mode=split_mode,\n",
    "        seed=seed,\n",
    "    )\n",
    "\n",
    "def inspect(loader, name):\n",
    "    b = next(iter(loader))\n",
    "    x, y, uid, fe, pe_cont, pe_cat = b\n",
    "    print(f\"[{name}] x:\", x.shape, x.device, x.dtype)\n",
    "    print(f\"[{name}] fe:\", fe.shape, fe.device, fe.dtype)\n",
    "    print(f\"[{name}] pe:\", pe_cont.shape, pe_cont.device, pe_cont.dtype)\n",
    "    print(f\"[{name}] future_exo_cb is None?\", loader.collate_fn.future_exo_cb is None)\n",
    "    if fe.shape[-1] > 0:\n",
    "        print(f\"[{name}] fe sample:\", fe[0, :3, :])\n",
    "\n",
    "# seed_list = [11, 22, 33, 44, 55]\n",
    "seed_list = [11]\n",
    "for seed in seed_list:\n",
    "    set_seed(seed)\n",
    "\n",
    "    save_dir = os.path.join(DIR, \"fit\", \"walmart_titan_ab\")\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    save_root_A0 = os.path.join(save_dir, \"A0_y_only\", f\"seed_{seed}\")\n",
    "    save_root_A1 = os.path.join(save_dir, \"A1_time_exog\", f\"seed_{seed}\")\n",
    "    save_root_A2 = os.path.join(save_dir, \"A2_time_holiday\", f\"seed_{seed}\")\n",
    "\n",
    "    data_module_A0 = build_datamodule(\"A0\", seed)\n",
    "    data_module_A1 = build_datamodule(\"A1\", seed)\n",
    "    data_module_A2 = build_datamodule(\"A2\", seed)\n",
    "\n",
    "    train_loader_A0 = data_module_A0.get_train_loader(batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    val_loader_A0   = data_module_A0.get_val_loader()\n",
    "\n",
    "    train_loader_A1 = data_module_A1.get_train_loader(batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    val_loader_A1   = data_module_A1.get_val_loader()\n",
    "\n",
    "    train_loader_A2 = data_module_A2.get_train_loader(batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    val_loader_A2   = data_module_A2.get_val_loader()\n",
    "\n",
    "    inspect(train_loader_A0, \"A0\")\n",
    "    inspect(train_loader_A1, \"A1\")\n",
    "    inspect(train_loader_A2, \"A2\")\n",
    "\n",
    "    # ============================================\n",
    "    # 학습 실행 (LTB total_train 포맷 유지)\n",
    "    # - 여기서는 \"외생변수 A/B/C\"만 비교하므로 use_ssl_pretrain=False로 고정\n",
    "    # Walmart처럼 항상 판매량이 있는(continuous) 데이터에서 “스파이크”를 잡는 규칙이:\n",
    "    # 스파이크 마스크가 과도하게 넓게 잡히거나(사실상 대부분 True)\n",
    "    # spike-loss가 MSE/제곱오차 기반인데 reduction이 sum 또는 정규화 없이 누적되어\n",
    "    # sales 스케일(1e4~1e5)에서 제곱오차가 1e8급으로 바로 올라가\n",
    "    # → 결과적으로 delta가 1e8 수준으로 튄다.\n",
    "    # → 그래서 최종적으로 이 상황에서는 spike_epoch를 0으로 잡아준다.\n",
    "    # ============================================\n",
    "    print('run result_A0')\n",
    "    results_A0 = run_total_train_weekly(\n",
    "        train_loader_A0,\n",
    "        val_loader_A0,\n",
    "        device=device,\n",
    "        lookback=lookback,\n",
    "        horizon=horizon,\n",
    "        warmup_epochs=30,\n",
    "        spike_epochs=10,\n",
    "        save_dir=save_root_A0,\n",
    "        use_exogenous_mode = False,\n",
    "        models_to_run=[\"titan\"],\n",
    "        use_ssl_pretrain=True,\n",
    "    )\n",
    "\n",
    "    print('run result_A1')\n",
    "    results_A1 = run_total_train_weekly(\n",
    "        train_loader_A1,\n",
    "        val_loader_A1,\n",
    "        device=device,\n",
    "        lookback=lookback,\n",
    "        horizon=horizon,\n",
    "        warmup_epochs=30,\n",
    "        spike_epochs=10,\n",
    "        save_dir=save_root_A1,\n",
    "        use_exogenous_mode = True,\n",
    "        models_to_run=[\"titan\"],\n",
    "        use_ssl_pretrain=True,\n",
    "    )\n",
    "\n",
    "    print('run result_A2')\n",
    "    results_A2 = run_total_train_weekly(\n",
    "        train_loader_A2,\n",
    "        val_loader_A2,\n",
    "        device=device,\n",
    "        lookback=lookback,\n",
    "        horizon=horizon,\n",
    "        warmup_epochs=30,\n",
    "        spike_epochs=10,\n",
    "        save_dir=save_root_A2,\n",
    "        use_exogenous_mode = True,\n",
    "        models_to_run=[\"titan\"],\n",
    "        use_ssl_pretrain=True,\n",
    "    )\n",
    "\n",
    "    # builders = {\n",
    "    # \"patchtst_quantile\": build_patchTST_quantile,\n",
    "    # \"patchtst\": build_patchTST_base,\n",
    "    # }\n",
    "    # builders = {\n",
    "    #     'PatchMixerQuantile': build_patch_mixer_quantile,\n",
    "    #     'PatchMixerBase': build_patch_mixer_base,\n",
    "    # }\n",
    "    builders = {\n",
    "        'TitanLMM': build_titan_lmm,\n",
    "        'TitanBase': build_titan_base,\n",
    "        'TitanSeq2Seq': build_titan_seq2seq\n",
    "    }\n",
    "\n",
    "    print(load_model_dict(save_root_A0, builders, device = device))\n",
    "\n",
    "    model_A0 = load_model_dict(save_root_A0, builders, device = device)['titan_base']\n",
    "    model_A1 = load_model_dict(save_root_A1, builders, device = device)['titan_base']\n",
    "    model_A2 = load_model_dict(save_root_A2, builders, device = device)['titan_base']\n",
    "\n",
    "    y0, yhat0 = eval_on_loader(model_A0, val_loader_A0, device=device)\n",
    "    y1, yhat1 = eval_on_loader(model_A1, val_loader_A1, device=device, future_exo_cb = future_exo_cb_time)\n",
    "    y2, yhat2 = eval_on_loader(model_A2, val_loader_A2, device=device, future_exo_cb = future_exo_cb_time_plus_holiday)\n",
    "\n",
    "    metric_A0 = {\n",
    "    \"MAE\": float(mae(y0.reshape(-1), yhat0.reshape(-1))),\n",
    "    \"RMSE\": float(rmse(y0.reshape(-1), yhat0.reshape(-1))),\n",
    "    \"SMAPE\": float(smape(y0.reshape(-1), yhat0.reshape(-1))),\n",
    "    }\n",
    "    metric_A1 = {\n",
    "        \"MAE\": float(mae(y1.reshape(-1), yhat1.reshape(-1))),\n",
    "        \"RMSE\": float(rmse(y1.reshape(-1), yhat1.reshape(-1))),\n",
    "        \"SMAPE\": float(smape(y1.reshape(-1), yhat1.reshape(-1))),\n",
    "    }\n",
    "    metric_A2 = {\n",
    "        \"MAE\": float(mae(y2.reshape(-1), yhat2.reshape(-1))),\n",
    "        \"RMSE\": float(rmse(y2.reshape(-1), yhat2.reshape(-1))),\n",
    "        \"SMAPE\": float(smape(y2.reshape(-1), yhat2.reshape(-1))),\n",
    "    }\n",
    "\n",
    "    model_A0_lmm = load_model_dict(save_root_A0, builders, device = device)['titan_lmm']\n",
    "    model_A1_lmm = load_model_dict(save_root_A1, builders, device = device)['titan_lmm']\n",
    "    model_A2_lmm = load_model_dict(save_root_A2, builders, device = device)['titan_lmm']\n",
    "\n",
    "    y0_lmm, yhat0_lmm = eval_on_loader(model_A0_lmm, val_loader_A0, device=device)\n",
    "    y1_lmm, yhat1_lmm = eval_on_loader(model_A1_lmm, val_loader_A1, device=device, future_exo_cb = future_exo_cb_time)\n",
    "    y2_lmm, yhat2_lmm = eval_on_loader(model_A2_lmm, val_loader_A2, device=device, future_exo_cb = future_exo_cb_time_plus_holiday)\n",
    "\n",
    "    metric_A0_lmm = {\n",
    "    \"MAE\": float(mae(y0_lmm.reshape(-1), yhat0_lmm.reshape(-1))),\n",
    "    \"RMSE\": float(rmse(y0_lmm.reshape(-1), yhat0_lmm.reshape(-1))),\n",
    "    \"SMAPE\": float(smape(y0_lmm.reshape(-1), yhat0_lmm.reshape(-1))),\n",
    "    }\n",
    "    metric_A1_lmm = {\n",
    "        \"MAE\": float(mae(y1_lmm.reshape(-1), yhat1_lmm.reshape(-1))),\n",
    "        \"RMSE\": float(rmse(y1_lmm.reshape(-1), yhat1_lmm.reshape(-1))),\n",
    "        \"SMAPE\": float(smape(y1_lmm.reshape(-1), yhat1_lmm.reshape(-1))),\n",
    "    }\n",
    "    metric_A2_lmm = {\n",
    "        \"MAE\": float(mae(y2_lmm.reshape(-1), yhat2_lmm.reshape(-1))),\n",
    "        \"RMSE\": float(rmse(y2_lmm.reshape(-1), yhat2_lmm.reshape(-1))),\n",
    "        \"SMAPE\": float(smape(y2_lmm.reshape(-1), yhat2_lmm.reshape(-1))),\n",
    "    }\n",
    "\n",
    "    model_A0_seq = load_model_dict(save_root_A0, builders, device = device)['titan_seq2seq']\n",
    "    model_A1_seq = load_model_dict(save_root_A1, builders, device = device)['titan_seq2seq']\n",
    "    model_A2_seq = load_model_dict(save_root_A2, builders, device = device)['titan_seq2seq']\n",
    "\n",
    "    y0_seq, yhat0_seq = eval_on_loader(model_A0_seq, val_loader_A0, device=device)\n",
    "    y1_seq, yhat1_seq = eval_on_loader(model_A1_seq, val_loader_A1, device=device, future_exo_cb = future_exo_cb_time)\n",
    "    y2_seq, yhat2_seq = eval_on_loader(model_A2_seq, val_loader_A2, device=device, future_exo_cb = future_exo_cb_time_plus_holiday)\n",
    "\n",
    "    metric_A0_seq = {\n",
    "    \"MAE\": float(mae(y0_seq.reshape(-1), yhat0_seq.reshape(-1))),\n",
    "    \"RMSE\": float(rmse(y0_seq.reshape(-1), yhat0_seq.reshape(-1))),\n",
    "    \"SMAPE\": float(smape(y0_seq.reshape(-1), yhat0_seq.reshape(-1))),\n",
    "    }\n",
    "    metric_A1_seq = {\n",
    "        \"MAE\": float(mae(y1_seq.reshape(-1), yhat1_seq.reshape(-1))),\n",
    "        \"RMSE\": float(rmse(y1_seq.reshape(-1), yhat1_seq.reshape(-1))),\n",
    "        \"SMAPE\": float(smape(y1_seq.reshape(-1), yhat1_seq.reshape(-1))),\n",
    "    }\n",
    "    metric_A2_seq = {\n",
    "        \"MAE\": float(mae(y2_seq.reshape(-1), yhat2_seq.reshape(-1))),\n",
    "        \"RMSE\": float(rmse(y2_seq.reshape(-1), yhat2_seq.reshape(-1))),\n",
    "        \"SMAPE\": float(smape(y2_seq.reshape(-1), yhat2_seq.reshape(-1))),\n",
    "    }\n",
    "\n",
    "\n",
    "    # -------------------------\n",
    "    # Point metrics row append\n",
    "    # -------------------------\n",
    "    rows.append({\n",
    "        \"seed\": seed,\n",
    "        \"variant\": \"A0\",\n",
    "        \"model_type\": \"point\",\n",
    "        \"MAE\": metric_A0[\"MAE\"],\n",
    "        \"RMSE\": metric_A0[\"RMSE\"],\n",
    "        \"SMAPE\": metric_A0[\"SMAPE\"],\n",
    "        \"save_root\": save_root_A0,\n",
    "    })\n",
    "    rows.append({\n",
    "        \"seed\": seed,\n",
    "        \"variant\": \"A1\",\n",
    "        \"model_type\": \"point\",\n",
    "        \"MAE\": metric_A1[\"MAE\"],\n",
    "        \"RMSE\": metric_A1[\"RMSE\"],\n",
    "        \"SMAPE\": metric_A1[\"SMAPE\"],\n",
    "        \"save_root\": save_root_A1,\n",
    "    })\n",
    "    rows.append({\n",
    "        \"seed\": seed,\n",
    "        \"variant\": \"A2\",\n",
    "        \"model_type\": \"point\",\n",
    "        \"MAE\": metric_A2[\"MAE\"],\n",
    "        \"RMSE\": metric_A2[\"RMSE\"],\n",
    "        \"SMAPE\": metric_A2[\"SMAPE\"],\n",
    "        \"save_root\": save_root_A2,\n",
    "    })\n",
    "\n",
    "    # -------------------------\n",
    "    # Quantile metrics row append\n",
    "    # (주의: q50 등 기준이 명확해야 함)\n",
    "    # -------------------------\n",
    "    rows.append({\n",
    "        \"seed\": seed,\n",
    "        \"variant\": \"A0\",\n",
    "        \"model_type\": \"quantile(q50)\",\n",
    "        \"MAE\": metric_A0_lmm[\"MAE\"],\n",
    "        \"RMSE\": metric_A0_lmm[\"RMSE\"],\n",
    "        \"SMAPE\": metric_A0_lmm[\"SMAPE\"],\n",
    "        \"save_root\": save_root_A0,\n",
    "    })\n",
    "    rows.append({\n",
    "        \"seed\": seed,\n",
    "        \"variant\": \"A1\",\n",
    "        \"model_type\": \"quantile(q50)\",\n",
    "        \"MAE\": metric_A1_lmm[\"MAE\"],\n",
    "        \"RMSE\": metric_A1_lmm[\"RMSE\"],\n",
    "        \"SMAPE\": metric_A1_lmm[\"SMAPE\"],\n",
    "        \"save_root\": save_root_A1,\n",
    "    })\n",
    "    rows.append({\n",
    "        \"seed\": seed,\n",
    "        \"variant\": \"A2\",\n",
    "        \"model_type\": \"quantile(q50)\",\n",
    "        \"MAE\": metric_A2_lmm[\"MAE\"],\n",
    "        \"RMSE\": metric_A2_lmm[\"RMSE\"],\n",
    "        \"SMAPE\": metric_A2_lmm[\"SMAPE\"],\n",
    "        \"save_root\": save_root_A2,\n",
    "    })\n",
    "\n",
    "    rows.append({\n",
    "        \"seed\": seed,\n",
    "        \"variant\": \"A0\",\n",
    "        \"model_type\": \"quantile(q50)\",\n",
    "        \"MAE\": metric_A0_seq[\"MAE\"],\n",
    "        \"RMSE\": metric_A0_seq[\"RMSE\"],\n",
    "        \"SMAPE\": metric_A0_seq[\"SMAPE\"],\n",
    "        \"save_root\": save_root_A0,\n",
    "    })\n",
    "    rows.append({\n",
    "        \"seed\": seed,\n",
    "        \"variant\": \"A1\",\n",
    "        \"model_type\": \"quantile(q50)\",\n",
    "        \"MAE\": metric_A1_seq[\"MAE\"],\n",
    "        \"RMSE\": metric_A1_seq[\"RMSE\"],\n",
    "        \"SMAPE\": metric_A1_seq[\"SMAPE\"],\n",
    "        \"save_root\": save_root_A1,\n",
    "    })\n",
    "    rows.append({\n",
    "        \"seed\": seed,\n",
    "        \"variant\": \"A2\",\n",
    "        \"model_type\": \"quantile(q50)\",\n",
    "        \"MAE\": metric_A2_seq[\"MAE\"],\n",
    "        \"RMSE\": metric_A2_seq[\"RMSE\"],\n",
    "        \"SMAPE\": metric_A2_seq[\"SMAPE\"],\n",
    "        \"save_root\": save_root_A2,\n",
    "    })\n",
    "\n",
    "# loop 종료 후 저장\n",
    "df_out = pd.DataFrame(rows)\n",
    "\n",
    "out_csv = os.path.join(save_dir, \"ab_results_by_seed.csv\")\n",
    "df_out.to_csv(out_csv, index=False)\n",
    "\n",
    "# variant별 mean/std 요약도 같이 저장 추천\n",
    "summary = (\n",
    "    df_out.groupby([\"variant\", \"model_type\"])[[\"MAE\", \"RMSE\", \"SMAPE\"]]\n",
    "    .agg([\"mean\", \"std\"])\n",
    "    .reset_index()\n",
    ")\n",
    "out_sum = os.path.join(save_dir, \"ab_results_summary.csv\")\n",
    "summary.to_csv(out_sum, index=False)\n",
    "\n",
    "print(\"saved:\", out_csv, out_sum)\n"
   ],
   "id": "9e408c0005cf8abf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ============================================\n",
    "# CELL 7) (선택) 예측 시각화 (첨부 노트북 스타일)\n",
    "# ============================================\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_samples(y_true, preds: dict, max_n: int = 64):\n",
    "    n = min(max_n, y_true.shape[0])\n",
    "    fig, axes = plt.subplots(nrows=n, ncols=1, figsize=(12, 2.2*n), sharex=True)\n",
    "\n",
    "    if n == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for i in range(n):\n",
    "        ax = axes[i]\n",
    "        ax.plot(y_true[i], label=\"true\")\n",
    "        for k, v in preds.items():\n",
    "            ax.plot(v[i], label=k)\n",
    "        ax.set_title(f\"sample={i}\", fontsize=9)\n",
    "        ax.legend(loc=\"upper right\", fontsize=8)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_samples(\n",
    "    y_true=y0_seq,\n",
    "    preds={\n",
    "        \"A0_y_only\": yhat0_lmm,\n",
    "        \"A1_time\": yhat1_lmm,\n",
    "        \"A2_time+holiday\": yhat2_lmm,\n",
    "    },\n",
    "    max_n=32,\n",
    ")"
   ],
   "id": "82226ae0778745d1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
