{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-01-14T09:51:13.836440Z",
     "start_time": "2026-01-14T09:51:12.696211Z"
    }
   },
   "source": [
    "import sys\n",
    "import torch\n",
    "\n",
    "'''\n",
    "pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128\n",
    "https://developer.nvidia.com/cuda-12-8-0-download-archive\n",
    "'''\n",
    "\n",
    "MAC_DIR = '/Users/igwanhyeong/PycharmProjects/ts_forecaster_lib/raw_data/'\n",
    "WINDOW_DIR = 'C:/Users/USER/PycharmProjects/ts_forecaster_lib/raw_data/'\n",
    "\n",
    "if sys.platform == 'win32':\n",
    "    DIR = WINDOW_DIR\n",
    "    print(torch.cuda.is_available())\n",
    "    print(torch.cuda.device_count())\n",
    "    print(torch.version.cuda)\n",
    "    print(torch.__version__)\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print(torch.__version__)\n",
    "else:\n",
    "    DIR = MAC_DIR\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "save_dir = DIR + 'fit/model_validation'\n",
    "\n",
    "# if os.path.exists(save_dir):\n",
    "#     files = glob.glob(os.path.join(save_dir, \"*.pt\"))\n",
    "#     print(f\"Deleting {len(files)} old checkpoint files...\")\n",
    "#     for f in files:\n",
    "#         try:\n",
    "#             os.remove(f)\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error deleting {f}: {e}\")\n",
    "# else:\n",
    "#     os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "print(\"Clean up complete.\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n",
      "12.8\n",
      "2.11.0.dev20260112+cu128\n",
      "NVIDIA GeForce RTX 5080\n",
      "2.11.0.dev20260112+cu128\n",
      "Clean up complete.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-14T09:51:14.598953Z",
     "start_time": "2026-01-14T09:51:14.254804Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "\n",
    "ETT1 = pl.read_csv(DIR + \"csv/ETT/ETTh1.csv\")\n",
    "\n",
    "df = (\n",
    "    ETT1\n",
    "    .select([\"date\", \"HUFL\"])\n",
    "    .with_columns(pl.lit(\"A\").alias(\"unique_id\"))\n",
    "    # 원본 date 문자열을 그대로 Datetime으로 파싱\n",
    "    .with_columns(\n",
    "        pl.col(\"date\").str.strptime(pl.Datetime, format=\"%Y-%m-%d %H:%M:%S\", strict=False).alias(\"date\")\n",
    "    )\n",
    "    .sort([\"unique_id\", \"date\"])\n",
    ")\n",
    "\n",
    "# time index\n",
    "df = df.with_columns(\n",
    "    pl.arange(0, pl.len()).over(\"unique_id\").alias(\"t_idx\")\n",
    ")\n",
    "\n",
    "# (1) known-future 스케줄: promo (예: 특정 시간대에만 1)\n",
    "# 하루 24시간 중 8~10시, 18~20시에 프로모션이라고 가정\n",
    "df = df.with_columns([\n",
    "    (pl.col(\"t_idx\") % 24).alias(\"hour\"),\n",
    "])\n",
    "\n",
    "df = df.with_columns([\n",
    "    (\n",
    "        ((pl.col(\"hour\") >= 8) & (pl.col(\"hour\") <= 10)) |\n",
    "        ((pl.col(\"hour\") >= 18) & (pl.col(\"hour\") <= 20))\n",
    "    ).cast(pl.Int8).alias(\"promo_flag\")\n",
    "])\n",
    "\n",
    "# (2) calendar exo: 24h sin/cos\n",
    "df = df.with_columns([\n",
    "    ( (2*np.pi*pl.col(\"t_idx\")/24.0).sin().cast(pl.Float32) ).alias(\"exo_fut_sin24\"),\n",
    "    ( (2*np.pi*pl.col(\"t_idx\")/24.0).cos().cast(pl.Float32) ).alias(\"exo_fut_cos24\"),\n",
    "])\n",
    "\n",
    "# (3) (중요) 타깃에 promo 효과 \"주입\" -> exo가 없으면 예측이 어려워지고, 있으면 쉬워짐\n",
    "# HUFL_y = HUFL + alpha*promo_flag + beta*sin24  (alpha는 체감되게 크게)\n",
    "alpha = 2.0\n",
    "beta  = 0.5\n",
    "df = df.with_columns([\n",
    "    (\n",
    "        pl.col(\"HUFL\").cast(pl.Float32)\n",
    "        + pl.col(\"promo_flag\").cast(pl.Float32) * pl.lit(alpha)\n",
    "        + pl.col(\"exo_fut_sin24\").cast(pl.Float32) * pl.lit(beta)\n",
    "    ).alias(\"y\")\n",
    "])\n",
    "\n",
    "# =========================\n",
    "# past_exo 후보 생성\n",
    "# =========================\n",
    "# 기준: y를 만들었으면 y 기반으로 만드는 게 가장 직관적.\n",
    "# (HUFL 원본 기반으로도 가능하나, 지금은 y에 promo/seasonality가 주입되어 있으니 y 기준 추천)\n",
    "\n",
    "df = df.with_columns([\n",
    "    # (A) lag / diff\n",
    "    pl.col(\"y\").shift(1).over(\"unique_id\").alias(\"pe_lag1_y\"),\n",
    "    pl.col(\"y\").shift(24).over(\"unique_id\").alias(\"pe_lag24_y\"),  # 하루 전(24시간 전)\n",
    "    (pl.col(\"y\") - pl.col(\"y\").shift(1).over(\"unique_id\")).alias(\"pe_diff1_y\"),\n",
    "    (pl.col(\"y\") - pl.col(\"y\").shift(24).over(\"unique_id\")).alias(\"pe_diff24_y\"),\n",
    "\n",
    "    # (B) rolling mean / std (짧은/중간 윈도우)\n",
    "    pl.col(\"y\").rolling_mean(window_size=6).over(\"unique_id\").alias(\"pe_rm6_y\"),\n",
    "    pl.col(\"y\").rolling_mean(window_size=24).over(\"unique_id\").alias(\"pe_rm24_y\"),\n",
    "    pl.col(\"y\").rolling_std(window_size=24).over(\"unique_id\").alias(\"pe_rs24_y\"),\n",
    "\n",
    "    # (C) z-score (24시간 기준)\n",
    "    (\n",
    "        (pl.col(\"y\") - pl.col(\"y\").rolling_mean(24).over(\"unique_id\"))\n",
    "        / (pl.col(\"y\").rolling_std(24).over(\"unique_id\") + 1e-6)\n",
    "    ).alias(\"pe_z24_y\"),\n",
    "\n",
    "    # (D) EMA (지수이동평균) - Polars ewm_mean 사용\n",
    "    pl.col(\"y\").ewm_mean(alpha=0.2).over(\"unique_id\").alias(\"pe_ema_a02_y\"),\n",
    "\n",
    "    # (E) promo의 과거 상태 (이벤트의 lag)\n",
    "    pl.col(\"promo_flag\").shift(1).over(\"unique_id\").cast(pl.Float32).alias(\"pe_lag1_promo\"),\n",
    "    pl.col(\"promo_flag\").rolling_mean(24).over(\"unique_id\").cast(pl.Float32).alias(\"pe_rm24_promo\"),\n",
    "])\n",
    "\n",
    "# rolling/shift로 인해 처음 구간에 null이 생깁니다.\n",
    "# TrainingDataset은 null을 그대로 numpy로 가져오면 nan이 될 수 있으니, 보통 0으로 채우는 편이 안전합니다.\n",
    "past_cols = [\n",
    "    \"pe_lag1_y\", \"pe_lag24_y\", \"pe_diff1_y\", \"pe_diff24_y\",\n",
    "    \"pe_rm6_y\", \"pe_rm24_y\", \"pe_rs24_y\", \"pe_z24_y\",\n",
    "    \"pe_ema_a02_y\", \"pe_lag1_promo\", \"pe_rm24_promo\",\n",
    "]\n",
    "\n",
    "df = df.with_columns([pl.col(c).fill_null(0.0).cast(pl.Float32) for c in past_cols])\n",
    "\n",
    "df.select([\"date\",\"promo_flag\", \"y\", 'HUFL'] + past_cols).head(5)\n"
   ],
   "id": "f07420259e7d153d",
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "지정된 파일을 찾을 수 없습니다. (os error 2): C:/Users/USER/PycharmProjects/ts_forecaster_lib/raw_data/csv/ETTh1.csv",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mFileNotFoundError\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[2]\u001B[39m\u001B[32m, line 4\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mpolars\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mpl\u001B[39;00m\n\u001B[32m      2\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mnumpy\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mnp\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m4\u001B[39m ETT1 = \u001B[43mpl\u001B[49m\u001B[43m.\u001B[49m\u001B[43mread_csv\u001B[49m\u001B[43m(\u001B[49m\u001B[43mDIR\u001B[49m\u001B[43m \u001B[49m\u001B[43m+\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mcsv/ETTh1.csv\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m      6\u001B[39m df = (\n\u001B[32m      7\u001B[39m     ETT1\n\u001B[32m      8\u001B[39m     .select([\u001B[33m\"\u001B[39m\u001B[33mdate\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mHUFL\u001B[39m\u001B[33m\"\u001B[39m])\n\u001B[32m   (...)\u001B[39m\u001B[32m     14\u001B[39m     .sort([\u001B[33m\"\u001B[39m\u001B[33munique_id\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mdate\u001B[39m\u001B[33m\"\u001B[39m])\n\u001B[32m     15\u001B[39m )\n\u001B[32m     17\u001B[39m \u001B[38;5;66;03m# time index\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniconda3\\envs\\tsf-win312\\Lib\\site-packages\\polars\\_utils\\deprecation.py:128\u001B[39m, in \u001B[36mdeprecate_renamed_parameter.<locals>.decorate.<locals>.wrapper\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    123\u001B[39m \u001B[38;5;129m@wraps\u001B[39m(function)\n\u001B[32m    124\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mwrapper\u001B[39m(*args: P.args, **kwargs: P.kwargs) -> T:\n\u001B[32m    125\u001B[39m     _rename_keyword_argument(\n\u001B[32m    126\u001B[39m         old_name, new_name, kwargs, function.\u001B[34m__qualname__\u001B[39m, version\n\u001B[32m    127\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m128\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunction\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniconda3\\envs\\tsf-win312\\Lib\\site-packages\\polars\\_utils\\deprecation.py:128\u001B[39m, in \u001B[36mdeprecate_renamed_parameter.<locals>.decorate.<locals>.wrapper\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    123\u001B[39m \u001B[38;5;129m@wraps\u001B[39m(function)\n\u001B[32m    124\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mwrapper\u001B[39m(*args: P.args, **kwargs: P.kwargs) -> T:\n\u001B[32m    125\u001B[39m     _rename_keyword_argument(\n\u001B[32m    126\u001B[39m         old_name, new_name, kwargs, function.\u001B[34m__qualname__\u001B[39m, version\n\u001B[32m    127\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m128\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunction\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniconda3\\envs\\tsf-win312\\Lib\\site-packages\\polars\\_utils\\deprecation.py:128\u001B[39m, in \u001B[36mdeprecate_renamed_parameter.<locals>.decorate.<locals>.wrapper\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    123\u001B[39m \u001B[38;5;129m@wraps\u001B[39m(function)\n\u001B[32m    124\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mwrapper\u001B[39m(*args: P.args, **kwargs: P.kwargs) -> T:\n\u001B[32m    125\u001B[39m     _rename_keyword_argument(\n\u001B[32m    126\u001B[39m         old_name, new_name, kwargs, function.\u001B[34m__qualname__\u001B[39m, version\n\u001B[32m    127\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m128\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunction\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniconda3\\envs\\tsf-win312\\Lib\\site-packages\\polars\\io\\csv\\functions.py:551\u001B[39m, in \u001B[36mread_csv\u001B[39m\u001B[34m(source, has_header, columns, new_columns, separator, comment_prefix, quote_char, skip_rows, skip_lines, schema, schema_overrides, null_values, missing_utf8_is_empty_string, ignore_errors, try_parse_dates, n_threads, infer_schema, infer_schema_length, batch_size, n_rows, encoding, low_memory, rechunk, use_pyarrow, storage_options, skip_rows_after_header, row_index_name, row_index_offset, sample_size, eol_char, raise_if_empty, truncate_ragged_lines, decimal_comma, glob)\u001B[39m\n\u001B[32m    543\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    544\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m prepare_file_arg(\n\u001B[32m    545\u001B[39m         source,\n\u001B[32m    546\u001B[39m         encoding=encoding,\n\u001B[32m   (...)\u001B[39m\u001B[32m    549\u001B[39m         storage_options=storage_options,\n\u001B[32m    550\u001B[39m     ) \u001B[38;5;28;01mas\u001B[39;00m data:\n\u001B[32m--> \u001B[39m\u001B[32m551\u001B[39m         df = \u001B[43m_read_csv_impl\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    552\u001B[39m \u001B[43m            \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    553\u001B[39m \u001B[43m            \u001B[49m\u001B[43mhas_header\u001B[49m\u001B[43m=\u001B[49m\u001B[43mhas_header\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    554\u001B[39m \u001B[43m            \u001B[49m\u001B[43mcolumns\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcolumns\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mcolumns\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mprojection\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    555\u001B[39m \u001B[43m            \u001B[49m\u001B[43mseparator\u001B[49m\u001B[43m=\u001B[49m\u001B[43mseparator\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    556\u001B[39m \u001B[43m            \u001B[49m\u001B[43mcomment_prefix\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcomment_prefix\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    557\u001B[39m \u001B[43m            \u001B[49m\u001B[43mquote_char\u001B[49m\u001B[43m=\u001B[49m\u001B[43mquote_char\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    558\u001B[39m \u001B[43m            \u001B[49m\u001B[43mskip_rows\u001B[49m\u001B[43m=\u001B[49m\u001B[43mskip_rows\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    559\u001B[39m \u001B[43m            \u001B[49m\u001B[43mskip_lines\u001B[49m\u001B[43m=\u001B[49m\u001B[43mskip_lines\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    560\u001B[39m \u001B[43m            \u001B[49m\u001B[43mschema_overrides\u001B[49m\u001B[43m=\u001B[49m\u001B[43mschema_overrides\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    561\u001B[39m \u001B[43m            \u001B[49m\u001B[43mschema\u001B[49m\u001B[43m=\u001B[49m\u001B[43mschema\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    562\u001B[39m \u001B[43m            \u001B[49m\u001B[43mnull_values\u001B[49m\u001B[43m=\u001B[49m\u001B[43mnull_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    563\u001B[39m \u001B[43m            \u001B[49m\u001B[43mmissing_utf8_is_empty_string\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmissing_utf8_is_empty_string\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    564\u001B[39m \u001B[43m            \u001B[49m\u001B[43mignore_errors\u001B[49m\u001B[43m=\u001B[49m\u001B[43mignore_errors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    565\u001B[39m \u001B[43m            \u001B[49m\u001B[43mtry_parse_dates\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtry_parse_dates\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    566\u001B[39m \u001B[43m            \u001B[49m\u001B[43mn_threads\u001B[49m\u001B[43m=\u001B[49m\u001B[43mn_threads\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    567\u001B[39m \u001B[43m            \u001B[49m\u001B[43minfer_schema_length\u001B[49m\u001B[43m=\u001B[49m\u001B[43minfer_schema_length\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    568\u001B[39m \u001B[43m            \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m=\u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    569\u001B[39m \u001B[43m            \u001B[49m\u001B[43mn_rows\u001B[49m\u001B[43m=\u001B[49m\u001B[43mn_rows\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    570\u001B[39m \u001B[43m            \u001B[49m\u001B[43mencoding\u001B[49m\u001B[43m=\u001B[49m\u001B[43mencoding\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mencoding\u001B[49m\u001B[43m \u001B[49m\u001B[43m==\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mutf8-lossy\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mutf8\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m    571\u001B[39m \u001B[43m            \u001B[49m\u001B[43mlow_memory\u001B[49m\u001B[43m=\u001B[49m\u001B[43mlow_memory\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    572\u001B[39m \u001B[43m            \u001B[49m\u001B[43mrechunk\u001B[49m\u001B[43m=\u001B[49m\u001B[43mrechunk\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    573\u001B[39m \u001B[43m            \u001B[49m\u001B[43mskip_rows_after_header\u001B[49m\u001B[43m=\u001B[49m\u001B[43mskip_rows_after_header\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    574\u001B[39m \u001B[43m            \u001B[49m\u001B[43mrow_index_name\u001B[49m\u001B[43m=\u001B[49m\u001B[43mrow_index_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    575\u001B[39m \u001B[43m            \u001B[49m\u001B[43mrow_index_offset\u001B[49m\u001B[43m=\u001B[49m\u001B[43mrow_index_offset\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    576\u001B[39m \u001B[43m            \u001B[49m\u001B[43meol_char\u001B[49m\u001B[43m=\u001B[49m\u001B[43meol_char\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    577\u001B[39m \u001B[43m            \u001B[49m\u001B[43mraise_if_empty\u001B[49m\u001B[43m=\u001B[49m\u001B[43mraise_if_empty\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    578\u001B[39m \u001B[43m            \u001B[49m\u001B[43mtruncate_ragged_lines\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtruncate_ragged_lines\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    579\u001B[39m \u001B[43m            \u001B[49m\u001B[43mdecimal_comma\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdecimal_comma\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    580\u001B[39m \u001B[43m            \u001B[49m\u001B[43mglob\u001B[49m\u001B[43m=\u001B[49m\u001B[43mglob\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    581\u001B[39m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    583\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m new_columns:\n\u001B[32m    584\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m _update_columns(df, new_columns)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniconda3\\envs\\tsf-win312\\Lib\\site-packages\\polars\\io\\csv\\functions.py:699\u001B[39m, in \u001B[36m_read_csv_impl\u001B[39m\u001B[34m(source, has_header, columns, separator, comment_prefix, quote_char, skip_rows, skip_lines, schema, schema_overrides, null_values, missing_utf8_is_empty_string, ignore_errors, try_parse_dates, n_threads, infer_schema_length, batch_size, n_rows, encoding, low_memory, rechunk, skip_rows_after_header, row_index_name, row_index_offset, sample_size, eol_char, raise_if_empty, truncate_ragged_lines, decimal_comma, glob)\u001B[39m\n\u001B[32m    695\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(msg)\n\u001B[32m    697\u001B[39m projection, columns = parse_columns_arg(columns)\n\u001B[32m--> \u001B[39m\u001B[32m699\u001B[39m pydf = \u001B[43mPyDataFrame\u001B[49m\u001B[43m.\u001B[49m\u001B[43mread_csv\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    700\u001B[39m \u001B[43m    \u001B[49m\u001B[43msource\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    701\u001B[39m \u001B[43m    \u001B[49m\u001B[43minfer_schema_length\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    702\u001B[39m \u001B[43m    \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    703\u001B[39m \u001B[43m    \u001B[49m\u001B[43mhas_header\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    704\u001B[39m \u001B[43m    \u001B[49m\u001B[43mignore_errors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    705\u001B[39m \u001B[43m    \u001B[49m\u001B[43mn_rows\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    706\u001B[39m \u001B[43m    \u001B[49m\u001B[43mskip_rows\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    707\u001B[39m \u001B[43m    \u001B[49m\u001B[43mskip_lines\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    708\u001B[39m \u001B[43m    \u001B[49m\u001B[43mprojection\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    709\u001B[39m \u001B[43m    \u001B[49m\u001B[43mseparator\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    710\u001B[39m \u001B[43m    \u001B[49m\u001B[43mrechunk\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    711\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcolumns\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    712\u001B[39m \u001B[43m    \u001B[49m\u001B[43mencoding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    713\u001B[39m \u001B[43m    \u001B[49m\u001B[43mn_threads\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    714\u001B[39m \u001B[43m    \u001B[49m\u001B[43mpath\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    715\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdtype_list\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    716\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdtype_slice\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    717\u001B[39m \u001B[43m    \u001B[49m\u001B[43mlow_memory\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    718\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcomment_prefix\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    719\u001B[39m \u001B[43m    \u001B[49m\u001B[43mquote_char\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    720\u001B[39m \u001B[43m    \u001B[49m\u001B[43mprocessed_null_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    721\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmissing_utf8_is_empty_string\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    722\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtry_parse_dates\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    723\u001B[39m \u001B[43m    \u001B[49m\u001B[43mskip_rows_after_header\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    724\u001B[39m \u001B[43m    \u001B[49m\u001B[43mparse_row_index_args\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrow_index_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrow_index_offset\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    725\u001B[39m \u001B[43m    \u001B[49m\u001B[43meol_char\u001B[49m\u001B[43m=\u001B[49m\u001B[43meol_char\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    726\u001B[39m \u001B[43m    \u001B[49m\u001B[43mraise_if_empty\u001B[49m\u001B[43m=\u001B[49m\u001B[43mraise_if_empty\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    727\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtruncate_ragged_lines\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtruncate_ragged_lines\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    728\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdecimal_comma\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdecimal_comma\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    729\u001B[39m \u001B[43m    \u001B[49m\u001B[43mschema\u001B[49m\u001B[43m=\u001B[49m\u001B[43mschema\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    730\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    731\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m wrap_df(pydf)\n",
      "\u001B[31mFileNotFoundError\u001B[39m: 지정된 파일을 찾을 수 없습니다. (os error 2): C:/Users/USER/PycharmProjects/ts_forecaster_lib/raw_data/csv/ETTh1.csv"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from modeling_module.utils.metrics import mae, smape\n",
    "\n",
    "from modeling_module.utils.metrics import rmse\n",
    "\n",
    "\n",
    "def load_model_ckpt(model, ckpt_path: str, device: str):\n",
    "    state = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "    model.load_state_dict(state[\"model_state\"], strict=True)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_on_loader(model, loader, device: str):\n",
    "    model.eval()\n",
    "    ys, yhats = [], []\n",
    "\n",
    "    print('batch_size', loader.batch_size)\n",
    "    for batch in loader:\n",
    "        x = batch[0].to(device)              # [B, 52, 1]\n",
    "        y = batch[1].to(device)              # [B, 27]\n",
    "        future_exo = batch[3].to(device)     # [B, 27, 4]\n",
    "        past_exo_cont = batch[4].to(device)  # [B, 52, 11]\n",
    "        past_exo_cat = batch[5].to(device)   # [B, 52, 0]\n",
    "\n",
    "        # PatchTSTPointModel.forward 시그니처에 맞춰 전달\n",
    "        yhat = model(\n",
    "            x,\n",
    "            future_exo=future_exo,\n",
    "            past_exo_cont=past_exo_cont,\n",
    "            past_exo_cat=past_exo_cat,\n",
    "        )  # [B, 27]\n",
    "\n",
    "        ys.append(y.detach().cpu().numpy())\n",
    "        yhats.append(yhat.detach().cpu().numpy())\n",
    "\n",
    "    y_all = np.concatenate(ys, axis=0)\n",
    "    yhat_all = np.concatenate(yhats, axis=0)\n",
    "    return y_all, yhat_all\n",
    "\n",
    "\n",
    "def _extract_pred_from_output(out, prefer_q=0.5):\n",
    "    \"\"\"\n",
    "    Quantile 모델 출력(out)이 dict/tuple/tensor 등일 수 있으므로\n",
    "    예측 텐서를 안전하게 꺼낸다.\n",
    "\n",
    "    반환:\n",
    "      - yhat_point: [B,H] (예: q=0.5 또는 첫 번째 출력)\n",
    "      - extra: 원본 out (필요 시 분석용)\n",
    "    \"\"\"\n",
    "    # 1) Tensor면 그대로\n",
    "    if torch.is_tensor(out):\n",
    "        return out, out\n",
    "\n",
    "    # 2) tuple/list면 첫 원소를 예측으로 가정\n",
    "    if isinstance(out, (tuple, list)):\n",
    "        # 가장 흔한 케이스: (pred, aux) 또는 (q_pred, ...)\n",
    "        first = out[0]\n",
    "        if torch.is_tensor(first):\n",
    "            return first, out\n",
    "        # 더 복잡하면 아래 dict 로직으로 넘기기 위해 out를 dict처럼 처리 불가 -> 에러\n",
    "        raise TypeError(f\"Unsupported tuple/list output types: {[type(x) for x in out]}\")\n",
    "\n",
    "    # 3) dict면 키 후보들에서 찾기\n",
    "    if isinstance(out, dict):\n",
    "        # 흔한 키 후보들\n",
    "        key_candidates = [\n",
    "            \"yhat\", \"pred\", \"prediction\", \"y_pred\", \"output\",\n",
    "            \"q_pred\", \"quantiles\", \"yq\", \"y_hat\"\n",
    "        ]\n",
    "        for k in key_candidates:\n",
    "            if k in out and torch.is_tensor(out[k]):\n",
    "                t = out[k]\n",
    "                # [B,H] 또는 [B,H,Q] 또는 [B,Q,H] 등 가능\n",
    "                return _select_point_from_quantile_tensor(t, prefer_q=prefer_q), out\n",
    "\n",
    "        # dict 안에 텐서가 하나뿐이면 그걸 쓰기\n",
    "        tensor_items = [(k, v) for k, v in out.items() if torch.is_tensor(v)]\n",
    "        if len(tensor_items) == 1:\n",
    "            t = tensor_items[0][1]\n",
    "            return _select_point_from_quantile_tensor(t, prefer_q=prefer_q), out\n",
    "\n",
    "        raise KeyError(f\"Cannot find tensor prediction in dict output. keys={list(out.keys())}\")\n",
    "\n",
    "    raise TypeError(f\"Unsupported model output type: {type(out)}\")\n",
    "\n",
    "\n",
    "def _select_point_from_quantile_tensor(t: torch.Tensor, prefer_q=0.5):\n",
    "    \"\"\"\n",
    "    t가\n",
    "      - [B,H]이면 그대로\n",
    "      - [B,H,Q]이면 Q축에서 median(0.5)에 해당하는 index 선택\n",
    "      - [B,Q,H]이면 Q축에서 선택 후 [B,H]로\n",
    "    \"\"\"\n",
    "    if t.ndim == 2:\n",
    "        return t\n",
    "\n",
    "    if t.ndim == 3:\n",
    "        B, d1, d2 = t.shape\n",
    "\n",
    "        # [B,H,Q] 케이스 가정: d1이 horizon, d2가 quantile\n",
    "        # [B,Q,H] 케이스 가정: d1이 quantile, d2가 horizon\n",
    "        # heuristic: horizon은 보통 27 같은 값, quantile은 보통 3/5/9 등 작은 값\n",
    "        if d1 > d2:\n",
    "            # [B,H,Q]\n",
    "            q_dim = 2\n",
    "            q_len = d2\n",
    "            h_dim = 1\n",
    "        else:\n",
    "            # [B,Q,H]\n",
    "            q_dim = 1\n",
    "            q_len = d1\n",
    "            h_dim = 2\n",
    "\n",
    "        # prefer_q=0.5에 해당하는 index를 선택 (가능하면 중앙)\n",
    "        # q 값 리스트를 out에 포함하지 않는 경우가 많으니 중앙 index를 사용\n",
    "        q_idx = int(round((q_len - 1) * prefer_q))  # median index\n",
    "\n",
    "        if q_dim == 2:\n",
    "            # [B,H,Q] -> [B,H]\n",
    "            return t[:, :, q_idx]\n",
    "        else:\n",
    "            # [B,Q,H] -> [B,H]\n",
    "            return t[:, q_idx, :]\n",
    "\n",
    "    raise ValueError(f\"Unexpected prediction tensor shape: {t.shape}\")\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_on_loader_quantile(model, loader, device: str, prefer_q: float = 0.5):\n",
    "    \"\"\"\n",
    "    Quantile 모델 평가:\n",
    "      - y_true: [N,H]\n",
    "      - yhat_point: [N,H] (기본: 중앙 quantile로 point화)\n",
    "      - outs: 필요 시 분석용 raw output list(옵션으로 쓸 수 있음)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    ys, yhats = [], []\n",
    "\n",
    "    for batch in loader:\n",
    "        x = batch[0].to(device)              # [B, 52, 1]\n",
    "        y = batch[1].to(device)              # [B, 27]\n",
    "        future_exo = batch[3].to(device)     # [B, 27, 4]\n",
    "        past_exo_cont = batch[4].to(device)  # [B, 52, 11]\n",
    "        past_exo_cat = batch[5].to(device)   # [B, 52, 0]\n",
    "\n",
    "        out = model(\n",
    "            x,\n",
    "            future_exo=future_exo,\n",
    "            past_exo_cont=past_exo_cont,\n",
    "            past_exo_cat=past_exo_cat,\n",
    "        )\n",
    "\n",
    "        yhat_point, _ = _extract_pred_from_output(out, prefer_q=prefer_q)\n",
    "\n",
    "        ys.append(y.detach().cpu().numpy())\n",
    "        yhats.append(yhat_point.detach().cpu().numpy())\n",
    "\n",
    "    y_all = np.concatenate(ys, axis=0)\n",
    "    yhat_all = np.concatenate(yhats, axis=0)\n",
    "    return y_all, yhat_all"
   ],
   "id": "6e859be070e0f372",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from modeling_module.data_loader import MultiPartExoDataModule\n",
    "from modeling_module.utils.exogenous_utils import compose_exo_calendar_cb\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "lookback = 27\n",
    "horizon = 8\n",
    "\n",
    "def set_seed(seed = 11):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.backends.mps.is_available():\n",
    "        torch.mps.manual_seed(seed)\n",
    "\n",
    "set_seed(11)\n",
    "save_root_A = DIR + 'fit/model_validation/A_no_pretrain'\n",
    "save_root_B = DIR + 'fit/model_validation/B_with_pretrain'\n",
    "\n",
    "\n",
    "future_exo_cb = compose_exo_calendar_cb(date_type = 'H', sincos = True)\n",
    "\n",
    "data_module = MultiPartExoDataModule(\n",
    "    df,\n",
    "    id_col = 'unique_id',\n",
    "    date_col = 'date',\n",
    "    y_col = 'y',\n",
    "    lookback = lookback,\n",
    "    horizon = horizon,\n",
    "    batch_size = 128,\n",
    "    past_exo_cont_cols = past_cols,\n",
    "    future_exo_cb = future_exo_cb,\n",
    "    freq = 'hourly',\n",
    "    shuffle = True,\n",
    "    split_mode = 'multi',\n",
    ")\n",
    "\n",
    "train_loader = data_module.get_train_loader()\n",
    "val_loader = data_module.get_val_loader()"
   ],
   "id": "2e31b28da6d76f04",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from modeling_module.training.model_trainers.total_train import run_total_train_hourly, summarize_metrics\n",
    "\n",
    "results_A = run_total_train_hourly(\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    device=device,\n",
    "    lookback=lookback,\n",
    "    horizon=horizon,\n",
    "    save_dir=save_root_A,\n",
    "    models_to_run=[\"patchtst\"],\n",
    "    use_ssl_pretrain=False,         # ★ A: off\n",
    ")"
   ],
   "id": "5fcfb351b3e80989",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from modeling_module.training.model_trainers.total_train import run_total_train_hourly, summarize_metrics\n",
    "\n",
    "\n",
    "results_B = run_total_train_hourly(\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    device=device,\n",
    "    lookback=lookback,\n",
    "    horizon=horizon,\n",
    "    save_dir=save_root_B,\n",
    "    models_to_run=[\"patchtst\"],\n",
    "    use_ssl_pretrain=True,          # ★ B: on\n",
    "    ssl_pretrain_epochs=10,\n",
    "    ssl_mask_ratio=0.3,\n",
    "    ssl_loss_type=\"mse\",\n",
    "    ssl_freeze_encoder_before_ft=False,  # freeze->unfreeze는 다음 단계에서\n",
    ")\n"
   ],
   "id": "c72dc8e7f4ceed07",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from modeling_module.utils.checkpoint import load_model_dict\n",
    "from modeling_module.models import build_patchTST_base, build_patchTST_quantile\n",
    "\n",
    "builders = {\n",
    "    \"patchtst_quantile\": build_patchTST_quantile,\n",
    "}\n",
    "\n",
    "modelA = load_model_dict(f\"{save_root_A}\", builders, device = device)['patchtst_quantile']\n",
    "modelB = load_model_dict(f\"{save_root_B}\", builders, device = device)['patchtst_quantile']\n",
    "\n",
    "# yA, yhatA = eval_on_loader(modelA, val_loader, device)\n",
    "# yB, yhatB = eval_on_loader(modelB, val_loader, device)\n",
    "\n",
    "yA, yhatA = eval_on_loader_quantile(modelA, val_loader, device, prefer_q=0.5)\n",
    "yB, yhatB = eval_on_loader_quantile(modelB, val_loader, device, prefer_q=0.5)\n",
    "\n",
    "metricA = {\n",
    "    \"MAE\": float(mae(yA.reshape(-1), yhatA.reshape(-1))),\n",
    "    \"RMSE\": float(rmse(yA.reshape(-1), yhatA.reshape(-1))),\n",
    "    \"SMAPE\": float(smape(yA.reshape(-1), yhatA.reshape(-1))),\n",
    "}\n",
    "metricB = {\n",
    "    \"MAE\": float(mae(yB.reshape(-1), yhatB.reshape(-1))),\n",
    "    \"RMSE\": float(rmse(yB.reshape(-1), yhatB.reshape(-1))),\n",
    "    \"SMAPE\": float(smape(yB.reshape(-1), yhatB.reshape(-1))),\n",
    "}\n",
    "metricA, metricB\n"
   ],
   "id": "165ded8b3b441f8b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_one_sample(y_true, yhatA, yhatB, idx=0):\n",
    "    yt = y_true[idx].reshape(-1)\n",
    "    ya = yhatA[idx].reshape(-1)\n",
    "    yb = yhatB[idx].reshape(-1)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(yt, label=\"true\")\n",
    "    plt.plot(ya, label=\"A_no_pretrain\")\n",
    "    plt.plot(yb, label=\"B_with_pretrain\")\n",
    "    plt.legend()\n",
    "    plt.title(f\"A/B forecast comparison (sample={idx})\")\n",
    "    plt.show()\n",
    "\n",
    "plot_one_sample(yA, yhatA, yhatB, idx=128)"
   ],
   "id": "40b2fee3a02764a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_all_samples_vertical(y_true, yhatA, yhatB, max_n=128):\n",
    "    n = min(max_n, y_true.shape[0])\n",
    "\n",
    "    fig, axes = plt.subplots(\n",
    "        nrows=n, ncols=1,\n",
    "        figsize=(12, 2.2 * n),   # n이 커질수록 세로 길이 증가\n",
    "        sharex=True,\n",
    "        sharey=False\n",
    "    )\n",
    "\n",
    "    # n=1일 때 axes가 단일 객체가 되므로 list로 통일\n",
    "    if n == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for i in range(n):\n",
    "        yt = y_true[i].reshape(-1)\n",
    "        ya = yhatA[i].reshape(-1)\n",
    "        yb = yhatB[i].reshape(-1)\n",
    "\n",
    "        ax = axes[i]\n",
    "        ax.plot(yt, label=\"true\")\n",
    "        ax.plot(ya, label=\"A_no_pretrain\")\n",
    "        ax.plot(yb, label=\"B_with_pretrain\")\n",
    "        ax.set_title(f\"sample={i}\", fontsize=9)\n",
    "\n",
    "        # 너무 복잡해지니 legend는 첫 번째 축에만\n",
    "        if i == 0:\n",
    "            ax.legend(loc=\"upper right\", fontsize=8)\n",
    "\n",
    "    fig.suptitle(\"A/B forecast comparison (all samples)\", y=1.002, fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_all_samples_vertical(yA, yhatA, yhatB, max_n=128)\n"
   ],
   "id": "28d1fee5672795b5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "101682bbb926de93",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
